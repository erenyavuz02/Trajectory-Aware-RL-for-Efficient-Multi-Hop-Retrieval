{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#¬†Login wandb project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1749473049873,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "TS4oVS6DkUPZ",
    "outputId": "d70ca38e-45ba-4946-8927-b621c703c948"
   },
   "outputs": [],
   "source": [
    "#!pip install -U transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#¬†Mount google drive if using colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "e5e7917c4a364b868179019231ae1bff",
      "3335fb1d97f349d39301a592503226c1",
      "85f92b4750214608ac3c7ff809ea9aec",
      "33fab839b102417e99e8ec9204550cc2",
      "0cafa7349eef4b7b8cc5253802943b9a",
      "be53745e6e034ce09d34e2268820684a",
      "c3764dc8905d48cb8537ddd68ce72064",
      "756d9e19eada4f15a9a5d24669211455",
      "fbf2a60b9ab54eba868d3e8f99f92255",
      "c2e142305b1a4cf1941c19d03bda6c62",
      "32be6281e7e3404685b4609ea6dc377f"
     ]
    },
    "executionInfo": {
     "elapsed": 3346,
     "status": "ok",
     "timestamp": 1749473075680,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "ShNkRy0xkUdh",
    "outputId": "714903c8-2b4a-4114-ecd5-202a2e535909"
   },
   "outputs": [],
   "source": [
    "# Add this import at the top of your model loading cell (cell 6)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.utils as utils  # Add this line\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        self.data = []\n",
    "        for question, entry in raw_data.items():\n",
    "            for hop, hop_data in entry[\"hops\"].items():\n",
    "                queries = hop_data[\"queries\"]\n",
    "                preferences = hop_data[\"preference_pairs\"]\n",
    "                for i, j in preferences:\n",
    "                    self.data.append({\n",
    "                        \"question\": question,\n",
    "                        \"preferred\": queries[i],\n",
    "                        \"dispreferred\": queries[j]\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2447,
     "status": "ok",
     "timestamp": 1749473084147,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "rFgSgz03s3id",
    "outputId": "dd8c5653-6b11-493e-cf57-5759aace64ad"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "59dfdf60a56d4c4dacf5b37260c50d3b",
      "fe5493fd0ba247ec96d735befed62fd0",
      "539cef24281245139a1002c0f1c53fea",
      "f2a65985f11c4b07965612499dc807a9",
      "15820aef970b43cca7266993df78365e",
      "b9ceec0fc7824ddc8c56290aed1f0c35",
      "99247cb0e0fb420b84452bd4f6151cca",
      "07e433061b02424fb88975c726d63c86",
      "361d3827ea954df8b7e31748251153db",
      "49329b09131c4a9da35b4ab8d68bf7e0",
      "c7235724003a49c29c704c89305cab8a"
     ]
    },
    "executionInfo": {
     "elapsed": 8096,
     "status": "ok",
     "timestamp": 1749473356966,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "a8bV7D60kZaH",
    "outputId": "fee635c4-24ec-4b91-f339-b98c9d4007c3"
   },
   "outputs": [],
   "source": [
    "login(\"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\")\n",
    "\n",
    "# Use a smaller model to avoid memory issues\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # Much smaller than LLaMA-3-8B\n",
    "# Alternative: \"distilbert-base-uncased\" or \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Load model with more memory-efficient settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Let it decide device placement\n",
    "    low_cpu_mem_usage=True,  # More memory efficient loading\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    \"model\": model_name,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"lr\": 5e-6,\n",
    "    \"tau\": 0.05,\n",
    "    \"batch_size\": 1,  # Reduced batch size\n",
    "    \"epochs\": 3,\n",
    "}\n",
    "\n",
    "# === Helpers ===\n",
    "def safe_ipo_loss(logp_win, logp_lose, tau=0.05):\n",
    "    \"\"\"Safe IPO loss with NaN detection and clipping\"\"\"\n",
    "    # Ensure inputs are valid\n",
    "    if torch.isnan(logp_win).any() or torch.isnan(logp_lose).any():\n",
    "        print(f\"NaN detected in logp: win={logp_win.item()}, lose={logp_lose.item()}\")\n",
    "        return torch.tensor(0.0, device=logp_win.device, requires_grad=True)\n",
    "    \n",
    "    # Clip extreme values\n",
    "    logp_win = torch.clamp(logp_win, min=-10, max=10)\n",
    "    logp_lose = torch.clamp(logp_lose, min=-10, max=10)\n",
    "    \n",
    "    # Compute IPO loss with safety checks\n",
    "    diff = logp_win - logp_lose - 0.5 / tau\n",
    "    loss = (diff ** 2).mean()\n",
    "    \n",
    "    # Check for NaN in loss\n",
    "    if torch.isnan(loss):\n",
    "        print(f\"NaN in loss computation: diff={diff.item()}\")\n",
    "        return torch.tensor(0.0, device=logp_win.device, requires_grad=True)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def safe_compute_logp(prompt, completion):\n",
    "    \"\"\"Safe log probability computation with error handling\"\"\"\n",
    "    try:\n",
    "        full_input = prompt + completion\n",
    "        \n",
    "        # Shorter sequences to avoid issues\n",
    "        encoded = tokenizer(full_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "        # Get prompt length\n",
    "        prompt_encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "        prompt_len = prompt_encoded.input_ids.shape[-1]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:, :prompt_len] = -100  # mask out prompt\n",
    "\n",
    "        # Use full precision for stability\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "        logp = -outputs.loss\n",
    "        \n",
    "        # Safety checks\n",
    "        if torch.isnan(logp) or torch.isinf(logp):\n",
    "            print(f\"Invalid logp: {logp.item()}\")\n",
    "            return torch.tensor(-1.0, device=device, requires_grad=True)\n",
    "            \n",
    "        return logp.detach().requires_grad_(True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_logp: {e}\")\n",
    "        return torch.tensor(-1.0, device=next(model.parameters()).device, requires_grad=True)\n",
    "\n",
    "        \n",
    "print(f\"Model {model_name} loaded successfully!\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "error",
     "timestamp": 1749473433469,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "N4SISxZekb9_",
    "outputId": "806a5d55-635a-4565-fd3b-b34bc053c867"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'preference_dataset_hotpotqa_final.json'\n",
    "dataset = PreferenceDataset(dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "tau = 0.05\n",
    "num_epochs = 1\n",
    "\n",
    "def compute_logp(prompt, completion):\n",
    "    \"\"\"Basic log probability computation\"\"\"\n",
    "    return safe_compute_logp(prompt, completion)\n",
    "\n",
    "def ipo_loss(logp_win, logp_lose, tau=0.05):\n",
    "    \"\"\"Basic IPO loss\"\"\"\n",
    "    return safe_ipo_loss(logp_win, logp_lose, tau)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        questions = batch[\"question\"]\n",
    "        preferred = batch[\"preferred\"] \n",
    "        dispreferred = batch[\"dispreferred\"]\n",
    "\n",
    "        logp_w_list = []\n",
    "        logp_l_list = []\n",
    "\n",
    "        for q, w, l in zip(questions, preferred, dispreferred):\n",
    "            prompt = f\"Query: \"\n",
    "            \n",
    "            logp_w = compute_logp(prompt, w.strip())\n",
    "            logp_l = compute_logp(prompt, l.strip())\n",
    "            \n",
    "            logp_w_list.append(logp_w)\n",
    "            logp_l_list.append(logp_l)\n",
    "\n",
    "        logp_w_batch = torch.stack(logp_w_list)\n",
    "        logp_l_batch = torch.stack(logp_l_list)\n",
    "        \n",
    "        loss = ipo_loss(logp_w_batch, logp_l_batch, tau)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST TRAINING - Stop after 1 iteration ===\n",
    "dataset_path = 'preference_dataset_hotpotqa_final.json'\n",
    "dataset = PreferenceDataset(dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "tau = 0.05\n",
    "num_epochs = 1  # Only 1 epoch for testing\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print(f\"Starting test run - will stop after 1 iteration\")\n",
    "print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Define missing functions\n",
    "def compute_logp(prompt, completion):\n",
    "    \"\"\"Basic log probability computation\"\"\"\n",
    "    return safe_compute_logp(prompt, completion)\n",
    "\n",
    "def ipo_loss(logp_win, logp_lose, tau=0.05):\n",
    "    \"\"\"Basic IPO loss\"\"\"\n",
    "    return safe_ipo_loss(logp_win, logp_lose, tau)\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Test Epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        print(f\"Processing batch {batch_idx}\")\n",
    "        \n",
    "        questions = batch[\"question\"]\n",
    "        preferred = batch[\"preferred\"]\n",
    "        dispreferred = batch[\"dispreferred\"]\n",
    "        \n",
    "        print(f\"Question: {questions[0][:100]}...\")\n",
    "        print(f\"Preferred: {preferred[0]}\")\n",
    "        print(f\"Dispreferred: {dispreferred[0]}\")\n",
    "\n",
    "        logp_w_list, logp_l_list = [], []\n",
    "\n",
    "        for q, w, l in zip(questions, preferred, dispreferred):\n",
    "            try:\n",
    "                prompt = f\"Generate a search query for the following question:\\n{q}\\nQuery:\"\n",
    "                print(f\"Computing logp for preferred: {w}\")\n",
    "                logp_w = compute_logp(prompt, \" \" + w)\n",
    "                print(f\"Preferred logp: {logp_w.item():.4f}\")\n",
    "                \n",
    "                print(f\"Computing logp for dispreferred: {l}\")\n",
    "                logp_l = compute_logp(prompt, \" \" + l)\n",
    "                print(f\"Dispreferred logp: {logp_l.item():.4f}\")\n",
    "                \n",
    "                logp_w_list.append(logp_w)\n",
    "                logp_l_list.append(logp_l)\n",
    "                print(f\"Batch {batch_idx} completed successfully\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                clear_cuda_cache()\n",
    "                continue\n",
    "\n",
    "        if logp_w_list and logp_l_list:\n",
    "            logp_w_batch = torch.stack(logp_w_list)\n",
    "            logp_l_batch = torch.stack(logp_l_list)\n",
    "            loss = ipo_loss(logp_w_batch, logp_l_batch, tau)\n",
    "            \n",
    "            print(f\"IPO Loss: {loss.item():.4f}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Loss computed and backprop completed: {loss.item():.4f}\")\n",
    "            \n",
    "            # Clear intermediate tensors and cache\n",
    "            del logp_w_list, logp_l_list, logp_w_batch, logp_l_batch, loss\n",
    "            \n",
    "        clear_cuda_cache()\n",
    "        print(f\"GPU memory after batch: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "        avg_loss = total_loss / max(1, batch_idx + 1)\n",
    "        pbar.set_postfix({\"loss\": avg_loss})\n",
    "        print(f\"Test loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # STOP AFTER FIRST ITERATION FOR TESTING\n",
    "        print(\"TEST COMPLETE - Stopping after 1 iteration\")\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    print(f\"[Test Epoch {epoch + 1}] Average Loss: {total_loss / max(1, 1):.4f}\")\n",
    "    clear_cuda_cache()\n",
    "\n",
    "print(\"Test run completed successfully!\")\n",
    "print(f\"Final GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1749473335432,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "XQbXRzWqvBBU"
   },
   "outputs": [],
   "source": [
    "# prompt: free the cuda gpu memory aggresivley, atomic way, nuclear it : \"OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 76.88 MiB is free. Process 72946 has 39.47 GiB memory in use. Of the allocated memory 38.61 GiB is allocated by PyTorch, and 371.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "# \"\n",
    "\n",
    "# Function to clear CUDA cache\n",
    "def clear_cuda_cache():\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n",
    "\n",
    "# Call this function after each batch or at the end of each epoch\n",
    "# Example usage within your training loop:\n",
    "# ... (inside the for batch loop) ...\n",
    "#         optimizer.step()\n",
    "#         clear_cuda_cache() # Call after optimizer step\n",
    "# ... (outside the for batch loop) ...\n",
    "# print(f\"[Epoch {epoch + 1}] Average Loss: {total_loss / len(dataloader):.4f}\")\n",
    "# clear_cuda_cache() # Call at the end of the epoch\n",
    "\n",
    "# You can also call it at the beginning of the loop or whenever you suspect memory issues.\n",
    "\n",
    "# Alternatively, you can also try deleting tensors that are no longer needed.\n",
    "# For example, inside the batch loop:\n",
    "# del logp_w_list, logp_l_list, logp_w_batch, logp_l_batch, loss\n",
    "# clear_cuda_cache()\n",
    "\n",
    "# Add this function definition somewhere before the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuclear GPU memory clearing - run this first\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def nuclear_gpu_clear():\n",
    "    \"\"\"Aggressively clear GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Clear all cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Reset memory stats\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "        \n",
    "        # Try to clear any remaining allocations\n",
    "        try:\n",
    "            torch.cuda.synchronize()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        print(f\"GPU memory cleared. Free: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()} bytes\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory} bytes\")\n",
    "        print(f\"Currently allocated: {torch.cuda.memory_allocated()} bytes\")\n",
    "\n",
    "# Set environment variable for memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear GPU memory now\n",
    "nuclear_gpu_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING compute_logp FUNCTION\n",
      "================================================================================\n",
      "\n",
      "üìã Test Case: Simple Query Generation\n",
      "Prompt: 'Query: '\n",
      "------------------------------------------------------------\n",
      "Invalid logp: nan\n",
      "1. Completion: 'what is machine learning'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "2. Completion: 'machine learning definition'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "3. Completion: 'ML basics'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "4. Completion: 'artificial intelligence'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "5. Completion: 'random text here'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "üèÜ RANKING (Best to Worst):\n",
      "1. 'what is machine learning' (logp: -1.0000)\n",
      "2. 'machine learning definition' (logp: -1.0000)\n",
      "3. 'ML basics' (logp: -1.0000)\n",
      "4. 'artificial intelligence' (logp: -1.0000)\n",
      "5. 'random text here' (logp: -1.0000)\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìã Test Case: Question-Answer Format\n",
      "Prompt: 'Question: What is the capital of France?\n",
      "Answer:'\n",
      "------------------------------------------------------------\n",
      "Invalid logp: nan\n",
      "1. Completion: ' Paris'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "2. Completion: ' London'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "3. Completion: ' Berlin'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "4. Completion: ' The capital is Paris'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "5. Completion: ' I don't know'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "üèÜ RANKING (Best to Worst):\n",
      "1. ' Paris' (logp: -1.0000)\n",
      "2. ' London' (logp: -1.0000)\n",
      "3. ' Berlin' (logp: -1.0000)\n",
      "4. ' The capital is Paris' (logp: -1.0000)\n",
      "5. ' I don't know' (logp: -1.0000)\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìã Test Case: Search Query Context\n",
      "Prompt: 'Generate a search query for: How does photosynthesis work?\n",
      "Query:'\n",
      "------------------------------------------------------------\n",
      "Invalid logp: nan\n",
      "1. Completion: ' photosynthesis process'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "2. Completion: ' how photosynthesis works'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "3. Completion: ' plant biology photosynthesis'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "4. Completion: ' chlorophyll sunlight conversion'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "5. Completion: ' unrelated topic'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "üèÜ RANKING (Best to Worst):\n",
      "1. ' photosynthesis process' (logp: -1.0000)\n",
      "2. ' how photosynthesis works' (logp: -1.0000)\n",
      "3. ' plant biology photosynthesis' (logp: -1.0000)\n",
      "4. ' chlorophyll sunlight conversion' (logp: -1.0000)\n",
      "5. ' unrelated topic' (logp: -1.0000)\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìã Test Case: Short vs Long Completions\n",
      "Prompt: 'Query: '\n",
      "------------------------------------------------------------\n",
      "Invalid logp: nan\n",
      "1. Completion: 'AI'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "2. Completion: 'artificial intelligence'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "3. Completion: 'artificial intelligence and machine learning overview'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "4. Completion: 'comprehensive guide to artificial intelligence and machine learning technologies'\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "Invalid logp: nan\n",
      "5. Completion: ''\n",
      "   Log Prob: -1.0000\n",
      "   Probability: 0.367879\n",
      "\n",
      "üèÜ RANKING (Best to Worst):\n",
      "1. 'AI' (logp: -1.0000)\n",
      "2. 'artificial intelligence' (logp: -1.0000)\n",
      "3. 'artificial intelligence and machine learning overview' (logp: -1.0000)\n",
      "4. 'comprehensive guide to artificial intelligence and machine learning technologies' (logp: -1.0000)\n",
      "5. '' (logp: -1.0000)\n",
      "\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING PROMPT VARIATIONS\n",
      "Fixed completion: 'machine learning algorithms'\n",
      "================================================================================\n",
      "Invalid logp: nan\n",
      "Prompt: 'Query: '\n",
      "Log Prob: -1.0000\n",
      "\n",
      "Invalid logp: nan\n",
      "Prompt: 'Search: '\n",
      "Log Prob: -1.0000\n",
      "\n",
      "Invalid logp: nan\n",
      "Prompt: 'Find information about '\n",
      "Log Prob: -1.0000\n",
      "\n",
      "Invalid logp: nan\n",
      "Prompt: 'Generate a query for '\n",
      "Log Prob: -1.0000\n",
      "\n",
      "Invalid logp: nan\n",
      "Prompt: 'What should I search for regarding '\n",
      "Log Prob: -1.0000\n",
      "\n",
      "Invalid logp: nan\n",
      "Prompt: ''\n",
      "Log Prob: -1.0000\n",
      "\n",
      "üèÜ BEST PROMPTS:\n",
      "1. 'Query: ' (logp: -1.0000)\n",
      "2. 'Search: ' (logp: -1.0000)\n",
      "3. 'Find information about ' (logp: -1.0000)\n",
      "4. 'Generate a query for ' (logp: -1.0000)\n",
      "5. 'What should I search for regarding ' (logp: -1.0000)\n",
      "6. '' (logp: -1.0000)\n",
      "\n",
      "================================================================================\n",
      "TESTING LENGTH EFFECT\n",
      "================================================================================\n",
      "Invalid logp: nan\n",
      "Length: 16 chars\n",
      "Text: 'machine learning'\n",
      "Log Prob: -1.0000\n",
      "Avg Log Prob per char: -0.0625\n",
      "----------------------------------------\n",
      "Invalid logp: nan\n",
      "Length: 27 chars\n",
      "Text: 'machine learning algorithms'\n",
      "Log Prob: -1.0000\n",
      "Avg Log Prob per char: -0.0370\n",
      "----------------------------------------\n",
      "Invalid logp: nan\n",
      "Length: 47 chars\n",
      "Text: 'machine learning algorithms and neural networks'\n",
      "Log Prob: -1.0000\n",
      "Avg Log Prob per char: -0.0213\n",
      "----------------------------------------\n",
      "Invalid logp: nan\n",
      "Length: 64 chars\n",
      "Text: 'machine learning algorithms and neural networks for data science'\n",
      "Log Prob: -1.0000\n",
      "Avg Log Prob per char: -0.0156\n",
      "----------------------------------------\n",
      "Invalid logp: nan\n",
      "Length: 77 chars\n",
      "Text: 'machine learning algorithms and neural networks for data science applications'\n",
      "Log Prob: -1.0000\n",
      "Avg Log Prob per char: -0.0130\n",
      "----------------------------------------\n",
      "\n",
      "‚úÖ Testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Test compute_logp function with various examples\n",
    "import torch\n",
    "\n",
    "def test_compute_logp_examples():\n",
    "    \"\"\"Test compute_logp with various prompt-completion pairs\"\"\"\n",
    "    \n",
    "    # Test examples with different scenarios\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Simple Query Generation\",\n",
    "            \"prompt\": \"Query: \",\n",
    "            \"completions\": [\n",
    "                \"what is machine learning\",\n",
    "                \"machine learning definition\",\n",
    "                \"ML basics\",\n",
    "                \"artificial intelligence\",\n",
    "                \"random text here\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Question-Answer Format\",\n",
    "            \"prompt\": \"Question: What is the capital of France?\\nAnswer:\",\n",
    "            \"completions\": [\n",
    "                \" Paris\",\n",
    "                \" London\", \n",
    "                \" Berlin\",\n",
    "                \" The capital is Paris\",\n",
    "                \" I don't know\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Search Query Context\",\n",
    "            \"prompt\": \"Generate a search query for: How does photosynthesis work?\\nQuery:\",\n",
    "            \"completions\": [\n",
    "                \" photosynthesis process\",\n",
    "                \" how photosynthesis works\",\n",
    "                \" plant biology photosynthesis\",\n",
    "                \" chlorophyll sunlight conversion\",\n",
    "                \" unrelated topic\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Short vs Long Completions\",\n",
    "            \"prompt\": \"Query: \",\n",
    "            \"completions\": [\n",
    "                \"AI\",\n",
    "                \"artificial intelligence\",\n",
    "                \"artificial intelligence and machine learning overview\",\n",
    "                \"comprehensive guide to artificial intelligence and machine learning technologies\",\n",
    "                \"\"  # empty completion\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"TESTING compute_logp FUNCTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        print(f\"\\nüìã Test Case: {test_case['name']}\")\n",
    "        print(f\"Prompt: '{test_case['prompt']}'\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, completion in enumerate(test_case['completions']):\n",
    "            try:\n",
    "                # Compute log probability\n",
    "                logp = compute_logp(test_case['prompt'], completion)\n",
    "                logp_value = logp.item() if hasattr(logp, 'item') else float(logp)\n",
    "                \n",
    "                # Store result\n",
    "                results.append({\n",
    "                    'completion': completion,\n",
    "                    'logp': logp_value,\n",
    "                    'prob': torch.exp(logp).item() if hasattr(logp, 'item') else float(torch.exp(torch.tensor(logp_value)))\n",
    "                })\n",
    "                \n",
    "                print(f\"{i+1}. Completion: '{completion}'\")\n",
    "                print(f\"   Log Prob: {logp_value:.4f}\")\n",
    "                print(f\"   Probability: {results[-1]['prob']:.6f}\")\n",
    "                print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{i+1}. Completion: '{completion}' - ERROR: {e}\")\n",
    "                print()\n",
    "        \n",
    "        # Sort by log probability (highest first)\n",
    "        results.sort(key=lambda x: x['logp'], reverse=True)\n",
    "        \n",
    "        print(\"üèÜ RANKING (Best to Worst):\")\n",
    "        for rank, result in enumerate(results, 1):\n",
    "            print(f\"{rank}. '{result['completion']}' (logp: {result['logp']:.4f})\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "def test_prompt_variations():\n",
    "    \"\"\"Test how different prompts affect the same completion\"\"\"\n",
    "    \n",
    "    completion = \"machine learning algorithms\"\n",
    "    \n",
    "    prompts = [\n",
    "        \"Query: \",\n",
    "        \"Search: \",\n",
    "        \"Find information about \",\n",
    "        \"Generate a query for \",\n",
    "        \"What should I search for regarding \",\n",
    "        \"\"  # no prompt\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TESTING PROMPT VARIATIONS\")\n",
    "    print(f\"Fixed completion: '{completion}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            logp = compute_logp(prompt, completion)\n",
    "            logp_value = logp.item() if hasattr(logp, 'item') else float(logp)\n",
    "            \n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'logp': logp_value\n",
    "            })\n",
    "            \n",
    "            print(f\"Prompt: '{prompt}'\")\n",
    "            print(f\"Log Prob: {logp_value:.4f}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prompt: '{prompt}' - ERROR: {e}\")\n",
    "            print()\n",
    "    \n",
    "    # Sort by log probability\n",
    "    results.sort(key=lambda x: x['logp'], reverse=True)\n",
    "    \n",
    "    print(\"üèÜ BEST PROMPTS:\")\n",
    "    for rank, result in enumerate(results, 1):\n",
    "        print(f\"{rank}. '{result['prompt']}' (logp: {result['logp']:.4f})\")\n",
    "\n",
    "def test_length_effect():\n",
    "    \"\"\"Test how completion length affects log probability\"\"\"\n",
    "    \n",
    "    prompt = \"Query: \"\n",
    "    base_text = \"machine learning\"\n",
    "    \n",
    "    completions = [\n",
    "        base_text,\n",
    "        base_text + \" algorithms\",\n",
    "        base_text + \" algorithms and neural networks\",\n",
    "        base_text + \" algorithms and neural networks for data science\",\n",
    "        base_text + \" algorithms and neural networks for data science applications\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TESTING LENGTH EFFECT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for completion in completions:\n",
    "        try:\n",
    "            logp = compute_logp(prompt, completion)\n",
    "            logp_value = logp.item() if hasattr(logp, 'item') else float(logp)\n",
    "            \n",
    "            print(f\"Length: {len(completion)} chars\")\n",
    "            print(f\"Text: '{completion}'\")\n",
    "            print(f\"Log Prob: {logp_value:.4f}\")\n",
    "            print(f\"Avg Log Prob per char: {logp_value/len(completion):.4f}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Text: '{completion}' - ERROR: {e}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "# Run all tests\n",
    "if __name__ == \"__main__\":\n",
    "    test_compute_logp_examples()\n",
    "    test_prompt_variations()\n",
    "    test_length_effect()\n",
    "    \n",
    "    print(\"\\n‚úÖ Testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKING MODEL STATE\n",
      "================================================================================\n",
      "Model type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.float16\n",
      "Model training mode: True\n",
      "Model vocab size: 50257\n",
      "Tokenizer vocab size: 50257\n",
      "‚úÖ Model forward pass works\n",
      "Output logits shape: torch.Size([1, 1, 50257])\n",
      "Output logits range: nan to nan\n",
      "================================================================================\n",
      "TESTING SIMPLE CASES\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîç DEBUGGING compute_logp\n",
      "Prompt: ''\n",
      "Completion: 'hello'\n",
      "Full input: 'hello'\n",
      "Full input tokens: tensor([[31373]])\n",
      "Full input decoded: hello\n",
      "Prompt tokens: tensor([], size=(1, 0))\n",
      "Prompt length: 0\n",
      "Prompt decoded: \n",
      "Labels (masked): tensor([[31373]], device='cuda:0')\n",
      "Completion tokens (for loss): tensor([[31373]], device='cuda:0')\n",
      "Completion decoded: hello\n",
      "Model device: cuda:0\n",
      "Input shape: torch.Size([1, 1])\n",
      "Raw loss: nan\n",
      "Loss device: cuda:0\n",
      "Loss dtype: torch.float32\n",
      "Is loss NaN: True\n",
      "Is loss infinite: False\n",
      "Logits shape: torch.Size([1, 1, 50257])\n",
      "Logits range: nan to nan\n",
      "Log probability: nan\n",
      "‚úÖ Success! Log prob: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîç DEBUGGING compute_logp\n",
      "Prompt: 'Q:'\n",
      "Completion: 'A'\n",
      "Full input: 'Q:A'\n",
      "Full input tokens: tensor([[48, 25, 32]])\n",
      "Full input decoded: Q:A\n",
      "Prompt tokens: tensor([[48, 25]])\n",
      "Prompt length: 2\n",
      "Prompt decoded: Q:\n",
      "Labels (masked): tensor([[-100, -100,   32]], device='cuda:0')\n",
      "Completion tokens (for loss): tensor([[32]], device='cuda:0')\n",
      "Completion decoded: A\n",
      "Model device: cuda:0\n",
      "Input shape: torch.Size([1, 3])\n",
      "Raw loss: nan\n",
      "Loss device: cuda:0\n",
      "Loss dtype: torch.float32\n",
      "Is loss NaN: True\n",
      "Is loss infinite: False\n",
      "Logits shape: torch.Size([1, 3, 50257])\n",
      "Logits range: nan to nan\n",
      "Log probability: nan\n",
      "‚úÖ Success! Log prob: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîç DEBUGGING compute_logp\n",
      "Prompt: 'Query:'\n",
      "Completion: 'search'\n",
      "Full input: 'Query:search'\n",
      "Full input tokens: tensor([[20746,    25, 12947]])\n",
      "Full input decoded: Query:search\n",
      "Prompt tokens: tensor([[20746,    25]])\n",
      "Prompt length: 2\n",
      "Prompt decoded: Query:\n",
      "Labels (masked): tensor([[ -100,  -100, 12947]], device='cuda:0')\n",
      "Completion tokens (for loss): tensor([[12947]], device='cuda:0')\n",
      "Completion decoded: search\n",
      "Model device: cuda:0\n",
      "Input shape: torch.Size([1, 3])\n",
      "Raw loss: nan\n",
      "Loss device: cuda:0\n",
      "Loss dtype: torch.float32\n",
      "Is loss NaN: True\n",
      "Is loss infinite: False\n",
      "Logits shape: torch.Size([1, 3, 50257])\n",
      "Logits range: nan to nan\n",
      "Log probability: nan\n",
      "‚úÖ Success! Log prob: nan\n"
     ]
    }
   ],
   "source": [
    "# Debug compute_logp function - run this to understand what's happening\n",
    "import torch\n",
    "\n",
    "def debug_compute_logp(prompt, completion):\n",
    "    \"\"\"Debug version of compute_logp with detailed logging\"\"\"\n",
    "    print(f\"\\nüîç DEBUGGING compute_logp\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Completion: '{completion}'\")\n",
    "    \n",
    "    full_input = prompt + completion\n",
    "    print(f\"Full input: '{full_input}'\")\n",
    "    \n",
    "    # Tokenize and show tokens\n",
    "    encoded = tokenizer(full_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    print(f\"Full input tokens: {encoded.input_ids}\")\n",
    "    print(f\"Full input decoded: {tokenizer.decode(encoded.input_ids[0])}\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Get prompt length\n",
    "    prompt_encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    prompt_len = prompt_encoded.input_ids.shape[-1]\n",
    "    print(f\"Prompt tokens: {prompt_encoded.input_ids}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Prompt decoded: {tokenizer.decode(prompt_encoded.input_ids[0])}\")\n",
    "    \n",
    "    # Show what tokens we're computing loss on\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :prompt_len] = -100\n",
    "    print(f\"Labels (masked): {labels}\")\n",
    "    \n",
    "    # Show which tokens will be used for loss computation\n",
    "    completion_tokens = input_ids[:, prompt_len:]\n",
    "    print(f\"Completion tokens (for loss): {completion_tokens}\")\n",
    "    print(f\"Completion decoded: {tokenizer.decode(completion_tokens[0])}\")\n",
    "    \n",
    "    # Model forward pass with debugging\n",
    "    print(f\"Model device: {device}\")\n",
    "    print(f\"Input shape: {input_ids.shape}\")\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():  # Don't track gradients for debugging\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            print(f\"Raw loss: {outputs.loss}\")\n",
    "            print(f\"Loss device: {outputs.loss.device}\")\n",
    "            print(f\"Loss dtype: {outputs.loss.dtype}\")\n",
    "            print(f\"Is loss NaN: {torch.isnan(outputs.loss)}\")\n",
    "            print(f\"Is loss infinite: {torch.isinf(outputs.loss)}\")\n",
    "            \n",
    "            if hasattr(outputs, 'logits'):\n",
    "                print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "                print(f\"Logits range: {outputs.logits.min():.4f} to {outputs.logits.max():.4f}\")\n",
    "                \n",
    "            logp = -outputs.loss\n",
    "            print(f\"Log probability: {logp}\")\n",
    "            \n",
    "            return logp\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in model forward pass: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def test_simple_cases():\n",
    "    \"\"\"Test very simple cases to isolate the issue\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TESTING SIMPLE CASES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"\", \"hello\"),  # No prompt\n",
    "        (\"Q:\", \"A\"),    # Very short\n",
    "        (\"Query:\", \"search\"),  # Simple case\n",
    "    ]\n",
    "    \n",
    "    for prompt, completion in test_cases:\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        result = debug_compute_logp(prompt, completion)\n",
    "        if result is not None:\n",
    "            print(f\"‚úÖ Success! Log prob: {result.item():.4f}\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed!\")\n",
    "\n",
    "def check_model_state():\n",
    "    \"\"\"Check if model is in correct state\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CHECKING MODEL STATE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"Model type: {type(model)}\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "    print(f\"Model training mode: {model.training}\")\n",
    "    print(f\"Model vocab size: {model.config.vocab_size}\")\n",
    "    print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    # Test simple generation\n",
    "    try:\n",
    "        test_input = \"Hello\"\n",
    "        inputs = tokenizer(test_input, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            print(f\"‚úÖ Model forward pass works\")\n",
    "            print(f\"Output logits shape: {outputs.logits.shape}\")\n",
    "            print(f\"Output logits range: {outputs.logits.min():.4f} to {outputs.logits.max():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model forward pass failed: {e}\")\n",
    "\n",
    "# Run diagnostics\n",
    "check_model_state()\n",
    "test_simple_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjEizTQivAvQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reloading model with proper configuration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa8e8bce1634cf4b3cdb5f9c625d3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fce36dd83541e4a31618beda44b1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02885c751a154734a1dc7d03e15e39fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6962cb1604e24881aa1ea003e85a0799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f94b7c75bf24388a0db7944bf9d2af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12c946c7f2b4adfa1a4d6c7df548b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0048f95d8a427a9259890b85184491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model gpt2 loaded successfully!\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.float32\n",
      "Test logits shape: torch.Size([1, 2, 50257])\n",
      "Test logits range: -114.9173 to -75.9695\n",
      "Any NaN in logits: False\n",
      "Any Inf in logits: False\n",
      "‚úÖ Model is working correctly!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix the NaN logits issue - run this cell to reload the model properly\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Clear any existing model from memory\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "\n",
    "\n",
    "print(\"üîÑ Reloading model with proper configuration...\")\n",
    "\n",
    "# Use GPT2 instead of DialoGPT for better stability\n",
    "model_name = \"gpt2\"  # More stable than DialoGPT-medium\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Load model with FLOAT32 precision to avoid NaN issues\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Use float32 instead of float16\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(f\"‚úÖ Model {model_name} loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "# Test the model to ensure it works\n",
    "def test_model_sanity():\n",
    "    \"\"\"Quick test to ensure model produces valid outputs\"\"\"\n",
    "    test_input = \"Hello world\"\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    print(f\"Test logits shape: {logits.shape}\")\n",
    "    print(f\"Test logits range: {logits.min():.4f} to {logits.max():.4f}\")\n",
    "    print(f\"Any NaN in logits: {torch.isnan(logits).any()}\")\n",
    "    print(f\"Any Inf in logits: {torch.isinf(logits).any()}\")\n",
    "    \n",
    "    if not torch.isnan(logits).any() and not torch.isinf(logits).any():\n",
    "        print(\"‚úÖ Model is working correctly!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Model still has issues!\")\n",
    "        return False\n",
    "\n",
    "test_model_sanity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING FIXED compute_logp FUNCTION\n",
      "================================================================================\n",
      "\n",
      "Prompt: ''\n",
      "Completion: 'hello'\n",
      "‚ö†Ô∏è Invalid loss detected: nan\n",
      "Log probability: 0.0000\n",
      "Probability: 1.000000\n",
      "‚úÖ Success!\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: 'Query: '\n",
      "Completion: 'machine learning'\n",
      "Log probability: -6.5192\n",
      "Probability: 0.001475\n",
      "‚úÖ Success!\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: 'Question: What is AI?\n",
      "Answer: '\n",
      "Completion: 'Artificial Intelligence'\n",
      "Log probability: -0.5831\n",
      "Probability: 0.558155\n",
      "‚úÖ Success!\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: 'Search: '\n",
      "Completion: 'python tutorial'\n",
      "Log probability: -9.3671\n",
      "Probability: 0.000085\n",
      "‚úÖ Success!\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now test the fixed compute_logp function\n",
    "def fixed_compute_logp(prompt, completion):\n",
    "    \"\"\"Fixed version of compute_logp\"\"\"\n",
    "    full_input = prompt + completion\n",
    "    encoded = tokenizer(full_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "    # Get prompt length more carefully\n",
    "    prompt_encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    prompt_len = prompt_encoded.input_ids.shape[-1]\n",
    "\n",
    "    # Create labels for loss computation\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :prompt_len] = -100  # Mask prompt tokens\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    # Check for valid loss\n",
    "    if torch.isnan(outputs.loss) or torch.isinf(outputs.loss):\n",
    "        print(f\"‚ö†Ô∏è Invalid loss detected: {outputs.loss}\")\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    return -outputs.loss\n",
    "\n",
    "# Test the fixed function\n",
    "def test_fixed_compute_logp():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TESTING FIXED compute_logp FUNCTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"\", \"hello\"),\n",
    "        (\"Query: \", \"machine learning\"),\n",
    "        (\"Question: What is AI?\\nAnswer: \", \"Artificial Intelligence\"),\n",
    "        (\"Search: \", \"python tutorial\"),\n",
    "    ]\n",
    "    \n",
    "    for prompt, completion in test_cases:\n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(f\"Completion: '{completion}'\")\n",
    "        \n",
    "        try:\n",
    "            logp = fixed_compute_logp(prompt, completion)\n",
    "            print(f\"Log probability: {logp.item():.4f}\")\n",
    "            print(f\"Probability: {torch.exp(logp).item():.6f}\")\n",
    "            print(\"‚úÖ Success!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "test_fixed_compute_logp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting IPO training with fixed compute_logp...\n",
      "Dataset size: 70073\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1/70073 [00:00<2:07:22,  9.17it/s, loss=101.9563, batch_loss=107.8358, valid_batches=2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0:\n",
      "Preferred: 'Query:location Oksan Station East Asian country' ‚Üí logp: -3.2360\n",
      "Dispreferred: '(‰∏≠Âúã /‰∏≠ÂõΩ ) in its native language.\n",
      "Matsu Daily () is a newspaper owned by the government of the Lienchiang County, Fujian Province, Republic of China, an East Asian country which is commonly known by its largest island Taiwan.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Oksan Station is located in which East Asian country? \n",
      "Query:Oksan Station East Asian country\n",
      "\n",
      "Context' ‚Üí logp: -3.4341\n",
      "\n",
      "Batch 1:\n",
      "Preferred: 'Examples:\n",
      "Question:Henry of Almain was the song of one of the wealthiest men in Europe who joined what?\n",
      "Query:Henry of Almain father wealthiest man Europe joined what\n",
      "\n",
      "Context:\n",
      " The song was written by group members Van Stephenson and Dave Robbins, along with Desmond Child.\n",
      " The song was written by group members Dave Robbins and Henry Paul, along with Lee Thomas Miller.\n",
      " Urban plays guitar on the Dixie Chicks' rendition.\n",
      "\"Some Days You Gotta Dance\" is a song written by Troy Johnson and Marshall Morgan, and recorded by American country music group Dixie Chicks.\n",
      " \"Some Days You Gotta Dance\" was previously recorded by Keith Urban's band, The Ranch, in 1997.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What member of the country music group Blackhawk plays guitar and also helped write the song \"Days of America\"?   More\n",
      "\n",
      "Examples:\n",
      "Question:Henry of Almain was the' ‚Üí logp: -4.8093\n",
      "Dispreferred: 'Query:Blackhawk country music group guitar member song \"Days' ‚Üí logp: -4.4249\n",
      "\n",
      "Batch 2:\n",
      "Preferred: 'Examples:\n",
      "Question:In which city in Missouri is this public land-grant research university located for which Thomas Kelley served as the head coach in the 1920s?\n",
      "Query:Thomas Kelley 1920s head coach Missouri public land-grant research university city\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Which company produced the 1968 crime drama film in which the wife of Buck Owens played a role? \n",
      "Query:1968 crime drama film Buck Owens wife company' ‚Üí logp: -3.2614\n",
      "Dispreferred: 'Examples:\n",
      "Question:Who is the wife of Rene Angelil?\n",
      "Query:Rene Angelil wife\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Which company produced the 1968 crime drama film in which the wife of Buck Owens played a role? Query:Company 1968 film Buck Owens wife\n",
      "\n",
      "Generate' ‚Üí logp: -2.8169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   3%|‚ñé         | 2232/70073 [02:24<1:12:58, 15.49it/s, loss=96.9433, batch_loss=258.4277, valid_batches=2232]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m utils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 100\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m valid_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Update progress bar with current loss\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Updated training code with fixed compute_logp function\n",
    "\n",
    "dataset_path = 'preference_dataset_hotpotqa_final.json'\n",
    "dataset = PreferenceDataset(dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "tau = 0.05\n",
    "num_epochs = 1\n",
    "\n",
    "def compute_logp(prompt, completion):\n",
    "    \"\"\"Fixed version of compute_logp with NaN handling\"\"\"\n",
    "    full_input = prompt + completion\n",
    "    encoded = tokenizer(full_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "    prompt_encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    prompt_len = prompt_encoded.input_ids.shape[-1]\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :prompt_len] = -100\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    # Check for valid loss\n",
    "    if torch.isnan(outputs.loss) or torch.isinf(outputs.loss):\n",
    "        print(f\"‚ö†Ô∏è Invalid loss detected for: '{completion[:50]}...'\")\n",
    "        return torch.tensor(-10.0, device=device, requires_grad=True)  # Use -10 instead of 0\n",
    "    \n",
    "    return -outputs.loss\n",
    "\n",
    "def ipo_loss(logp_win, logp_lose, tau=0.05):\n",
    "    \"\"\"IPO loss with additional safety checks\"\"\"\n",
    "    # Clamp values to prevent extreme differences\n",
    "    logp_win = torch.clamp(logp_win, min=-15, max=5)\n",
    "    logp_lose = torch.clamp(logp_lose, min=-15, max=5)\n",
    "    \n",
    "    diff = logp_win - logp_lose - 0.5 / tau\n",
    "    loss = (diff ** 2).mean()\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        print(f\"‚ö†Ô∏è NaN in IPO loss computation\")\n",
    "        return torch.tensor(1.0, device=logp_win.device, requires_grad=True)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"üöÄ Starting IPO training with fixed compute_logp...\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    valid_batches = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        questions = batch[\"question\"]\n",
    "        preferred = batch[\"preferred\"] \n",
    "        dispreferred = batch[\"dispreferred\"]\n",
    "\n",
    "        logp_w_list = []\n",
    "        logp_l_list = []\n",
    "\n",
    "        for q, w, l in zip(questions, preferred, dispreferred):\n",
    "            # Use a more informative prompt\n",
    "            prompt = f\"Generate a search query for: {q[:100]}...\\nQuery: \"\n",
    "            \n",
    "            try:\n",
    "                logp_w = compute_logp(prompt, w.strip())\n",
    "                logp_l = compute_logp(prompt, l.strip())\n",
    "                \n",
    "                logp_w_list.append(logp_w)\n",
    "                logp_l_list.append(logp_l)\n",
    "                \n",
    "                # Optional: Print first few examples to see what's happening\n",
    "                if batch_idx < 3:\n",
    "                    print(f\"\\nBatch {batch_idx}:\")\n",
    "                    print(f\"Preferred: '{w.strip()}' ‚Üí logp: {logp_w.item():.4f}\")\n",
    "                    print(f\"Dispreferred: '{l.strip()}' ‚Üí logp: {logp_l.item():.4f}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if logp_w_list and logp_l_list:\n",
    "            logp_w_batch = torch.stack(logp_w_list)\n",
    "            logp_l_batch = torch.stack(logp_l_list)\n",
    "            \n",
    "            loss = ipo_loss(logp_w_batch, logp_l_batch, tau)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            valid_batches += 1\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            avg_loss = total_loss / valid_batches\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{avg_loss:.4f}\",\n",
    "                \"batch_loss\": f\"{loss.item():.4f}\",\n",
    "                \"valid_batches\": valid_batches\n",
    "            })\n",
    "            \n",
    "            # Clear memory\n",
    "            del logp_w_list, logp_l_list, logp_w_batch, logp_l_batch, loss\n",
    "            \n",
    "        # Periodic memory cleanup\n",
    "        if batch_idx % 10 == 0:\n",
    "            clear_cuda_cache()\n",
    "\n",
    "    final_avg_loss = total_loss / max(valid_batches, 1)\n",
    "    print(f\"\\n[Epoch {epoch + 1}] Average Loss: {final_avg_loss:.4f}\")\n",
    "    print(f\"Valid batches processed: {valid_batches}/{len(dataloader)}\")\n",
    "\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07e433061b02424fb88975c726d63c86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cafa7349eef4b7b8cc5253802943b9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15820aef970b43cca7266993df78365e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32be6281e7e3404685b4609ea6dc377f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3335fb1d97f349d39301a592503226c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be53745e6e034ce09d34e2268820684a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c3764dc8905d48cb8537ddd68ce72064",
      "value": ""
     }
    },
    "33fab839b102417e99e8ec9204550cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2e142305b1a4cf1941c19d03bda6c62",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_32be6281e7e3404685b4609ea6dc377f",
      "value": "‚Äá0/0‚Äá[00:00&lt;?,‚Äá?it/s]"
     }
    },
    "361d3827ea954df8b7e31748251153db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49329b09131c4a9da35b4ab8d68bf7e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "539cef24281245139a1002c0f1c53fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07e433061b02424fb88975c726d63c86",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_361d3827ea954df8b7e31748251153db",
      "value": 4
     }
    },
    "59dfdf60a56d4c4dacf5b37260c50d3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe5493fd0ba247ec96d735befed62fd0",
       "IPY_MODEL_539cef24281245139a1002c0f1c53fea",
       "IPY_MODEL_f2a65985f11c4b07965612499dc807a9"
      ],
      "layout": "IPY_MODEL_15820aef970b43cca7266993df78365e"
     }
    },
    "756d9e19eada4f15a9a5d24669211455": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "85f92b4750214608ac3c7ff809ea9aec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_756d9e19eada4f15a9a5d24669211455",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbf2a60b9ab54eba868d3e8f99f92255",
      "value": 0
     }
    },
    "99247cb0e0fb420b84452bd4f6151cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ceec0fc7824ddc8c56290aed1f0c35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be53745e6e034ce09d34e2268820684a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2e142305b1a4cf1941c19d03bda6c62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3764dc8905d48cb8537ddd68ce72064": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7235724003a49c29c704c89305cab8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5e7917c4a364b868179019231ae1bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3335fb1d97f349d39301a592503226c1",
       "IPY_MODEL_85f92b4750214608ac3c7ff809ea9aec",
       "IPY_MODEL_33fab839b102417e99e8ec9204550cc2"
      ],
      "layout": "IPY_MODEL_0cafa7349eef4b7b8cc5253802943b9a"
     }
    },
    "f2a65985f11c4b07965612499dc807a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49329b09131c4a9da35b4ab8d68bf7e0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c7235724003a49c29c704c89305cab8a",
      "value": "‚Äá4/4‚Äá[00:05&lt;00:00,‚Äá‚Äá1.23s/it]"
     }
    },
    "fbf2a60b9ab54eba868d3e8f99f92255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe5493fd0ba247ec96d735befed62fd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9ceec0fc7824ddc8c56290aed1f0c35",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_99247cb0e0fb420b84452bd4f6151cca",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
