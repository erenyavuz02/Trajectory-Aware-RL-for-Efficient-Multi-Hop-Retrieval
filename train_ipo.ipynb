{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login wandb project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1749473049873,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "TS4oVS6DkUPZ",
    "outputId": "d70ca38e-45ba-4946-8927-b621c703c948"
   },
   "outputs": [],
   "source": [
    "#!pip install -U transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount google drive if using colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "e5e7917c4a364b868179019231ae1bff",
      "3335fb1d97f349d39301a592503226c1",
      "85f92b4750214608ac3c7ff809ea9aec",
      "33fab839b102417e99e8ec9204550cc2",
      "0cafa7349eef4b7b8cc5253802943b9a",
      "be53745e6e034ce09d34e2268820684a",
      "c3764dc8905d48cb8537ddd68ce72064",
      "756d9e19eada4f15a9a5d24669211455",
      "fbf2a60b9ab54eba868d3e8f99f92255",
      "c2e142305b1a4cf1941c19d03bda6c62",
      "32be6281e7e3404685b4609ea6dc377f"
     ]
    },
    "executionInfo": {
     "elapsed": 3346,
     "status": "ok",
     "timestamp": 1749473075680,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "ShNkRy0xkUdh",
    "outputId": "714903c8-2b4a-4114-ecd5-202a2e535909"
   },
   "outputs": [],
   "source": [
    "# Add this import at the top of your model loading cell (cell 6)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.utils as utils  # Add this line\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        self.data = []\n",
    "        for question, entry in raw_data.items():\n",
    "            for hop, hop_data in entry[\"hops\"].items():\n",
    "                queries = hop_data[\"queries\"]\n",
    "                preferences = hop_data[\"preference_pairs\"]\n",
    "                for i, j in preferences:\n",
    "                    self.data.append({\n",
    "                        \"question\": question,\n",
    "                        \"preferred\": queries[i],\n",
    "                        \"dispreferred\": queries[j]\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2447,
     "status": "ok",
     "timestamp": 1749473084147,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "rFgSgz03s3id",
    "outputId": "dd8c5653-6b11-493e-cf57-5759aace64ad"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "59dfdf60a56d4c4dacf5b37260c50d3b",
      "fe5493fd0ba247ec96d735befed62fd0",
      "539cef24281245139a1002c0f1c53fea",
      "f2a65985f11c4b07965612499dc807a9",
      "15820aef970b43cca7266993df78365e",
      "b9ceec0fc7824ddc8c56290aed1f0c35",
      "99247cb0e0fb420b84452bd4f6151cca",
      "07e433061b02424fb88975c726d63c86",
      "361d3827ea954df8b7e31748251153db",
      "49329b09131c4a9da35b4ab8d68bf7e0",
      "c7235724003a49c29c704c89305cab8a"
     ]
    },
    "executionInfo": {
     "elapsed": 8096,
     "status": "ok",
     "timestamp": 1749473356966,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "a8bV7D60kZaH",
    "outputId": "fee635c4-24ec-4b91-f339-b98c9d4007c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model microsoft/DialoGPT-medium loaded successfully!\n",
      "Model is on device: cuda:0\n",
      "GPU memory allocated: 0.73 GB\n"
     ]
    }
   ],
   "source": [
    "login(\"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\")\n",
    "\n",
    "# Use a smaller model to avoid memory issues\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # Much smaller than LLaMA-3-8B\n",
    "# Alternative: \"distilbert-base-uncased\" or \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Load model with more memory-efficient settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Let it decide device placement\n",
    "    low_cpu_mem_usage=True,  # More memory efficient loading\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    \"model\": model_name,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"lr\": 5e-6,\n",
    "    \"tau\": 0.05,\n",
    "    \"batch_size\": 1,  # Reduced batch size\n",
    "    \"epochs\": 3,\n",
    "}\n",
    "\n",
    "# === Helpers ===\n",
    "def safe_ipo_loss(logp_win, logp_lose, tau=0.05):\n",
    "    \"\"\"Safe IPO loss with NaN detection and clipping\"\"\"\n",
    "    # Ensure inputs are valid\n",
    "    if torch.isnan(logp_win).any() or torch.isnan(logp_lose).any():\n",
    "        print(f\"NaN detected in logp: win={logp_win.item()}, lose={logp_lose.item()}\")\n",
    "        return torch.tensor(0.0, device=logp_win.device, requires_grad=True)\n",
    "    \n",
    "    # Clip extreme values\n",
    "    logp_win = torch.clamp(logp_win, min=-10, max=10)\n",
    "    logp_lose = torch.clamp(logp_lose, min=-10, max=10)\n",
    "    \n",
    "    # Compute IPO loss with safety checks\n",
    "    diff = logp_win - logp_lose - 0.5 / tau\n",
    "    loss = (diff ** 2).mean()\n",
    "    \n",
    "    # Check for NaN in loss\n",
    "    if torch.isnan(loss):\n",
    "        print(f\"NaN in loss computation: diff={diff.item()}\")\n",
    "        return torch.tensor(0.0, device=logp_win.device, requires_grad=True)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def safe_compute_logp(prompt, completion):\n",
    "    \"\"\"Safe log probability computation with error handling\"\"\"\n",
    "    try:\n",
    "        full_input = prompt + completion\n",
    "        \n",
    "        # Shorter sequences to avoid issues\n",
    "        encoded = tokenizer(full_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "        # Get prompt length\n",
    "        prompt_encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "        prompt_len = prompt_encoded.input_ids.shape[-1]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:, :prompt_len] = -100  # mask out prompt\n",
    "\n",
    "        # Use full precision for stability\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "        logp = -outputs.loss\n",
    "        \n",
    "        # Safety checks\n",
    "        if torch.isnan(logp) or torch.isinf(logp):\n",
    "            print(f\"Invalid logp: {logp.item()}\")\n",
    "            return torch.tensor(-1.0, device=device, requires_grad=True)\n",
    "            \n",
    "        return logp.detach().requires_grad_(True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_logp: {e}\")\n",
    "        return torch.tensor(-1.0, device=next(model.parameters()).device, requires_grad=True)\n",
    "\n",
    "        \n",
    "print(f\"Model {model_name} loaded successfully!\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "error",
     "timestamp": 1749473433469,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "N4SISxZekb9_",
    "outputId": "806a5d55-635a-4565-fd3b-b34bc053c867"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'preference_dataset_hotpotqa_final.json'\n",
    "dataset = PreferenceDataset(dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "tau = 0.05\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    valid_batches = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"SAFE Epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        try:\n",
    "            questions = batch[\"question\"]\n",
    "            preferred = batch[\"preferred\"] \n",
    "            dispreferred = batch[\"dispreferred\"]\n",
    "\n",
    "            logp_w_list, logp_l_list = [], []\n",
    "\n",
    "            for q, w, l in zip(questions, preferred, dispreferred):\n",
    "                # Clean the queries (remove extra formatting)\n",
    "                w_clean = w.strip()\n",
    "                l_clean = l.strip()\n",
    "                \n",
    "                prompt = f\"Query: \"\n",
    "                \n",
    "                print(f\"Batch {batch_idx}: Computing logp...\")\n",
    "                logp_w = safe_compute_logp(prompt, w_clean)\n",
    "                logp_l = safe_compute_logp(prompt, l_clean)\n",
    "                \n",
    "                print(f\"  Preferred logp: {logp_w.item():.4f}\")\n",
    "                print(f\"  Dispreferred logp: {logp_l.item():.4f}\")\n",
    "                \n",
    "                logp_w_list.append(logp_w)\n",
    "                logp_l_list.append(logp_l)\n",
    "\n",
    "            if logp_w_list and logp_l_list:\n",
    "                logp_w_batch = torch.stack(logp_w_list)\n",
    "                logp_l_batch = torch.stack(logp_l_list)\n",
    "                \n",
    "                loss = safe_ipo_loss(logp_w_batch, logp_l_batch, tau)\n",
    "                \n",
    "                print(f\"  Computed loss: {loss.item():.6f}\")\n",
    "                \n",
    "                # Skip if loss is 0 (safety fallback)\n",
    "                if loss.item() > 0:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping for stability\n",
    "                    utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                    \n",
    "                    print(f\"  Backprop completed successfully\")\n",
    "                else:\n",
    "                    print(f\"  Skipped batch due to zero loss\")\n",
    "                \n",
    "                # Clean up\n",
    "                del logp_w_list, logp_l_list, logp_w_batch, logp_l_batch, loss\n",
    "\n",
    "            clear_cuda_cache()\n",
    "\n",
    "            if valid_batches > 0:\n",
    "                avg_loss = total_loss / valid_batches\n",
    "                pbar.set_postfix({\"loss\": avg_loss, \"valid_batches\": valid_batches})\n",
    "                print(f\"Average loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            # Stop after a few batches for testing\n",
    "            if batch_idx >= 5:\n",
    "                print(f\"Stopping after {batch_idx+1} batches for testing\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            clear_cuda_cache()\n",
    "            continue\n",
    "\n",
    "    if valid_batches > 0:\n",
    "        final_avg = total_loss / valid_batches\n",
    "        print(f\"[Epoch {epoch + 1}] Valid batches: {valid_batches}, Average Loss: {final_avg:.6f}\")\n",
    "    else:\n",
    "        print(f\"[Epoch {epoch + 1}] No valid batches processed!\")\n",
    "    \n",
    "    clear_cuda_cache()\n",
    "\n",
    "print(\"SAFE training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n",
      "Starting test run - will stop after 1 iteration\n",
      "GPU memory before training: 0.74 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1:   0%|          | 2/70073 [00:00<1:03:41, 18.33it/s, loss=76]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0\n",
      "Question: What is the name of the engagement on the northern coast of France on 19 August 1942 in which the HM...\n",
      "Preferred: Query: HMS Calpe engagement northern coast France 19\n",
      "Dispreferred: launch point for a fleet of small boats that attacked a French convoy in the Gironde.\n",
      " In 1805 he sighted the French fleet under Admiral Ganteaume attempting to escape and warned the Offshore Squadron, who drove the French back into Brest in a brief engagement.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What is the name of the engagement on the northern coast of France on 19 August 1942 in which the HMS \"Calpe\" participated? \n",
      "Query:engagement northern coast France 1942 HMS\n",
      "Computing logp for preferred: Query: HMS Calpe engagement northern coast France 19\n",
      "Preferred logp: -6.4026\n",
      "Computing logp for dispreferred: launch point for a fleet of small boats that attacked a French convoy in the Gironde.\n",
      " In 1805 he sighted the French fleet under Admiral Ganteaume attempting to escape and warned the Offshore Squadron, who drove the French back into Brest in a brief engagement.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What is the name of the engagement on the northern coast of France on 19 August 1942 in which the HMS \"Calpe\" participated? \n",
      "Query:engagement northern coast France 1942 HMS\n",
      "Dispreferred logp: -7.4198\n",
      "Batch 0 completed successfully\n",
      "IPO Loss: 80.6909\n",
      "Loss computed and backprop completed: 80.6909\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 80.690926\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 1\n",
      "Question: What retired American soccer player is the head coach  of the team owned by Anthony Precourt and Pre...\n",
      "Preferred: the head coach  of the team owned by Anthony Precourt and Precourt Sports Ventures LLC? Query:retired American soccer player head coach owned by Anthony\n",
      "Dispreferred: a retired American soccer player who played professionally in the North American Soccer League and is currently the head coach of the Connecticut College men's soccer team.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What retired American soccer player is the head coach  of the team owned by Anthony Precourt and Precourt Sports Ventures LLC? \n",
      "Query:retired American soccer player head coach Major League\n",
      "Computing logp for preferred: the head coach  of the team owned by Anthony Precourt and Precourt Sports Ventures LLC? Query:retired American soccer player head coach owned by Anthony\n",
      "Preferred logp: -4.2269\n",
      "Computing logp for dispreferred: a retired American soccer player who played professionally in the North American Soccer League and is currently the head coach of the Connecticut College men's soccer team.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What retired American soccer player is the head coach  of the team owned by Anthony Precourt and Precourt Sports Ventures LLC? \n",
      "Query:retired American soccer player head coach Major League\n",
      "Dispreferred logp: -4.8975\n",
      "Batch 1 completed successfully\n",
      "IPO Loss: 87.0373\n",
      "Loss computed and backprop completed: 87.0373\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 83.864109\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 2\n",
      "Question: In which of the 13 countries that Telenor owns networks is Grameen Telecom located?...\n",
      "Preferred: which of the 13 countries that Telenor owns networks is Grameen Telecom located? \n",
      "Query: Telenor countries Grameen Telecom location\n",
      "Dispreferred: % subscriber market share (as of August 2016), Grameenphone is the largest mobile phone operator in the country.\n",
      " Currently Grameen Telecom provides mobile phones among the villagers of the country.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "In which of the 13 countries that Telenor owns networks is Grameen Telecom located? \n",
      "\n",
      "Query: Telenor 13 countries Grameen\n",
      "Computing logp for preferred: which of the 13 countries that Telenor owns networks is Grameen Telecom located? \n",
      "Query: Telenor countries Grameen Telecom location\n",
      "Preferred logp: -2.9552\n",
      "Computing logp for dispreferred: % subscriber market share (as of August 2016), Grameenphone is the largest mobile phone operator in the country.\n",
      " Currently Grameen Telecom provides mobile phones among the villagers of the country.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "In which of the 13 countries that Telenor owns networks is Grameen Telecom located? \n",
      "\n",
      "Query: Telenor 13 countries Grameen\n",
      "Dispreferred logp: -5.9497\n",
      "Batch 2 completed successfully\n",
      "IPO Loss: 49.0776\n",
      "Loss computed and backprop completed: 49.0776\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 72.268613\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 3\n",
      "Question: What locations is the folklore which Huldra belongs to representative of?...\n",
      "Preferred: Examples:\n",
      "Question:James Wolk is an American actor in a drama series based on a novel by who?\n",
      "Query:James Wolk drama series based on novel by\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What locations is the folklore which Huldra belongs to representative of? (Example: The folklore which Huldra belongs to is\n",
      "Dispreferred: Examples:\n",
      "Question:What action does Moves and The Artist's Magazine have in common?\n",
      "Query:Moves magazine and The Artist's Magazine common action\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What locations is the folklore which Huldra belongs to representative of? \n",
      "Query: Folklore location Huldra\n",
      "\n",
      "Example query\n",
      "Computing logp for preferred: Examples:\n",
      "Question:James Wolk is an American actor in a drama series based on a novel by who?\n",
      "Query:James Wolk drama series based on novel by\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What locations is the folklore which Huldra belongs to representative of? (Example: The folklore which Huldra belongs to is\n",
      "Preferred logp: -5.7258\n",
      "Computing logp for dispreferred: Examples:\n",
      "Question:What action does Moves and The Artist's Magazine have in common?\n",
      "Query:Moves magazine and The Artist's Magazine common action\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What locations is the folklore which Huldra belongs to representative of? \n",
      "Query: Folklore location Huldra\n",
      "\n",
      "Example query\n",
      "Dispreferred logp: -6.3915\n",
      "Batch 3 completed successfully\n",
      "IPO Loss: 87.1287\n",
      "Loss computed and backprop completed: 87.1287\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 75.983633\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 4\n",
      "Question: Could Ann Beattie and  Edgar Rice Burroughs be dating?...\n",
      "Preferred: and science-fiction genres.\n",
      "Carson of Venus is the third book in the Venus series (Sometimes called the \"Carson Napier of Venus series\") by American writer Edgar Rice Burroughs.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Could Ann Beattie and  Edgar Rice Burroughs be dating? (There is no evidence that they are or were dating,\n",
      "Dispreferred: .\n",
      "Carson of Venus is the third book in the Venus series (Sometimes called the \"Carson Napier of Venus series\") by American writer Edgar Rice Burroughs.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Could Ann Beattie and  Edgar Rice Burroughs be dating? What is the time period for the two writers to be in\n",
      "Computing logp for preferred: and science-fiction genres.\n",
      "Carson of Venus is the third book in the Venus series (Sometimes called the \"Carson Napier of Venus series\") by American writer Edgar Rice Burroughs.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Could Ann Beattie and  Edgar Rice Burroughs be dating? (There is no evidence that they are or were dating,\n",
      "Preferred logp: -6.4234\n",
      "Computing logp for dispreferred: .\n",
      "Carson of Venus is the third book in the Venus series (Sometimes called the \"Carson Napier of Venus series\") by American writer Edgar Rice Burroughs.\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Could Ann Beattie and  Edgar Rice Burroughs be dating? What is the time period for the two writers to be in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1:   0%|          | 8/70073 [00:00<49:09, 23.75it/s, loss=93.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispreferred logp: -5.6678\n",
      "Batch 4 completed successfully\n",
      "IPO Loss: 115.6833\n",
      "Loss computed and backprop completed: 115.6833\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 83.923558\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 5\n",
      "Question: What type of gesture does Donets-Krivoy Rog Soviet Republic and have in common?...\n",
      "Preferred: Examples:\n",
      "Question:Comedian Frank Gorshin was a member of The Kopykats, a group of comedians on which television variety show in 1972?\n",
      "Query:Frank Gorshin The Kopykats television variety show 1972\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What type of gesture does Donets-Krivoy Rog Soviet Republic and have in common? Donets-Krivoy Rog Soviet Republic and the People's\n",
      "Dispreferred: (The answer is a flag.)\n",
      "\n",
      "Query:Donets-K\n",
      "Computing logp for preferred: Examples:\n",
      "Question:Comedian Frank Gorshin was a member of The Kopykats, a group of comedians on which television variety show in 1972?\n",
      "Query:Frank Gorshin The Kopykats television variety show 1972\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What type of gesture does Donets-Krivoy Rog Soviet Republic and have in common? Donets-Krivoy Rog Soviet Republic and the People's\n",
      "Preferred logp: -5.6166\n",
      "Computing logp for dispreferred: (The answer is a flag.)\n",
      "\n",
      "Query:Donets-K\n",
      "Dispreferred logp: -9.1457\n",
      "Batch 5 completed successfully\n",
      "IPO Loss: 41.8735\n",
      "Loss computed and backprop completed: 41.8735\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 76.915209\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 6\n",
      "Question: John Martin & Co. is responsible for the pageant that has been known as what since 1996?...\n",
      "Preferred: Query:John Martin & Co. pageant 1996\n",
      "Dispreferred: Examples:\n",
      "Question:What 2017 film about Mozart starred the actor who created the spoof heavy metal band Bad News?\n",
      "Query:2017 Mozart film actor Bad News band\n",
      "\n",
      "Generate a search query for the following question:\n",
      "John Martin & Co. is responsible for the pageant that has been known as what since 1996? \n",
      "Query:John Martin & Co. pageant name\n",
      "Computing logp for preferred: Query:John Martin & Co. pageant 1996\n",
      "Preferred logp: -6.3292\n",
      "Computing logp for dispreferred: Examples:\n",
      "Question:What 2017 film about Mozart starred the actor who created the spoof heavy metal band Bad News?\n",
      "Query:2017 Mozart film actor Bad News band\n",
      "\n",
      "Generate a search query for the following question:\n",
      "John Martin & Co. is responsible for the pageant that has been known as what since 1996? \n",
      "Query:John Martin & Co. pageant name\n",
      "Dispreferred logp: -6.8442\n",
      "Batch 6 completed successfully\n",
      "IPO Loss: 89.9669\n",
      "Loss computed and backprop completed: 89.9669\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 78.779732\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 7\n",
      "Question: Who funds the bowling team that includes the school bus driver for Springfield Elementary School?...\n",
      "Preferred: Examples:\n",
      "Question:Lee Hwan-kyung debuted with a movie written by who?\n",
      "Query:Lee Hwan-kyung debut movie written by whom\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who funds the bowling team that includes the school bus driver for Springfield Elementary School? (Reference: The Simpsons)\n",
      "Query:Who funds the bowling\n",
      "Dispreferred: Examples:\n",
      "Question:Which has more species, Calendula or Purshia?\n",
      "Query:Calendula vs Purshia number of species\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who funds the bowling team that includes the school bus driver for Springfield Elementary School? \n",
      "Query:bowling team school bus driver springfield elementary\n",
      "Computing logp for preferred: Examples:\n",
      "Question:Lee Hwan-kyung debuted with a movie written by who?\n",
      "Query:Lee Hwan-kyung debut movie written by whom\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who funds the bowling team that includes the school bus driver for Springfield Elementary School? (Reference: The Simpsons)\n",
      "Query:Who funds the bowling\n",
      "Preferred logp: -6.9694\n",
      "Computing logp for dispreferred: Examples:\n",
      "Question:Which has more species, Calendula or Purshia?\n",
      "Query:Calendula vs Purshia number of species\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who funds the bowling team that includes the school bus driver for Springfield Elementary School? \n",
      "Query:bowling team school bus driver springfield elementary\n",
      "Dispreferred logp: -5.8426\n",
      "Batch 7 completed successfully\n",
      "IPO Loss: 123.8067\n",
      "Loss computed and backprop completed: 123.8067\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 84.408104\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 8\n",
      "Question: What ideas did this Italian philosopher, poet, essayist, and philologist, who wrote the poem L'infin...\n",
      "Preferred: in the translation of the song Valencia which was included in a silent film with the same name?\n",
      "Query:French inventor scientist mathematician balloonist Valencia song silent film\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What ideas did this Italian philosopher, poet, essayist, and philologist, who wrote the poem L'infinito, come in touch with while living in a secluded town in the conservative Papal States? \n",
      "\n",
      "Query:Italian philosopher poet essayist philologist L'\n",
      "Dispreferred: , who wrote the poem L'infinito, come in touch with while living in a secluded town in the conservative Papal States? (Hint: he was a key figure in the Romantic movement\n",
      "Computing logp for preferred: in the translation of the song Valencia which was included in a silent film with the same name?\n",
      "Query:French inventor scientist mathematician balloonist Valencia song silent film\n",
      "\n",
      "Generate a search query for the following question:\n",
      "What ideas did this Italian philosopher, poet, essayist, and philologist, who wrote the poem L'infinito, come in touch with while living in a secluded town in the conservative Papal States? \n",
      "\n",
      "Query:Italian philosopher poet essayist philologist L'\n",
      "Preferred logp: -6.7764\n",
      "Computing logp for dispreferred: , who wrote the poem L'infinito, come in touch with while living in a secluded town in the conservative Papal States? (Hint: he was a key figure in the Romantic movement\n",
      "Dispreferred logp: -3.8947\n",
      "Batch 8 completed successfully\n",
      "IPO Loss: 165.9383\n",
      "Loss computed and backprop completed: 165.9383\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 93.467012\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 9\n",
      "Question: This Is Animal Music is the debut studio album by Look Mexico, the album can easily be noted as a be...\n",
      "Preferred: , the album can easily be noted as a beginning for the band's commercial success, as it has sparked attention among major music news sources and has even got the band featured in Alternative Press as a band to look for in which year? \n",
      "Query:Look Mexico debut studio album commercial success major music\n",
      "Dispreferred: ane or her father Glen Keane?\n",
      "Query:Claire Keane vs Glen Keane Disney films\n",
      "\n",
      "Generate a search query for the following question:\n",
      "This Is Animal Music is the debut studio album by Look Mexico, the album can easily be noted as a beginning for the band's commercial success, as it has sparked attention among major music news sources and has even got the band featured in Alternative Press as a band to look for in which year? \n",
      "Query: Look Mexico debut album commercial success year \n",
      "\n",
      "Question\n",
      "Computing logp for preferred: , the album can easily be noted as a beginning for the band's commercial success, as it has sparked attention among major music news sources and has even got the band featured in Alternative Press as a band to look for in which year? \n",
      "Query:Look Mexico debut studio album commercial success major music\n",
      "Preferred logp: -5.6723\n",
      "Computing logp for dispreferred: ane or her father Glen Keane?\n",
      "Query:Claire Keane vs Glen Keane Disney films\n",
      "\n",
      "Generate a search query for the following question:\n",
      "This Is Animal Music is the debut studio album by Look Mexico, the album can easily be noted as a beginning for the band's commercial success, as it has sparked attention among major music news sources and has even got the band featured in Alternative Press as a band to look for in which year? \n",
      "Query: Look Mexico debut album commercial success year \n",
      "\n",
      "Question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1:   0%|          | 8/70073 [00:00<49:09, 23.75it/s, loss=92.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispreferred logp: -6.6928\n",
      "Batch 9 completed successfully\n",
      "IPO Loss: 80.6311\n",
      "Loss computed and backprop completed: 80.6311\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 92.183425\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "Processing batch 10\n",
      "Question: Who nominated the justice who wrote the majority opinion in the Supreme Court case Christopher v. Sm...\n",
      "Preferred: Question:Are Stuart Murdoch and Lee Sung-min both singers?\n",
      "Query:Are Stuart Murdoch and Lee Sung-min both singers?\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who nominated the justice who wrote the majority opinion in the Supreme Court case Christopher v. SmithKline Beecham Corp? \n",
      "Query:Who nominated the justice who wrote the majority opinion\n",
      "Dispreferred: genus has more species, Worsleya or Gordonia?\n",
      "Query:Worsleya vs Gordonia number of species\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who nominated the justice who wrote the majority opinion in the Supreme Court case Christopher v. SmithKline Beecham Corp? \n",
      "Query:Christopher v. SmithKline Beecham\n",
      "Computing logp for preferred: Question:Are Stuart Murdoch and Lee Sung-min both singers?\n",
      "Query:Are Stuart Murdoch and Lee Sung-min both singers?\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who nominated the justice who wrote the majority opinion in the Supreme Court case Christopher v. SmithKline Beecham Corp? \n",
      "Query:Who nominated the justice who wrote the majority opinion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1:   0%|          | 10/70073 [00:00<54:38, 21.37it/s, loss=93.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferred logp: -5.2667\n",
      "Computing logp for dispreferred: genus has more species, Worsleya or Gordonia?\n",
      "Query:Worsleya vs Gordonia number of species\n",
      "\n",
      "Generate a search query for the following question:\n",
      "Who nominated the justice who wrote the majority opinion in the Supreme Court case Christopher v. SmithKline Beecham Corp? \n",
      "Query:Christopher v. SmithKline Beecham\n",
      "Dispreferred logp: -5.0829\n",
      "Batch 10 completed successfully\n",
      "IPO Loss: 103.7095\n",
      "Loss computed and backprop completed: 103.7095\n",
      "CUDA cache cleared.\n",
      "GPU memory after batch: 0.74 GB\n",
      "Test loss: 93.231250\n",
      "TEST COMPLETE - Stopping after 1 iteration\n",
      "[Test Epoch 1] Average Loss: 1025.5438\n",
      "CUDA cache cleared.\n",
      "Test run completed successfully!\n",
      "Final GPU memory: 0.74 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === TEST TRAINING - Stop after 1 iteration ===\n",
    "dataset_path = 'preference_dataset_hotpotqa_final.json'\n",
    "dataset = PreferenceDataset(dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "tau = 0.05\n",
    "num_epochs = 1  # Only 1 epoch for testing\n",
    "\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print(f\"Starting test run - will stop after 1 iteration\")\n",
    "print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Define missing functions\n",
    "def compute_logp(prompt, completion):\n",
    "    \"\"\"Basic log probability computation\"\"\"\n",
    "    return safe_compute_logp(prompt, completion)\n",
    "\n",
    "def ipo_loss(logp_win, logp_lose, tau=0.05):\n",
    "    \"\"\"Basic IPO loss\"\"\"\n",
    "    return safe_ipo_loss(logp_win, logp_lose, tau)\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Test Epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        print(f\"Processing batch {batch_idx}\")\n",
    "        \n",
    "        questions = batch[\"question\"]\n",
    "        preferred = batch[\"preferred\"]\n",
    "        dispreferred = batch[\"dispreferred\"]\n",
    "        \n",
    "        print(f\"Question: {questions[0][:100]}...\")\n",
    "        print(f\"Preferred: {preferred[0]}\")\n",
    "        print(f\"Dispreferred: {dispreferred[0]}\")\n",
    "\n",
    "        logp_w_list, logp_l_list = [], []\n",
    "\n",
    "        for q, w, l in zip(questions, preferred, dispreferred):\n",
    "            try:\n",
    "                prompt = f\"Generate a search query for the following question:\\n{q}\\nQuery:\"\n",
    "                print(f\"Computing logp for preferred: {w}\")\n",
    "                logp_w = compute_logp(prompt, \" \" + w)\n",
    "                print(f\"Preferred logp: {logp_w.item():.4f}\")\n",
    "                \n",
    "                print(f\"Computing logp for dispreferred: {l}\")\n",
    "                logp_l = compute_logp(prompt, \" \" + l)\n",
    "                print(f\"Dispreferred logp: {logp_l.item():.4f}\")\n",
    "                \n",
    "                logp_w_list.append(logp_w)\n",
    "                logp_l_list.append(logp_l)\n",
    "                print(f\"Batch {batch_idx} completed successfully\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                clear_cuda_cache()\n",
    "                continue\n",
    "\n",
    "        if logp_w_list and logp_l_list:\n",
    "            logp_w_batch = torch.stack(logp_w_list)\n",
    "            logp_l_batch = torch.stack(logp_l_list)\n",
    "            loss = ipo_loss(logp_w_batch, logp_l_batch, tau)\n",
    "            \n",
    "            print(f\"IPO Loss: {loss.item():.4f}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Loss computed and backprop completed: {loss.item():.4f}\")\n",
    "            \n",
    "            # Clear intermediate tensors and cache\n",
    "            del logp_w_list, logp_l_list, logp_w_batch, logp_l_batch, loss\n",
    "            \n",
    "        clear_cuda_cache()\n",
    "        print(f\"GPU memory after batch: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "        avg_loss = total_loss / max(1, batch_idx + 1)\n",
    "        pbar.set_postfix({\"loss\": avg_loss})\n",
    "        print(f\"Test loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        # STOP AFTER FIRST ITERATION FOR TESTING\n",
    "        print(\"TEST COMPLETE - Stopping after 1 iteration\")\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    print(f\"[Test Epoch {epoch + 1}] Average Loss: {total_loss / max(1, 1):.4f}\")\n",
    "    clear_cuda_cache()\n",
    "\n",
    "print(\"Test run completed successfully!\")\n",
    "print(f\"Final GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1749473335432,
     "user": {
      "displayName": "EREN YAVUZ",
      "userId": "09376515966251765638"
     },
     "user_tz": -180
    },
    "id": "XQbXRzWqvBBU"
   },
   "outputs": [],
   "source": [
    "# prompt: free the cuda gpu memory aggresivley, atomic way, nuclear it : \"OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 76.88 MiB is free. Process 72946 has 39.47 GiB memory in use. Of the allocated memory 38.61 GiB is allocated by PyTorch, and 371.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "# \"\n",
    "\n",
    "# Function to clear CUDA cache\n",
    "def clear_cuda_cache():\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n",
    "\n",
    "# Call this function after each batch or at the end of each epoch\n",
    "# Example usage within your training loop:\n",
    "# ... (inside the for batch loop) ...\n",
    "#         optimizer.step()\n",
    "#         clear_cuda_cache() # Call after optimizer step\n",
    "# ... (outside the for batch loop) ...\n",
    "# print(f\"[Epoch {epoch + 1}] Average Loss: {total_loss / len(dataloader):.4f}\")\n",
    "# clear_cuda_cache() # Call at the end of the epoch\n",
    "\n",
    "# You can also call it at the beginning of the loop or whenever you suspect memory issues.\n",
    "\n",
    "# Alternatively, you can also try deleting tensors that are no longer needed.\n",
    "# For example, inside the batch loop:\n",
    "# del logp_w_list, logp_l_list, logp_w_batch, logp_l_batch, loss\n",
    "# clear_cuda_cache()\n",
    "\n",
    "# Add this function definition somewhere before the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuclear GPU memory clearing - run this first\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def nuclear_gpu_clear():\n",
    "    \"\"\"Aggressively clear GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Clear all cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Reset memory stats\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "        \n",
    "        # Try to clear any remaining allocations\n",
    "        try:\n",
    "            torch.cuda.synchronize()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        print(f\"GPU memory cleared. Free: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()} bytes\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory} bytes\")\n",
    "        print(f\"Currently allocated: {torch.cuda.memory_allocated()} bytes\")\n",
    "\n",
    "# Set environment variable for memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear GPU memory now\n",
    "nuclear_gpu_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjEizTQivAvQ"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07e433061b02424fb88975c726d63c86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cafa7349eef4b7b8cc5253802943b9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15820aef970b43cca7266993df78365e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32be6281e7e3404685b4609ea6dc377f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3335fb1d97f349d39301a592503226c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be53745e6e034ce09d34e2268820684a",
      "placeholder": "​",
      "style": "IPY_MODEL_c3764dc8905d48cb8537ddd68ce72064",
      "value": ""
     }
    },
    "33fab839b102417e99e8ec9204550cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2e142305b1a4cf1941c19d03bda6c62",
      "placeholder": "​",
      "style": "IPY_MODEL_32be6281e7e3404685b4609ea6dc377f",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "361d3827ea954df8b7e31748251153db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49329b09131c4a9da35b4ab8d68bf7e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "539cef24281245139a1002c0f1c53fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07e433061b02424fb88975c726d63c86",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_361d3827ea954df8b7e31748251153db",
      "value": 4
     }
    },
    "59dfdf60a56d4c4dacf5b37260c50d3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe5493fd0ba247ec96d735befed62fd0",
       "IPY_MODEL_539cef24281245139a1002c0f1c53fea",
       "IPY_MODEL_f2a65985f11c4b07965612499dc807a9"
      ],
      "layout": "IPY_MODEL_15820aef970b43cca7266993df78365e"
     }
    },
    "756d9e19eada4f15a9a5d24669211455": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "85f92b4750214608ac3c7ff809ea9aec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_756d9e19eada4f15a9a5d24669211455",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbf2a60b9ab54eba868d3e8f99f92255",
      "value": 0
     }
    },
    "99247cb0e0fb420b84452bd4f6151cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ceec0fc7824ddc8c56290aed1f0c35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be53745e6e034ce09d34e2268820684a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2e142305b1a4cf1941c19d03bda6c62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3764dc8905d48cb8537ddd68ce72064": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7235724003a49c29c704c89305cab8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5e7917c4a364b868179019231ae1bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3335fb1d97f349d39301a592503226c1",
       "IPY_MODEL_85f92b4750214608ac3c7ff809ea9aec",
       "IPY_MODEL_33fab839b102417e99e8ec9204550cc2"
      ],
      "layout": "IPY_MODEL_0cafa7349eef4b7b8cc5253802943b9a"
     }
    },
    "f2a65985f11c4b07965612499dc807a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49329b09131c4a9da35b4ab8d68bf7e0",
      "placeholder": "​",
      "style": "IPY_MODEL_c7235724003a49c29c704c89305cab8a",
      "value": " 4/4 [00:05&lt;00:00,  1.23s/it]"
     }
    },
    "fbf2a60b9ab54eba868d3e8f99f92255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe5493fd0ba247ec96d735befed62fd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9ceec0fc7824ddc8c56290aed1f0c35",
      "placeholder": "​",
      "style": "IPY_MODEL_99247cb0e0fb420b84452bd4f6151cca",
      "value": "Loading checkpoint shards: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
