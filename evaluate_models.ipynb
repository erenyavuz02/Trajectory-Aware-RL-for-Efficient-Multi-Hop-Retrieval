{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330514c1",
   "metadata": {},
   "source": [
    "# Query Generation for HotpotQA with ColBERT\n",
    "\n",
    "This notebook demonstrates a query generation system for the HotpotQA dataset using ColBERT embeddings. The workflow includes:\n",
    "\n",
    "- Loading the HotpotQA fullwiki dataset\n",
    "- Setting up ColBERT model for text embedding and retrieval\n",
    "- Implementing few-shot prompting for query generation\n",
    "- Evaluating query quality for multi-hop question answering\n",
    "\n",
    "The system generates search queries that can effectively retrieve relevant passages for complex multi-hop questions in the HotpotQA benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5bd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x140849f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Importing the necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Setting the seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Iniitializing the retrieval model\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\").to(device)\n",
    "colbert_model.eval()\n",
    "\n",
    "# === Hugging Face auth ===\n",
    "login(\"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\")\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
    "test_dataset = dataset['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa9fe5",
   "metadata": {},
   "source": [
    "# Load few-shot prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47fc24d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which Nirvana album featured The Vaselines, Dave Grohl, and Chad Channing?\n",
      "Query: Nirvana album featuring The Vaselines Dave Grohl Chad Channing\n",
      "----------------------------------------\n",
      "Question: Who contributed to more Disney films, Claire Keane or her father Glen Keane?\n",
      "Query: Claire Keane vs Glen Keane Disney films\n",
      "----------------------------------------\n",
      "Question: What do the films \"Giuliani Time\" and \"Life After People\" have in common?\n",
      "Query: Giuliani Time and Life After People commonalities\n",
      "----------------------------------------\n",
      "Question:  \"I Saw Her Again\" was co-written by what Canadian singer born in 1940?\n",
      "Query: \"I Saw Her Again\" co-written by Canadian singer born 1940\n",
      "----------------------------------------\n",
      "Question: What former Detroit Pistons player hosted a talkshow on MTV in 1996?\n",
      "Query: Detroit Pistons player hosted MTV talk show 1996\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get the fewshot examples from the json file\n",
    "with open(\"fewshot_examples.json\", \"r\") as f:\n",
    "    FEWSHOT_EXAMPLES = json.load(f)\n",
    "\n",
    "# print some examples\n",
    "for example in FEWSHOT_EXAMPLES[:5]:  # Print the first 5 examples\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37671fcd",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd6e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Few-shot examples for generating search queries\n",
    "def build_fewshot_prompt(question, context=\"\", add_fewshot=False):\n",
    "    \n",
    "    task_str = f\"Generate a search query for the following question:\\n{question}\"\n",
    "    \n",
    "    context_str = f\"Context:\\n{context}\\n\\n\" if context else \"\"\n",
    "    \n",
    "    if add_fewshot:\n",
    "        num_fewshots = random.randint(1, 3)\n",
    "        fewshots = random.sample(FEWSHOT_EXAMPLES, num_fewshots)\n",
    "\n",
    "        fewshot_str = \"Examples:\\n\"\n",
    "        for ex in fewshots:\n",
    "            fewshot_str += f\"Question:{ex['question']}\\nQuery:{ex['query']}\\n\\n\"\n",
    "        return f\"{fewshot_str}{context_str}{task_str}\"\n",
    "    else:\n",
    "        return f\"{context_str}{task_str}\"\n",
    "    \n",
    "# === Embedding utility ===\n",
    "def compute_colbert_embeddings(texts):\n",
    "    encoded = colbert_tokenizer(\n",
    "        texts,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = colbert_model(**encoded).last_hidden_state\n",
    "    masks = encoded[\"attention_mask\"].bool()\n",
    "    return [output[i][masks[i]].cpu().numpy() for i in range(len(texts))]\n",
    "\n",
    "# === Scoring utility ===\n",
    "def maxsim_score(query_emb, doc_embs):\n",
    "    return float((torch.matmul(query_emb, doc_embs.T)).max(dim=1).values.sum())\n",
    "\n",
    "def compute_ap_recall_precision(supporting_pairs, retrieved_ids, sentence_metadata):\n",
    "    if not retrieved_ids or not supporting_pairs:\n",
    "        return 0.0, 0.0, 0.0\n",
    "        \n",
    "    retrieved_pairs = {\n",
    "        (sentence_metadata[i][\"title\"], sentence_metadata[i][\"sent_idx\"]) for i in retrieved_ids\n",
    "    }\n",
    "    hits = [1 if (sentence_metadata[i][\"title\"], sentence_metadata[i][\"sent_idx\"]) in supporting_pairs else 0 for i in retrieved_ids]\n",
    "    \n",
    "    # Calculate AP (Average Precision)\n",
    "    ap = sum(hits[i] / (i + 1) for i in range(len(hits)) if hits[i]) / max(sum(hits), 1)\n",
    "    \n",
    "    # Calculate regular precision\n",
    "    precision = sum(hits) / len(retrieved_ids) if retrieved_ids else 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = sum(hits) / len(supporting_pairs) if supporting_pairs else 0\n",
    "    \n",
    "    return ap, precision, recall\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return (2 * precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ea14e",
   "metadata": {},
   "source": [
    "# Evaluater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hotpotqa(\n",
    "    eval_dataset,\n",
    "    query_generator,\n",
    "    query_tokenizer,\n",
    "    num_hops=2,\n",
    "    top_k_retrieval=5,\n",
    "    max_new_tokens=20 # Allow more tokens for potentially longer queries\n",
    "):\n",
    "    print(f\"Starting evaluation with {len(eval_dataset)} samples...\")\n",
    "\n",
    "    # Initialize per-hop metrics\n",
    "    metrics_per_hop = [{\n",
    "        \"total_ap\": 0.0,\n",
    "        \"total_precision\": 0.0,\n",
    "        \"total_recall\": 0.0,\n",
    "        \"num_samples\": 0\n",
    "    } for _ in range(num_hops)]\n",
    "\n",
    "    all_results = [] # To store detailed results for inspection\n",
    "    \n",
    "    for idx in tqdm(range(1), desc=\"Evaluating Samples\"):\n",
    "        sample = eval_dataset[idx]\n",
    "        question = sample['question']\n",
    "        supporting_facts = sample['supporting_facts']\n",
    "        \n",
    "        context_titles = sample['context']['title']\n",
    "        context_sentences_grouped = sample['context']['sentences']\n",
    "        flattened_sentences = []\n",
    "        sentence_metadata = []\n",
    "        for title, sentences in zip(context_titles, context_sentences_grouped):\n",
    "            for i, sent in enumerate(sentences):\n",
    "                flattened_sentences.append(sent)\n",
    "                sentence_metadata.append({\"title\": title, \"sent_idx\": i})\n",
    "\n",
    "        # Compute embeddings for the entire context once\n",
    "        context_embeddings = compute_colbert_embeddings(flattened_sentences)\n",
    "\n",
    "        # Convert list of numpy arrays to a list of tensors for maxsim_score\n",
    "        # vector_store_embeddings_for_scoring stores individual document token embeddings\n",
    "        vector_store_embeddings_for_scoring = [torch.tensor(emb, dtype=torch.float32).to(device) for emb in context_embeddings]\n",
    "\n",
    "        current_context = \"\"  # No context for the first hop\n",
    "\n",
    "        # Ground truth supporting pairs for the current question\n",
    "        ground_truth_supporting_pairs = set(zip(supporting_facts['title'], supporting_facts['sent_id']))\n",
    "\n",
    "        # Store results for this question\n",
    "        question_results = {\n",
    "            \"question\": question,\n",
    "            \"ground_truth_supporting_pairs\": list(ground_truth_supporting_pairs),\n",
    "            \"hops\": []\n",
    "        }\n",
    "\n",
    "        # Skip questions with no supporting facts, as AP/Recall/F1 are ill-defined\n",
    "        if not ground_truth_supporting_pairs:\n",
    "            # print(f\"Skipping question '{question}' due to no supporting facts.\") # Keep this for debugging if needed\n",
    "            continue\n",
    "\n",
    "        for hop in range(num_hops):\n",
    "\n",
    "            prompt = build_fewshot_prompt(question, context=current_context)\n",
    "\n",
    "            inputs = query_tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True, # Apply padding if batching (though num_return_sequences=1 here)\n",
    "                truncation=True\n",
    "            ).to(query_generator.device)\n",
    "\n",
    "            # For T5 (Seq2Seq), you don't typically slice by prompt_length from `outputs.sequences`\n",
    "            # Instead, the decoder output is directly the generated text.\n",
    "            # You feed `input_ids` to the encoder, and the decoder generates.\n",
    "            outputs = query_generator.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False, # You can set this to False for deterministic generation\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=query_tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=False\n",
    "            )\n",
    "\n",
    "            # For T5 (Seq2Seq models), the `generated_sequences` are just the decoded output.\n",
    "            # No need to slice by `prompt_length`.\n",
    "            generated_sequence = outputs.sequences[0]\n",
    "            generated_query = query_tokenizer.decode(generated_sequence, skip_special_tokens=True).strip()\n",
    "\n",
    "            if not generated_query:\n",
    "                print(f\"Warning: Empty query generated for question: '{question}' hop: {hop}.\")\n",
    "                continue # Skip if empty query\n",
    "\n",
    "            # === Retrieval and Scoring ===\n",
    "            query_emb_list = compute_colbert_embeddings([generated_query])\n",
    "            if not query_emb_list:\n",
    "                print(f\"Warning: No embedding generated for query: '{generated_query}' for question: '{question}' hop: {hop}.\")\n",
    "                continue # Skip if embedding fails\n",
    "\n",
    "            query_emb = query_emb_list[0] # This is already a numpy array from compute_colbert_embeddings\n",
    "            # No need for `torch.tensor().to(device)` here, as `maxsim_score` will handle it for each call\n",
    "            # `maxsim_score` itself converts to tensor on the device.\n",
    "\n",
    "            scores = []\n",
    "            for doc_emb in vector_store_embeddings_for_scoring: # Iterate through each document's token embeddings\n",
    "                scores.append(maxsim_score(query_emb, doc_emb)) # query_emb (numpy), doc_emb (tensor already)\n",
    "\n",
    "            if not scores:\n",
    "                continue\n",
    "\n",
    "            # Get top_k retrieved document indices\n",
    "            top_indices = np.argsort(scores)[-top_k_retrieval:][::-1]\n",
    "\n",
    "            # Calculate AP, precision, and recall for the current hop\n",
    "            ap, precision, recall = compute_ap_recall_precision(\n",
    "                ground_truth_supporting_pairs, \n",
    "                top_indices, \n",
    "                sentence_metadata\n",
    "            )\n",
    "\n",
    "            f1 = calculate_f1(precision, recall)\n",
    "\n",
    "            # Accumulate scores per hop\n",
    "            metrics_per_hop[hop][\"total_ap\"] += ap\n",
    "            metrics_per_hop[hop][\"total_precision\"] += precision\n",
    "            metrics_per_hop[hop][\"total_recall\"] += recall\n",
    "            metrics_per_hop[hop][\"num_samples\"] += 1\n",
    "\n",
    "            retrieved_context = [flattened_sentences[i] for i in top_indices]\n",
    "\n",
    "            question_results[\"hops\"].append({\n",
    "                \"hop\": hop,\n",
    "                \"generated_query\": generated_query,\n",
    "                \"raw_generated_query\": generated_query,\n",
    "                \"ap\": ap,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"top_k_retrieved_docs\": retrieved_context,\n",
    "                \"top_k_retrieved_ids\": top_indices.tolist()\n",
    "            })\n",
    "\n",
    "            # Update current_context for the next hop\n",
    "            current_context = \"\\n\".join(retrieved_context)\n",
    "\n",
    "        all_results.append(question_results)\n",
    "\n",
    "    # Calculate and print metrics for each hop\n",
    "    print(\"\\n=== Per-Hop Evaluation Summary ===\")\n",
    "    hop_summaries = []\n",
    "    for hop in range(num_hops):\n",
    "        num_samples = metrics_per_hop[hop][\"num_samples\"]\n",
    "        if num_samples > 0:\n",
    "            avg_ap = metrics_per_hop[hop][\"total_ap\"] / num_samples\n",
    "            avg_precision = metrics_per_hop[hop][\"total_precision\"] / num_samples\n",
    "            avg_recall = metrics_per_hop[hop][\"total_recall\"] / num_samples\n",
    "            avg_f1 = calculate_f1(avg_precision, avg_recall)\n",
    "            \n",
    "            print(f\"\\nHop {hop + 1} Metrics:\")\n",
    "            print(f\"Number of Samples: {num_samples}\")\n",
    "            print(f\"Average AP: {avg_ap:.4f}\")\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "            print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "            print(f\"Average F1: {avg_f1:.4f}\")\n",
    "            \n",
    "            hop_summaries.append({\n",
    "                \"hop\": hop + 1,\n",
    "                \"num_samples\": num_samples,\n",
    "                \"average_ap\": avg_ap,\n",
    "                \"average_precision\": avg_precision,\n",
    "                \"average_recall\": avg_recall,\n",
    "                \"average_f1\": avg_f1\n",
    "            })\n",
    "\n",
    "    # Calculate overall metrics (averaged across all hops)\n",
    "    total_samples = sum(hop[\"num_samples\"] for hop in metrics_per_hop)\n",
    "    overall_ap = sum(hop[\"total_ap\"] for hop in metrics_per_hop) / total_samples if total_samples > 0 else 0.0\n",
    "    overall_precision = sum(hop[\"total_precision\"] for hop in metrics_per_hop) / total_samples if total_samples > 0 else 0.0\n",
    "    overall_recall = sum(hop[\"total_recall\"] for hop in metrics_per_hop) / total_samples if total_samples > 0 else 0.0\n",
    "    overall_f1 = calculate_f1(overall_precision, overall_recall)\n",
    "\n",
    "    print(\"\\n=== Overall Metrics (Averaged Across Hops) ===\")\n",
    "    print(f\"Total Samples Evaluated: {total_samples}\")\n",
    "    print(f\"Overall AP: {overall_ap:.4f}\")\n",
    "    print(f\"Overall Precision: {overall_precision:.4f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "    print(f\"Overall F1: {overall_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"overall_metrics\": {\n",
    "            \"average_ap\": overall_ap,\n",
    "            \"average_precision\": overall_precision,\n",
    "            \"average_recall\": overall_recall,\n",
    "            \"average_f1\": overall_f1,\n",
    "            \"total_samples\": total_samples\n",
    "        },\n",
    "        \"per_hop_metrics\": hop_summaries,\n",
    "        \"detailed_results\": all_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f853cd",
   "metadata": {},
   "source": [
    "# Load the query generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b864d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Model to evaluate\n",
    "model_path= \"google/flan-t5-small\"\n",
    "model_to_eval = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "model_to_eval_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Test dataset\n",
    "dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
    "eval_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91428fa4",
   "metadata": {},
   "source": [
    "# Evaluate the base model on the HotpotQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "evaluation_metrics = evaluate_hotpotqa(\n",
    "    eval_dataset=eval_dataset,\n",
    "    query_generator=model_to_eval,\n",
    "    query_tokenizer=model_to_eval_tokenizer,\n",
    "    num_hops=2,           # Keep consistent with your training/preference dataset generation\n",
    "    top_k_retrieval=5,    # Keep consistent with your preference dataset generation\n",
    "    max_new_tokens=20     # Adjust as needed for query length\n",
    ")\n",
    "\n",
    "# --- Save Results (Optional) ---\n",
    "output_filename = \"hotpotqa_evaluation_results.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(evaluation_metrics, f, indent=4)\n",
    "print(f\"\\nDetailed evaluation results saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702e913",
   "metadata": {},
   "source": [
    "# Load the trained query generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained query generation model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Replace with the path to your trained model\n",
    "trained_model_path = \"path/to/your/trained/model\"  # Update this path\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "trained_query_generator = AutoModelForSeq2SeqLM.from_pretrained(trained_model_path).to(device)\n",
    "trained_query_tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n",
    "\n",
    "# Set to evaluation mode\n",
    "trained_query_generator.eval()\n",
    "\n",
    "print(f\"Loaded trained model from: {trained_model_path}\")\n",
    "print(f\"Model device: {trained_query_generator.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243aa3f",
   "metadata": {},
   "source": [
    "# Evalute the trained model on the HotpotQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run Evaluation ---\n",
    "evaluation_metrics_trained = evaluate_hotpotqa(\n",
    "    eval_dataset=eval_dataset,\n",
    "    query_generator=trained_query_generator,\n",
    "    query_tokenizer=trained_query_tokenizer,\n",
    "    num_hops=2,           # Keep consistent with your training/preference dataset generation\n",
    "    top_k_retrieval=5,    # Keep consistent with your preference dataset generation\n",
    "    max_new_tokens=20     # Adjust as needed for query length\n",
    ")\n",
    "\n",
    "# --- Save Results (Optional) ---\n",
    "output_filename = \"hotpotqa_evaluation_results_trained_model.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(evaluation_metrics_trained, f, indent=4)\n",
    "print(f\"\\nDetailed evaluation results saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
