{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d6b471f5",
      "metadata": {
        "id": "d6b471f5"
      },
      "source": [
        "# FLAN T 5 IPO Training\n",
        "\n",
        "Minimal implementation of Implicit Preference Optimization (IPO) training for FLAN T 5 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d240952",
      "metadata": {
        "id": "1d240952"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.utils as utils\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "class PreferenceDataset(Dataset):\n",
        "    def __init__(self, json_path):\n",
        "        with open(json_path, 'r') as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        self.data = []\n",
        "        for question, entry in raw_data.items():\n",
        "            for hop, hop_data in entry[\"hops\"].items():\n",
        "                queries = hop_data[\"queries\"]\n",
        "                preferences = hop_data[\"preference_pairs\"]\n",
        "                for i, j in preferences:\n",
        "                    self.data.append({\n",
        "                        \"question\": question,\n",
        "                        \"preferred\": queries[i],\n",
        "                        \"dispreferred\": queries[j]\n",
        "                    })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e1818b3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "e1818b3b",
        "outputId": "01e0cac5-02c0-415b-b56c-ec250aa6c1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model google/flan-t5-small loaded successfully!\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mohitit20\u001b[0m (\u001b[33mohitit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Replace the model loading cell with:\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "model.train()\n",
        "print(f\"Model {model_name} loaded successfully!\")\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = \"57b8585a9cdb363d54a7d215dd95c824d880868b\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fcc6defc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcc6defc",
        "outputId": "84bde4ef-3854-44b6-c0ec-7cabd3ce7933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 70073\n",
            "Starting IPO training...\n"
          ]
        }
      ],
      "source": [
        "def compute_logp(prompt, completion):\n",
        "    \"\"\"Compute log probability of completion given prompt for seq2seq model\"\"\"\n",
        "    # For T5, input is the prompt, target is the completion\n",
        "    input_encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    target_encoded = tokenizer(completion, return_tensors=\"pt\", truncation=True, max_length=64)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    input_ids = input_encoded.input_ids.to(device)\n",
        "    input_attention_mask = input_encoded.attention_mask.to(device)\n",
        "    labels = target_encoded.input_ids.to(device)\n",
        "\n",
        "    # Replace pad tokens in labels with -100\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=input_attention_mask,\n",
        "        labels=labels\n",
        "    )\n",
        "    return -outputs.loss\n",
        "\n",
        "def ipo_loss(logp_win, logp_lose, tau=0.05):\n",
        "    \"\"\"Compute IPO loss\"\"\"\n",
        "    diff = logp_win - logp_lose - 0.5 / tau\n",
        "    return (diff ** 2).mean()\n",
        "\n",
        "# Training setup\n",
        "parent_path = 'drive/MyDrive/c438_project'\n",
        "dataset_path = f'{parent_path}/preference_dataset_hotpotqa_final.json'\n",
        "dataset = PreferenceDataset(dataset_path)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "tau = 0.05\n",
        "num_epochs = 3\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Starting IPO training...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cbd776d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "1cbd776d",
        "outputId": "719facb6-771d-416d-b2b2-432c75e905b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250610_102719-uumedv92</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ohitit/c438_project/runs/uumedv92' target=\"_blank\">ipo_training_run</a></strong> to <a href='https://wandb.ai/ohitit/c438_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ohitit/c438_project' target=\"_blank\">https://wandb.ai/ohitit/c438_project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ohitit/c438_project/runs/uumedv92' target=\"_blank\">https://wandb.ai/ohitit/c438_project/runs/uumedv92</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3:  27%|██▋       | 19181/70073 [57:57<2:34:43,  5.48it/s, loss=91.4975]"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "wandb.init(\n",
        "    project=\"c438_project\",  # Name of the project in W&B\n",
        "    name=\"ipo_training_run\",    # A specific name for this run\n",
        "    config={\n",
        "        \"learning_rate\": optimizer.defaults['lr'],\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"batch_size\": dataloader.batch_size,\n",
        "        \"model_name\": model.name_or_path,\n",
        "        \"tau\": tau,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    global_step = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        questions = batch[\"question\"]\n",
        "        preferred = batch[\"preferred\"]\n",
        "        dispreferred = batch[\"dispreferred\"]\n",
        "\n",
        "        logp_w_list = []\n",
        "        logp_l_list = []\n",
        "\n",
        "        for q, w, l in zip(questions, preferred, dispreferred):\n",
        "            prompt = f\"Generate a search query for: {q}\\nQuery: \"\n",
        "\n",
        "            logp_w = compute_logp(prompt, w.strip())\n",
        "            logp_l = compute_logp(prompt, l.strip())\n",
        "\n",
        "            logp_w_list.append(logp_w)\n",
        "            logp_l_list.append(logp_l)\n",
        "\n",
        "        logp_w_batch = torch.stack(logp_w_list)\n",
        "        logp_l_batch = torch.stack(logp_l_list)\n",
        "\n",
        "        loss = ipo_loss(logp_w_batch, logp_l_batch, tau)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"Skipping batch {batch_idx} due to NaN loss.\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss = loss.item()\n",
        "        total_loss += current_loss\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "        pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
        "\n",
        "        wandb.log({\"step_loss\": current_loss, \"average_loss\": avg_loss})\n",
        "        \n",
        "        global_step = epoch * len(dataloader) + batch_idx + 1\n",
        "        \n",
        "        # Save model every 500 iterations (overwrite)s\n",
        "        if global_step % 500 == 0:\n",
        "            checkpoint_path = \"/ipo_trained_model/checkpoint_500\"\n",
        "            os.makedirs(checkpoint_path, exist_ok=True)\n",
        "            model.save_pretrained(checkpoint_path)\n",
        "            tokenizer.save_pretrained(checkpoint_path)\n",
        "            print(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
        "        \n",
        "        # Download model every 5000 iterations\n",
        "        if global_step % 5000 == 0:\n",
        "            download_path = \"/ipo_trained_model/checkpoint_5000\"\n",
        "            os.makedirs(download_path, exist_ok=True)\n",
        "            model.save_pretrained(download_path)\n",
        "            tokenizer.save_pretrained(download_path)\n",
        "            \n",
        "            # Create zip file and download\n",
        "            \n",
        "            zip_filename = f\"model_checkpoint_{global_step}.zip\"\n",
        "            with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "                for root, dirs, files_list in os.walk(download_path):\n",
        "                    for file in files_list:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        arcname = os.path.relpath(file_path, download_path)\n",
        "                        zipf.write(file_path, arcname)\n",
        "            \n",
        "            files.download(zip_filename)\n",
        "            print(f\"Model checkpoint at step {global_step} downloaded\")\n",
        "\n",
        "    epoch_avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"[Epoch {epoch + 1}] Average Loss: {epoch_avg_loss:.4f}\")\n",
        "\n",
        "    wandb.log({\"epoch\": epoch + 1, \"epoch_average_loss\": epoch_avg_loss})\n",
        "\n",
        "\n",
        "    epoch_save_path = f\"ipo_trained_model/epoch_{epoch+1}\"\n",
        "    os.makedirs(epoch_save_path, exist_ok=True)\n",
        "    model.save_pretrained(epoch_save_path)\n",
        "    tokenizer.save_pretrained(epoch_save_path)\n",
        "    print(f\"Epoch {epoch + 1} model saved to {epoch_save_path}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# --- Finish the W&B run ---\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "DMxskyzWT8uv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMxskyzWT8uv",
        "outputId": "5103b5e5-8a7d-4b14-a11f-92659e5090e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8f37bf",
      "metadata": {
        "id": "7b8f37bf"
      },
      "outputs": [],
      "source": [
        "def generate_query(question, max_length=30, temperature=0.8, top_p=0.9):\n",
        "    \"\"\"Generate a search query for the given question using the trained T5 model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # T5 works better with clear instruction format\n",
        "    prompt = f\"Generate a search query for this question: {question}\"\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    input_ids = encoded.input_ids.to(device)\n",
        "    attention_mask = encoded.attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            early_stopping=False,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    # For seq2seq models, decode the entire output (no need to slice)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up the generated text\n",
        "    generated_text = generated_text.strip()\n",
        "\n",
        "    # Remove question marks and periods\n",
        "    generated_text = generated_text.replace('?', '').replace('.', '')\n",
        "\n",
        "\n",
        "    return generated_text if generated_text else \"search query\"\n",
        "\n",
        "# Test with example questions\n",
        "test_questions = [\n",
        "    \"Who was the first person to climb Mount Everest?\",\n",
        "    \"What is the capital of the country where the Eiffel Tower is located?\",\n",
        "    \"Which movie won the Academy Award for Best Picture in 2020?\",\n",
        "    \"What is the largest planet in our solar system?\",\n",
        "    \"Who wrote the novel '1984'?\"\n",
        "]\n",
        "\n",
        "print(\"Testing the trained model on example questions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{i}. Question: {question}\")\n",
        "\n",
        "    # Generate multiple queries with different parameters\n",
        "    configs = [\n",
        "        {\"temperature\": 0.5, \"top_p\": 0.8},\n",
        "        {\"temperature\": 0.8, \"top_p\": 0.9},\n",
        "        {\"temperature\": 1.0, \"top_p\": 0.95}\n",
        "    ]\n",
        "\n",
        "    for j, config in enumerate(configs):\n",
        "        query = generate_query(question, **config)\n",
        "        print(f\"   Query {j+1}: {query}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nTesting completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
