{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d6b471f5",
      "metadata": {
        "id": "d6b471f5"
      },
      "source": [
        "# FLAN T 5 IPO Training\n",
        "\n",
        "Minimal implementation of Implicit Preference Optimization (IPO) training for FLAN T 5 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d240952",
      "metadata": {
        "id": "1d240952"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import optax\n",
        "import json\n",
        "from typing import Dict, List, Tuple\n",
        "from transformers import AutoTokenizer\n",
        "from flax.training import train_state\n",
        "import wandb\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Initialize TPU\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    print(\"TPU initialized successfully\")\n",
        "except:\n",
        "    print(\"TPU initialization failed, falling back to CPU/GPU\")\n",
        "\n",
        "class PreferenceDataset:\n",
        "    def __init__(self, json_path):\n",
        "        with open(json_path, 'r') as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        self.data = []\n",
        "        for question, entry in raw_data.items():\n",
        "            for hop, hop_data in entry[\"hops\"].items():\n",
        "                queries = hop_data[\"queries\"]\n",
        "                preferences = hop_data[\"preference_pairs\"]\n",
        "                for i, j in preferences:\n",
        "                    self.data.append({\n",
        "                        \"question\": question,\n",
        "                        \"preferred\": queries[i],\n",
        "                        \"dispreferred\": queries[j]\n",
        "                    })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "    \n",
        "    def get_batch(self, batch_size, rng_key):\n",
        "        \"\"\"Get a random batch for JAX training\"\"\"\n",
        "        indices = random.choice(rng_key, len(self.data), (batch_size,), replace=False)\n",
        "        batch = [self.data[int(idx)] for idx in indices]\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1818b3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "e1818b3b",
        "outputId": "01e0cac5-02c0-415b-b56c-ec250aa6c1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model google/flan-t5-small loaded successfully!\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mohitit20\u001b[0m (\u001b[33mohitit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install required packages for TPU\n",
        "!pip install -q flax transformers[flax] optax\n",
        "\n",
        "from transformers import FlaxT5ForConditionalGeneration, T5Config\n",
        "from flax import linen as nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "model_name = \"google/flan-t5-small\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load Flax model for TPU\n",
        "model = FlaxT5ForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=jnp.float32,\n",
        "    _do_init=True\n",
        ")\n",
        "\n",
        "print(f\"Model {model_name} loaded successfully for TPU!\")\n",
        "print(f\"JAX devices: {jax.devices()}\")\n",
        "print(f\"JAX device count: {jax.device_count()}\")\n",
        "\n",
        "# Setup W&B\n",
        "os.environ[\"WANDB_API_KEY\"] = \"57b8585a9cdb363d54a7d215dd95c824d880868b\"\n",
        "wandb.login()\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc6defc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcc6defc",
        "outputId": "84bde4ef-3854-44b6-c0ec-7cabd3ce7933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 70073\n",
            "Starting IPO training...\n"
          ]
        }
      ],
      "source": [
        "def compute_logp_jax(params, prompt_ids, prompt_mask, target_ids, target_mask):\n",
        "    \"\"\"Compute log probability using JAX/Flax model\"\"\"\n",
        "    # Prepare labels (replace pad tokens with -100)\n",
        "    labels = jnp.where(target_mask, target_ids, -100)\n",
        "    \n",
        "    outputs = model(\n",
        "        input_ids=prompt_ids,\n",
        "        attention_mask=prompt_mask,\n",
        "        labels=labels,\n",
        "        params=params,\n",
        "        train=True\n",
        "    )\n",
        "    return -outputs.loss\n",
        "\n",
        "def ipo_loss_jax(logp_win, logp_lose, tau=0.05):\n",
        "    \"\"\"Compute IPO loss in JAX\"\"\"\n",
        "    diff = logp_win - logp_lose - 0.5 / tau\n",
        "    return jnp.mean(diff ** 2)\n",
        "\n",
        "def tokenize_batch(questions, preferred, dispreferred, max_prompt_len=128, max_target_len=64):\n",
        "    \"\"\"Tokenize a batch of data\"\"\"\n",
        "    batch_size = len(questions)\n",
        "    \n",
        "    # Prepare prompts\n",
        "    prompts = [f\"Generate a search query for: {q}\\nQuery: \" for q in questions]\n",
        "    \n",
        "    # Tokenize prompts\n",
        "    prompt_encodings = tokenizer(\n",
        "        prompts, \n",
        "        padding=True, \n",
        "        truncation=True, \n",
        "        max_length=max_prompt_len,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    \n",
        "    # Tokenize preferred and dispreferred completions\n",
        "    preferred_encodings = tokenizer(\n",
        "        preferred,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_target_len,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    \n",
        "    dispreferred_encodings = tokenizer(\n",
        "        dispreferred,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_target_len,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'prompt_ids': jnp.array(prompt_encodings['input_ids']),\n",
        "        'prompt_mask': jnp.array(prompt_encodings['attention_mask']),\n",
        "        'preferred_ids': jnp.array(preferred_encodings['input_ids']),\n",
        "        'preferred_mask': jnp.array(preferred_encodings['attention_mask']),\n",
        "        'dispreferred_ids': jnp.array(dispreferred_encodings['input_ids']),\n",
        "        'dispreferred_mask': jnp.array(dispreferred_encodings['attention_mask'])\n",
        "    }\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, batch, tau=0.05):\n",
        "    \"\"\"Single training step compiled with JAX JIT\"\"\"\n",
        "    def loss_fn(params):\n",
        "        # Compute log probabilities for preferred completions\n",
        "        logp_preferred = jax.vmap(\n",
        "            lambda p_ids, p_mask, t_ids, t_mask: compute_logp_jax(params, p_ids[None], p_mask[None], t_ids[None], t_mask[None])\n",
        "        )(batch['prompt_ids'], batch['prompt_mask'], batch['preferred_ids'], batch['preferred_mask'])\n",
        "        \n",
        "        # Compute log probabilities for dispreferred completions\n",
        "        logp_dispreferred = jax.vmap(\n",
        "            lambda p_ids, p_mask, t_ids, t_mask: compute_logp_jax(params, p_ids[None], p_mask[None], t_ids[None], t_mask[None])\n",
        "        )(batch['prompt_ids'], batch['prompt_mask'], batch['dispreferred_ids'], batch['dispreferred_mask'])\n",
        "        \n",
        "        # Compute IPO loss\n",
        "        loss = ipo_loss_jax(logp_preferred, logp_dispreferred, tau)\n",
        "        return loss\n",
        "    \n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "# Training setup\n",
        "parent_path = '/content/drive/MyDrive/c438_project'\n",
        "dataset_path = f'{parent_path}/preference_dataset_hotpotqa_final.json'\n",
        "dataset = PreferenceDataset(dataset_path)\n",
        "\n",
        "# Training hyperparameters\n",
        "batch_size = 4  # Larger batch size for TPU efficiency\n",
        "learning_rate = 5e-6\n",
        "tau = 0.05\n",
        "num_epochs = 3\n",
        "\n",
        "# Initialize optimizer and training state\n",
        "optimizer = optax.adamw(learning_rate=learning_rate)\n",
        "rng = random.PRNGKey(42)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn=model.__call__,\n",
        "    params=model.params,\n",
        "    tx=optimizer\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Starting IPO training on TPU...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cbd776d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "1cbd776d",
        "outputId": "719facb6-771d-416d-b2b2-432c75e905b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250610_102719-uumedv92</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ohitit/c438_project/runs/uumedv92' target=\"_blank\">ipo_training_run</a></strong> to <a href='https://wandb.ai/ohitit/c438_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ohitit/c438_project' target=\"_blank\">https://wandb.ai/ohitit/c438_project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ohitit/c438_project/runs/uumedv92' target=\"_blank\">https://wandb.ai/ohitit/c438_project/runs/uumedv92</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3:  27%|██▋       | 19181/70073 [57:57<2:34:43,  5.48it/s, loss=91.4975]"
          ]
        }
      ],
      "source": [
        "wandb.init(\n",
        "    project=\"c438_project\",\n",
        "    name=\"ipo_training_tpu_run\",\n",
        "    config={\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"model_name\": model_name,\n",
        "        \"tau\": tau,\n",
        "        \"device\": \"TPU\",\n",
        "        \"device_count\": jax.device_count()\n",
        "    }\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "num_batches_per_epoch = len(dataset) // batch_size\n",
        "rng = random.PRNGKey(42)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    epoch_rng = random.fold_in(rng, epoch)\n",
        "    \n",
        "    pbar = tqdm(range(num_batches_per_epoch), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    \n",
        "    for batch_idx in pbar:\n",
        "        # Get batch\n",
        "        batch_rng = random.fold_in(epoch_rng, batch_idx)\n",
        "        raw_batch = dataset.get_batch(batch_size, batch_rng)\n",
        "        \n",
        "        # Extract data from batch\n",
        "        questions = [item['question'] for item in raw_batch]\n",
        "        preferred = [item['preferred'].strip() for item in raw_batch]\n",
        "        dispreferred = [item['dispreferred'].strip() for item in raw_batch]\n",
        "        \n",
        "        # Tokenize batch\n",
        "        batch = tokenize_batch(questions, preferred, dispreferred)\n",
        "        \n",
        "        # Training step\n",
        "        try:\n",
        "            state, loss = train_step(state, batch, tau)\n",
        "            current_loss = float(loss)\n",
        "            \n",
        "            if jnp.isnan(loss):\n",
        "                print(f\"Skipping batch {batch_idx} due to NaN loss.\")\n",
        "                continue\n",
        "            \n",
        "            total_loss += current_loss\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            \n",
        "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
        "            wandb.log({\"step_loss\": current_loss, \"average_loss\": avg_loss})\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    epoch_avg_loss = total_loss / num_batches_per_epoch\n",
        "    print(f\"[Epoch {epoch + 1}] Average Loss: {epoch_avg_loss:.4f}\")\n",
        "    wandb.log({\"epoch\": epoch + 1, \"epoch_average_loss\": epoch_avg_loss})\n",
        "    \n",
        "    # Save model checkpoint\n",
        "    epoch_save_path = f\"{parent_path}/ipo_trained_model_tpu/epoch_{epoch+1}\"\n",
        "    os.makedirs(epoch_save_path, exist_ok=True)\n",
        "    \n",
        "    # Save Flax model\n",
        "    model.save_pretrained(\n",
        "        epoch_save_path,\n",
        "        params=state.params,\n",
        "        push_to_hub=False\n",
        "    )\n",
        "    tokenizer.save_pretrained(epoch_save_path)\n",
        "    print(f\"Epoch {epoch + 1} model saved to {epoch_save_path}\")\n",
        "\n",
        "print(\"Training completed on TPU!\")\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8f37bf",
      "metadata": {
        "id": "7b8f37bf"
      },
      "outputs": [],
      "source": [
        "def generate_query(question, max_length=30, temperature=0.8, top_p=0.9):\n",
        "    \"\"\"Generate a search query for the given question using the trained T5 model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # T5 works better with clear instruction format\n",
        "    prompt = f\"Generate a search query for this question: {question}\"\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    input_ids = encoded.input_ids.to(device)\n",
        "    attention_mask = encoded.attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            early_stopping=False,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    # For seq2seq models, decode the entire output (no need to slice)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up the generated text\n",
        "    generated_text = generated_text.strip()\n",
        "\n",
        "    # Remove question marks and periods\n",
        "    generated_text = generated_text.replace('?', '').replace('.', '')\n",
        "\n",
        "\n",
        "    return generated_text if generated_text else \"search query\"\n",
        "\n",
        "# Test with example questions\n",
        "test_questions = [\n",
        "    \"Who was the first person to climb Mount Everest?\",\n",
        "    \"What is the capital of the country where the Eiffel Tower is located?\",\n",
        "    \"Which movie won the Academy Award for Best Picture in 2020?\",\n",
        "    \"What is the largest planet in our solar system?\",\n",
        "    \"Who wrote the novel '1984'?\"\n",
        "]\n",
        "\n",
        "print(\"Testing the trained model on example questions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{i}. Question: {question}\")\n",
        "\n",
        "    # Generate multiple queries with different parameters\n",
        "    configs = [\n",
        "        {\"temperature\": 0.5, \"top_p\": 0.8},\n",
        "        {\"temperature\": 0.8, \"top_p\": 0.9},\n",
        "        {\"temperature\": 1.0, \"top_p\": 0.95}\n",
        "    ]\n",
        "\n",
        "    for j, config in enumerate(configs):\n",
        "        query = generate_query(question, **config)\n",
        "        print(f\"   Query {j+1}: {query}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nTesting completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
