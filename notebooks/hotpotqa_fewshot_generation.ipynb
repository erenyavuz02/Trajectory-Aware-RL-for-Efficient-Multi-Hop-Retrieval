{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904716a9",
   "metadata": {},
   "source": [
    "# HotpotQA FullWiki Dataset Few-Shot Example Generation\n",
    "\n",
    "This notebook imports the HotpotQA FullWiki dataset and creates few-shot examples for multi-hop question answering. We'll extract questions, relevant contexts, irrelevant contexts, and generate queries needed to retrieve missing data from the first 1000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8565bf",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import the necessary libraries for data processing, manipulation, and random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ab738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f815771",
   "metadata": {},
   "source": [
    "## 2. Load HotpotQA FullWiki Dataset\n",
    "\n",
    "Load the HotpotQA FullWiki dataset using the Hugging Face datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7996dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HotpotQA FullWiki dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f7746917a4f7d83974866c163f7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FSTimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/implementations/http.py:262\u001b[0m, in \u001b[0;36mHTTPFileSystem._get_file\u001b[0;34m(self, rpath, lpath, chunk_size, callback, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m chunk:\n\u001b[0;32m--> 262\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mread(chunk_size)\n\u001b[1;32m    263\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/aiohttp/streams.py:393\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_nowait(n)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/aiohttp/streams.py:311\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/aiohttp/helpers.py:713\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load HotpotQA FullWiki dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading HotpotQA FullWiki dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotpot_qa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfullwiki\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get training data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m train_data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2084\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[1;32m   2085\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2086\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   2087\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m   2088\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m   2089\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2095\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[1;32m    926\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[1;32m    927\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[1;32m    930\u001b[0m )\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1649\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[1;32m   1650\u001b[0m         dl_manager,\n\u001b[1;32m   1651\u001b[0m         verification_mode,\n\u001b[1;32m   1652\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[1;32m   1653\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[1;32m   1654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[1;32m   1655\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/builder.py:979\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m    978\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 979\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/hotpot_qa/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5/hotpot_qa.py:116\u001b[0m, in \u001b[0;36mHotpotQA._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfullwiki\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    114\u001b[0m     paths[datasets\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTEST] \u001b[38;5;241m=\u001b[39m _URL_BASE \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotpot_test_fullwiki_v1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 116\u001b[0m files \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload(paths)\n\u001b[1;32m    118\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/download/download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m map_nested(\n\u001b[1;32m    160\u001b[0m         download_func,\n\u001b[1;32m    161\u001b[0m         url_or_urls,\n\u001b[1;32m    162\u001b[0m         map_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    163\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mnum_proc,\n\u001b[1;32m    164\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading data files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    165\u001b[0m         batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/py_utils.py:522\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    519\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    520\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    521\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 522\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    524\u001b[0m ]\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    526\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/py_utils.py:390\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    385\u001b[0m     batched\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    389\u001b[0m ):\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/download/download_manager.py:220\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[1;32m    207\u001b[0m         download_func,\n\u001b[1;32m    208\u001b[0m         url_or_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    222\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/download/download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m cached_path(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[1;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/file_utils.py:206\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Download external files\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m get_from_cache(\n\u001b[1;32m    207\u001b[0m             url_or_filename,\n\u001b[1;32m    208\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    209\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mforce_download,\n\u001b[1;32m    210\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muser_agent,\n\u001b[1;32m    211\u001b[0m             use_etag\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_etag,\n\u001b[1;32m    212\u001b[0m             token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    213\u001b[0m             storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    214\u001b[0m             download_desc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mdownload_desc,\n\u001b[1;32m    215\u001b[0m             disable_tqdm\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mdisable_tqdm,\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/file_utils.py:412\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, user_agent, use_etag, token, storage_options, download_desc, disable_tqdm)\u001b[0m\n\u001b[1;32m    410\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# GET file object\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     fsspec_get(url, temp_file, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, desc\u001b[38;5;241m=\u001b[39mdownload_desc, disable_tqdm\u001b[38;5;241m=\u001b[39mdisable_tqdm)\n\u001b[1;32m    414\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    415\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/file_utils.py:331\u001b[0m, in \u001b[0;36mfsspec_get\u001b[0;34m(url, temp_file, storage_options, desc, disable_tqdm)\u001b[0m\n\u001b[1;32m    318\u001b[0m fs, path \u001b[38;5;241m=\u001b[39m url_to_fs(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m    319\u001b[0m callback \u001b[38;5;241m=\u001b[39m TqdmCallback(\n\u001b[1;32m    320\u001b[0m     tqdm_kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m: desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     }\n\u001b[1;32m    330\u001b[0m )\n\u001b[0;32m--> 331\u001b[0m fs\u001b[38;5;241m.\u001b[39mget_file(path, temp_file\u001b[38;5;241m.\u001b[39mname, callback\u001b[38;5;241m=\u001b[39mcallback)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:101\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m return_result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# suppress asyncio.TimeoutError, raise FSTimeoutError\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load HotpotQA FullWiki dataset\n",
    "print(\"Loading HotpotQA FullWiki dataset...\")\n",
    "dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
    "\n",
    "# Get training data\n",
    "train_data = dataset['train']\n",
    "print(f\"Dataset loaded! Total training samples: {len(train_data)}\")\n",
    "\n",
    "# Convert first 1000 samples to a more manageable format\n",
    "first_1000 = train_data.select(range(min(1000, len(train_data))))\n",
    "print(f\"Selected first {len(first_1000)} samples for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f4e14",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset Structure\n",
    "\n",
    "Let's examine the structure of the HotpotQA dataset to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52201ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of a sample\n",
    "sample = first_1000[0]\n",
    "print(\"Sample structure:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, list) and len(value) > 0:\n",
    "        print(f\"{key}: {type(value)} with {len(value)} items\")\n",
    "        if isinstance(value[0], dict):\n",
    "            print(f\"  First item keys: {list(value[0].keys())}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)} - {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57299360",
   "metadata": {},
   "source": [
    "## 4. Randomly Select Samples\n",
    "\n",
    "Randomly select a subset of samples from the first 1000 entries for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a89556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select samples for few-shot example generation\n",
    "num_examples = 10  # Number of few-shot examples to generate\n",
    "selected_indices = random.sample(range(len(first_1000)), num_examples)\n",
    "\n",
    "print(f\"Selected {num_examples} random samples from indices: {selected_indices}\")\n",
    "\n",
    "# Extract selected samples\n",
    "selected_samples = [first_1000[i] for i in selected_indices]\n",
    "print(f\"Successfully extracted {len(selected_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352d4eb",
   "metadata": {},
   "source": [
    "## 5. Extract Relevant and Irrelevant Contexts\n",
    "\n",
    "For each sample, we'll identify relevant contexts (supporting facts) and create irrelevant contexts from other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5654f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_contexts(sample):\n",
    "    \"\"\"Extract relevant contexts based on supporting facts.\"\"\"\n",
    "    supporting_facts = sample['supporting_facts']\n",
    "    context = sample['context']\n",
    "    \n",
    "    relevant_contexts = []\n",
    "    \n",
    "    # supporting_facts is a dict with 'title' and 'sent_id' keys\n",
    "    supporting_titles = supporting_facts['title']\n",
    "    supporting_sent_ids = supporting_facts['sent_id']\n",
    "    \n",
    "    # context is a dict with 'title' and 'sentences' keys\n",
    "    context_titles = context['title']\n",
    "    context_sentences = context['sentences']\n",
    "    \n",
    "    for title, sent_id in zip(supporting_titles, supporting_sent_ids):\n",
    "        # Find the index of this title in context\n",
    "        try:\n",
    "            title_index = context_titles.index(title)\n",
    "            sentences = context_sentences[title_index]\n",
    "            \n",
    "            if sent_id < len(sentences):\n",
    "                relevant_contexts.append({\n",
    "                    'title': title,\n",
    "                    'sentence': sentences[sent_id],\n",
    "                    'sentence_id': sent_id\n",
    "                })\n",
    "        except (ValueError, IndexError):\n",
    "            # Title not found in context or sentence_id out of range\n",
    "            continue\n",
    "    \n",
    "    return relevant_contexts\n",
    "\n",
    "def extract_irrelevant_contexts(sample, num_irrelevant=3):\n",
    "    \"\"\"Extract irrelevant contexts from documents not in supporting facts.\"\"\"\n",
    "    supporting_facts = sample['supporting_facts']\n",
    "    context = sample['context']\n",
    "    \n",
    "    # Get supporting titles as a set for faster lookup\n",
    "    supporting_titles = set(supporting_facts['title'])\n",
    "    \n",
    "    context_titles = context['title']\n",
    "    context_sentences = context['sentences']\n",
    "    \n",
    "    irrelevant_contexts = []\n",
    "    \n",
    "    for i, (doc_title, sentences) in enumerate(zip(context_titles, context_sentences)):\n",
    "        if doc_title not in supporting_titles and sentences:\n",
    "            # Take first sentence from irrelevant documents\n",
    "            irrelevant_contexts.append({\n",
    "                'title': doc_title,\n",
    "                'sentence': sentences[0],\n",
    "                'sentence_id': 0\n",
    "            })\n",
    "    \n",
    "    # Randomly sample if we have too many\n",
    "    if len(irrelevant_contexts) > num_irrelevant:\n",
    "        irrelevant_contexts = random.sample(irrelevant_contexts, num_irrelevant)\n",
    "    \n",
    "    return irrelevant_contexts\n",
    "\n",
    "print(\"Context extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddaed8",
   "metadata": {},
   "source": [
    "## 6. Generate Queries for Missing Data\n",
    "\n",
    "Create queries that would help retrieve the missing information needed to answer each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_missing_data_query(sample, relevant_contexts):\n",
    "    \"\"\"Generate a query to retrieve missing data needed for the question.\"\"\"\n",
    "    question = sample['question']\n",
    "    answer = sample['answer']\n",
    "    question_type = sample.get('type', 'unknown')\n",
    "    \n",
    "    # Extract key entities from the question\n",
    "    question_lower = question.lower()\n",
    "    question_words = question_lower.split()\n",
    "    \n",
    "    # Extract entities from relevant contexts\n",
    "    entities = extract_entities_from_contexts(relevant_contexts)\n",
    "    \n",
    "    # Generate queries based on question type and patterns\n",
    "    if question_type == 'comparison':\n",
    "        # For comparison questions, create queries about the entities being compared\n",
    "        if len(entities) >= 2:\n",
    "            return f\"{entities[0]} vs {entities[1]} comparison\"\n",
    "        elif entities:\n",
    "            return f\"{entities[0]} information details\"\n",
    "    \n",
    "    elif 'when' in question_words or 'what year' in question_lower or 'date' in question_lower:\n",
    "        # Temporal questions\n",
    "        if entities:\n",
    "            return f\"when was {entities[0]} founded started established\"\n",
    "        else:\n",
    "            return f\"date year {' '.join(question_words[-3:])}\"\n",
    "    \n",
    "    elif 'where' in question_words or 'location' in question_lower:\n",
    "        # Location questions\n",
    "        if entities:\n",
    "            return f\"where is {entities[0]} located\"\n",
    "        else:\n",
    "            return f\"location of {answer}\"\n",
    "    \n",
    "    elif 'who' in question_words:\n",
    "        # Person questions\n",
    "        if entities:\n",
    "            return f\"who is {entities[0]} biography\"\n",
    "        else:\n",
    "            return f\"information about {answer}\"\n",
    "    \n",
    "    elif 'what' in question_words:\n",
    "        # General what questions\n",
    "        if 'magazine' in question_lower or 'publication' in question_lower:\n",
    "            if entities:\n",
    "                return f\"{entities[0]} magazine publication details\"\n",
    "        elif entities:\n",
    "            return f\"what is {entities[0]} definition\"\n",
    "    \n",
    "    elif 'which' in question_words:\n",
    "        # Which questions - often comparisons\n",
    "        if entities:\n",
    "            return f\"{' '.join(entities)} comparison details\"\n",
    "    \n",
    "    # Fallback: create a query based on the answer and question context\n",
    "    if entities:\n",
    "        return f\"{entities[0]} {answer} information\"\n",
    "    else:\n",
    "        return f\"information about {answer}\"\n",
    "\n",
    "def extract_entities_from_contexts(contexts):\n",
    "    \"\"\"Extract potential entities from context titles and sentences.\"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    for ctx in contexts:\n",
    "        title = ctx['title']\n",
    "        # Clean up titles - remove common words and extract main entities\n",
    "        title_clean = re.sub(r'\\(.*?\\)', '', title).strip()  # Remove parentheses\n",
    "        title_clean = re.sub(r\"'s\\b\", '', title_clean)  # Remove possessive 's\n",
    "        \n",
    "        # Split on common separators and take meaningful parts\n",
    "        parts = re.split(r'[,\\-–—]', title_clean)\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if len(part) > 2 and not part.lower() in ['the', 'and', 'or', 'of', 'in', 'for', 'to', 'a', 'an']:\n",
    "                entities.append(part)\n",
    "        \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_entities = []\n",
    "    for entity in entities:\n",
    "        if entity.lower() not in seen:\n",
    "            seen.add(entity.lower())\n",
    "            unique_entities.append(entity)\n",
    "    \n",
    "    return unique_entities[:3]  # Return first 3 unique entities\n",
    "\n",
    "print(\"Query generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd6f63",
   "metadata": {},
   "source": [
    "## 7. Generate Few-Shot Examples\n",
    "\n",
    "Combine all components to create comprehensive few-shot examples for multi-hop question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292197bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate few-shot examples\n",
    "few_shot_examples = []\n",
    "\n",
    "for i, sample in enumerate(selected_samples):\n",
    "    print(f\"Processing sample {i+1}/{len(selected_samples)}...\")\n",
    "    \n",
    "    # Extract components\n",
    "    question = sample['question']\n",
    "    answer = sample['answer']\n",
    "    question_type = sample.get('type', 'unknown')\n",
    "    level = sample.get('level', 'unknown')\n",
    "    \n",
    "    relevant_contexts = extract_relevant_contexts(sample)\n",
    "    irrelevant_contexts = extract_irrelevant_contexts(sample)\n",
    "    missing_data_query = generate_missing_data_query(sample, relevant_contexts)\n",
    "    \n",
    "    # Create a simple query based on the question for comparison\n",
    "    simple_query = question.replace('?', '').strip()\n",
    "    if len(simple_query) > 50:\n",
    "        # Take first part of long questions\n",
    "        simple_query = ' '.join(simple_query.split()[:8]) + '...'\n",
    "    \n",
    "    # Create few-shot example\n",
    "    few_shot_example = {\n",
    "        'id': sample['id'],\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'type': question_type,\n",
    "        'level': level,\n",
    "        'relevant_contexts': relevant_contexts,\n",
    "        'irrelevant_contexts': irrelevant_contexts,\n",
    "        'query': missing_data_query,  # Main generated query\n",
    "        'simple_query': simple_query,  # Simple fallback query\n",
    "        'num_supporting_facts': len(sample['supporting_facts']['title']),\n",
    "        'total_context_docs': len(sample['context']['title'])\n",
    "    }\n",
    "    \n",
    "    few_shot_examples.append(few_shot_example)\n",
    "    \n",
    "    # Print progress info\n",
    "    print(f\"  Question type: {question_type}, Level: {level}\")\n",
    "    print(f\"  Supporting facts: {len(relevant_contexts)}, Irrelevant contexts: {len(irrelevant_contexts)}\")\n",
    "    print(f\"  Generated query: {missing_data_query}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Generated {len(few_shot_examples)} few-shot examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1abd0",
   "metadata": {},
   "source": [
    "## 8. Display Generated Few-Shot Examples\n",
    "\n",
    "Let's examine the generated few-shot examples to verify their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated few-shot examples\n",
    "for i, example in enumerate(few_shot_examples[:3]):  # Show first 3 examples\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXAMPLE {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nID: {example['id']}\")\n",
    "    print(f\"Type: {example['type']} | Level: {example['level']}\")\n",
    "    print(f\"Supporting Facts: {example['num_supporting_facts']} | Total Context Docs: {example['total_context_docs']}\")\n",
    "    \n",
    "    print(f\"\\nQuestion: {example['question']}\")\n",
    "    print(f\"Answer: {example['answer']}\")\n",
    "    \n",
    "    print(f\"\\n--- RELEVANT CONTEXTS ({len(example['relevant_contexts'])}) ---\")\n",
    "    for j, ctx in enumerate(example['relevant_contexts']):\n",
    "        print(f\"  {j+1}. [{ctx['title']}] (Sent {ctx['sentence_id']})\")\n",
    "        print(f\"     {ctx['sentence']}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"--- IRRELEVANT CONTEXTS ({len(example['irrelevant_contexts'])}) ---\")\n",
    "    for j, ctx in enumerate(example['irrelevant_contexts']):\n",
    "        sentence_preview = ctx['sentence'][:100] + '...' if len(ctx['sentence']) > 100 else ctx['sentence']\n",
    "        print(f\"  {j+1}. [{ctx['title']}] {sentence_preview}\")\n",
    "    \n",
    "    print(f\"\\n--- GENERATED QUERIES ---\")\n",
    "    print(f\"Main Query: {example['query']}\")\n",
    "    print(f\"Simple Query: {example['simple_query']}\")\n",
    "\n",
    "print(f\"\\n\\nShowing 3 out of {len(few_shot_examples)} generated examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405a073",
   "metadata": {},
   "source": [
    "## 9. Save Few-Shot Examples\n",
    "\n",
    "Save the generated few-shot examples to a JSON file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save few-shot examples to JSON file\n",
    "output_file = 'hotpotqa_fewshot_examples.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(few_shot_examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Few-shot examples saved to {output_file}\")\n",
    "print(f\"Total examples: {len(few_shot_examples)}\")\n",
    "\n",
    "# Display comprehensive summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n📊 Dataset Overview:\")\n",
    "print(f\"- Total examples generated: {len(few_shot_examples)}\")\n",
    "print(f\"- Average relevant contexts per example: {np.mean([len(ex['relevant_contexts']) for ex in few_shot_examples]):.2f}\")\n",
    "print(f\"- Average irrelevant contexts per example: {np.mean([len(ex['irrelevant_contexts']) for ex in few_shot_examples]):.2f}\")\n",
    "print(f\"- Average supporting facts per example: {np.mean([ex['num_supporting_facts'] for ex in few_shot_examples]):.2f}\")\n",
    "print(f\"- Average total context docs per example: {np.mean([ex['total_context_docs'] for ex in few_shot_examples]):.2f}\")\n",
    "\n",
    "# Question type distribution\n",
    "print(f\"\\n🔍 Question Types:\")\n",
    "question_types = defaultdict(int)\n",
    "for ex in few_shot_examples:\n",
    "    question_types[ex['type']] += 1\n",
    "\n",
    "for qtype, count in sorted(question_types.items()):\n",
    "    percentage = (count / len(few_shot_examples)) * 100\n",
    "    print(f\"  - {qtype}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Question level distribution  \n",
    "print(f\"\\n📈 Difficulty Levels:\")\n",
    "levels = defaultdict(int)\n",
    "for ex in few_shot_examples:\n",
    "    levels[ex['level']] += 1\n",
    "\n",
    "for level, count in sorted(levels.items()):\n",
    "    percentage = (count / len(few_shot_examples)) * 100\n",
    "    print(f\"  - {level}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Query analysis\n",
    "print(f\"\\n📝 Query Analysis:\")\n",
    "query_lengths = [len(ex['query'].split()) for ex in few_shot_examples]\n",
    "print(f\"- Average query length: {np.mean(query_lengths):.1f} words\")\n",
    "print(f\"- Query length range: {min(query_lengths)}-{max(query_lengths)} words\")\n",
    "\n",
    "# Sample some queries by type\n",
    "print(f\"\\n🔍 Sample Queries by Type:\")\n",
    "for qtype in question_types.keys():\n",
    "    examples_of_type = [ex for ex in few_shot_examples if ex['type'] == qtype]\n",
    "    if examples_of_type:\n",
    "        sample_ex = examples_of_type[0]\n",
    "        print(f\"\\n  {qtype.upper()}:\")\n",
    "        print(f\"    Question: {sample_ex['question'][:80]}...\")\n",
    "        print(f\"    Query: {sample_ex['query']}\")\n",
    "\n",
    "print(f\"\\n✅ Few-shot dataset generation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plain examples from the HotpotQA few-shot dataset\n",
    "plain_examples = []\n",
    "\n",
    "# Select up to 3 examples\n",
    "for i, example in enumerate(few_shot_examples[:3]):\n",
    "    # Format relevant contexts as a single string\n",
    "    relevant_data = \" \".join([f\"[{ctx['title']}] {ctx['sentence']}\" for ctx in example['relevant_contexts']])\n",
    "    \n",
    "    # Format irrelevant contexts as a single string\n",
    "    irrelevant_data = \" \".join([f\"[{ctx['title']}] {ctx['sentence']}\" for ctx in example['irrelevant_contexts']])\n",
    "    \n",
    "    # Create plain example tuple\n",
    "    plain_example = {\n",
    "        'question': example['question'],\n",
    "        'relevant_data': relevant_data,\n",
    "        'irrelevant_data': irrelevant_data,\n",
    "        'query': example['query']\n",
    "    }\n",
    "    \n",
    "    plain_examples.append(plain_example)\n",
    "\n",
    "# Display the plain examples\n",
    "print(\"Plain Few-Shot Examples (Question, Relevant Data, Irrelevant Data, Query):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, example in enumerate(plain_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Relevant Data: {example['relevant_data'][:200]}...\")\n",
    "    print(f\"Irrelevant Data: {example['irrelevant_data'][:200]}...\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n✅ Created {len(plain_examples)} plain examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7283d90",
   "metadata": {},
   "source": [
    "## 10. Create Simplified Few-Shot Examples for Query Generation\n",
    "\n",
    "Create a simplified version of the few-shot examples specifically designed for query generation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simplified few-shot examples for query generation\n",
    "query_fewshot_examples = []\n",
    "\n",
    "for example in few_shot_examples:\n",
    "    # Create a simplified version focused on query generation\n",
    "    simplified_example = {\n",
    "        'question': example['question'],\n",
    "        'query': example['query'],\n",
    "        'type': example['type'],\n",
    "        'context': ' '.join([ctx['sentence'] for ctx in example['relevant_contexts'][:2]])  # Use first 2 relevant contexts\n",
    "    }\n",
    "    query_fewshot_examples.append(simplified_example)\n",
    "\n",
    "# Save simplified examples\n",
    "simplified_output_file = 'fewshot_examples.json'\n",
    "with open(simplified_output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(query_fewshot_examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Simplified few-shot examples saved to {simplified_output_file}\")\n",
    "print(f\"Format suitable for query generation training: {len(query_fewshot_examples)} examples\")\n",
    "\n",
    "# Show a few examples of the simplified format\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SIMPLIFIED FORMAT EXAMPLES\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for i, ex in enumerate(query_fewshot_examples[:2]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {ex['question']}\")\n",
    "    print(f\"Context: {ex['context'][:100]}...\")\n",
    "    print(f\"Query: {ex['query']}\")\n",
    "    print(f\"Type: {ex['type']}\")\n",
    "\n",
    "print(f\"\\n✅ Both detailed and simplified few-shot datasets created successfully!\")\n",
    "print(f\"- Detailed: {output_file} ({len(few_shot_examples)} examples)\")\n",
    "print(f\"- Simplified: {simplified_output_file} ({len(query_fewshot_examples)} examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25c108",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hotpotqa_fewshot_examples.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the detailed few-shot examples and format them as comprehensive strings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhotpotqa_fewshot_examples.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     fewshot_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_comprehensive_fewshot_as_string\u001b[39m(examples, num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hotpotqa_fewshot_examples.json'"
     ]
    }
   ],
   "source": [
    "def format_simple_fewshot_examples(examples, num_examples=3):\n",
    "    \"\"\"Create simple few-shot examples with question, contexts, and query.\"\"\"\n",
    "    formatted_examples = []\n",
    "    \n",
    "    # Select random examples\n",
    "    selected = random.sample(examples, min(num_examples, len(examples)))\n",
    "    \n",
    "    for example in selected:\n",
    "        # Format relevant contexts\n",
    "        relevant_contexts = []\n",
    "        for ctx in example['relevant_contexts']:\n",
    "            relevant_contexts.append(f\"[{ctx['title']}] {ctx['sentence']}\")\n",
    "        \n",
    "        # Format irrelevant contexts  \n",
    "        irrelevant_contexts = []\n",
    "        for ctx in example['irrelevant_contexts']:\n",
    "            irrelevant_contexts.append(f\"[{ctx['title']}] {ctx['sentence']}\")\n",
    "        \n",
    "        # Create formatted example\n",
    "        formatted_example = {\n",
    "            'question': example['question'],\n",
    "            'relevant_contexts': relevant_contexts,\n",
    "            'irrelevant_contexts': irrelevant_contexts,\n",
    "            'query': example['query']\n",
    "        }\n",
    "        \n",
    "        formatted_examples.append(formatted_example)\n",
    "    \n",
    "    return formatted_examples\n",
    "\n",
    "# Generate simple few-shot examples\n",
    "simple_fewshot_examples = format_simple_fewshot_examples(fewshot_data, 3)\n",
    "\n",
    "# Display the examples\n",
    "print(\"Simple Few-Shot Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, example in enumerate(simple_fewshot_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    \n",
    "    print(f\"\\nRelevant Contexts:\")\n",
    "    for j, ctx in enumerate(example['relevant_contexts'], 1):\n",
    "        print(f\"  {j}. {ctx}\")\n",
    "    \n",
    "    print(f\"\\nIrrelevant Contexts:\")\n",
    "    for j, ctx in enumerate(example['irrelevant_contexts'], 1):\n",
    "        print(f\"  {j}. {ctx}\")\n",
    "    \n",
    "    print(f\"\\nQuery: {example['query']}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n✅ Generated {len(simple_fewshot_examples)} simple few-shot examples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ff785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ab505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
