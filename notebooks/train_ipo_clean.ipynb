{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers torch==2.7 tqdm numpy pylate bitsandbytes accelerate huggingface_hub wandb torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b471f5",
   "metadata": {
    "id": "d6b471f5"
   },
   "source": [
    "# FLAN T 5 IPO Training\n",
    "\n",
    "Minimal implementation of Implicit Preference Optimization (IPO) training for FLAN T 5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8031f",
   "metadata": {},
   "source": [
    "# Import libraries and define Preference Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d240952",
   "metadata": {
    "id": "1d240952"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils as utils\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Modified PreferenceDataset to include hop information\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        self.data = []\n",
    "        for question, entry in raw_data.items():\n",
    "            for hop, hop_data in entry[\"hops\"].items():\n",
    "                queries = hop_data[\"queries\"]\n",
    "                preferences = hop_data[\"preference_pairs\"]\n",
    "                for i, j in preferences:\n",
    "                    self.data.append({\n",
    "                        \"question\": question,\n",
    "                        \"preferred\": queries[i],\n",
    "                        \"dispreferred\": queries[j],\n",
    "                        \"hop\": hop  # Add hop information\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb0103",
   "metadata": {},
   "source": [
    "# Import Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1818b3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "e1818b3b",
    "outputId": "01e0cac5-02c0-415b-b56c-ec250aa6c1fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model google/flan-t5-small loaded successfully!\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mohitit20\u001b[0m (\u001b[33mohitit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the model loading cell with:\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model.train()\n",
    "print(f\"Model {model_name} loaded successfully!\")\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"57b8585a9cdb363d54a7d215dd95c824d880868b\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68cff5e",
   "metadata": {},
   "source": [
    "# Loss Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6defc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcc6defc",
    "outputId": "84bde4ef-3854-44b6-c0ec-7cabd3ce7933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 70073\n",
      "Starting IPO training...\n"
     ]
    }
   ],
   "source": [
    "def compute_logp(prompt, completion):\n",
    "    \"\"\"Compute log probability of completion given prompt for seq2seq model\"\"\"\n",
    "    # For T5, input is the prompt, target is the completion\n",
    "    input_encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    target_encoded = tokenizer(completion, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = input_encoded.input_ids.to(device)\n",
    "    input_attention_mask = input_encoded.attention_mask.to(device)\n",
    "    labels = target_encoded.input_ids.to(device)\n",
    "\n",
    "    # Replace pad tokens in labels with -100\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=input_attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    return -outputs.loss\n",
    "\n",
    "def ipo_loss(logp_win, logp_lose, tau=0.05):\n",
    "    \"\"\"Compute IPO loss\"\"\"\n",
    "    diff = logp_win - logp_lose - 0.5 / tau\n",
    "    return (diff ** 2).mean()\n",
    "\n",
    "# Modified IPO loss function with weighting\n",
    "def ipo_loss_weighted(logp_win, logp_lose, weights, tau=0.05):\n",
    "    \"\"\"Compute weighted IPO loss based on hop importance\"\"\"\n",
    "    diff = logp_win - logp_lose - 0.5 / tau\n",
    "    weighted_loss = weights * (diff ** 2)\n",
    "    return weighted_loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae556a22",
   "metadata": {},
   "source": [
    "# Define Training Setup\n",
    "\n",
    "- set the preference dataset path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4feaaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "parent_path = 'drive/MyDrive/c438_project'\n",
    "dataset_path = f'{parent_path}/preference_dataset_hotpotqa_final.json'\n",
    "dataset = PreferenceDataset(dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "tau = 0.05\n",
    "num_epochs = 3\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Starting IPO training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd776d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "1cbd776d",
    "outputId": "719facb6-771d-416d-b2b2-432c75e905b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250610_102719-uumedv92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ohitit/c438_project/runs/uumedv92' target=\"_blank\">ipo_training_run</a></strong> to <a href='https://wandb.ai/ohitit/c438_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ohitit/c438_project' target=\"_blank\">https://wandb.ai/ohitit/c438_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ohitit/c438_project/runs/uumedv92' target=\"_blank\">https://wandb.ai/ohitit/c438_project/runs/uumedv92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  27%|██▋       | 19181/70073 [57:57<2:34:43,  5.48it/s, loss=91.4975]"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Set environment variable to catch CUDA errors early\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "model_configuration = \"base_flan-t5-small\"  # Change this to your model configuration\n",
    "\n",
    "wandb.init(\n",
    "    project=\"c438_project\",  # Name of the project in W&B\n",
    "    name=\"ipo_training_run\",    # A specific name for this run\n",
    "    config={\n",
    "        \"learning_rate\": optimizer.defaults['lr'],\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"batch_size\": dataloader.batch_size,\n",
    "        \"model_name\": model.name_or_path if hasattr(model, 'name_or_path') else \"flan-t5-small\",\n",
    "        \"tau\": tau,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Modified training loop with hop-aware weighting and accumulated loss\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accumulated_loss = 0.0\n",
    "    accumulation_steps = 4  # Accumulate gradients over 4 steps\n",
    "    skipped_batches = 0\n",
    "    \n",
    "    # Define hop weights - first hop gets higher weight\n",
    "    hop_weights = {\n",
    "        \"hop_1\": 1.0,    # Full weight for first hop\n",
    "        \"hop_2\": 0.7,    # Reduced weight for second hop\n",
    "        \"hop_3\": 0.5,    # Even lower for third hop (if exists)\n",
    "        \"hop_4\": 0.3     # Minimal weight for fourth hop (if exists)\n",
    "    }\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        try:\n",
    "            questions = batch[\"question\"]\n",
    "            preferred = batch[\"preferred\"]\n",
    "            dispreferred = batch[\"dispreferred\"]\n",
    "            hops = batch[\"hop\"]  # Get hop information\n",
    "\n",
    "            # Compute log probabilities for all preferences in batch\n",
    "            logp_w_list = []\n",
    "            logp_l_list = []\n",
    "            weights_list = []\n",
    "\n",
    "            batch_has_error = False\n",
    "            for q, w, l, hop in zip(questions, preferred, dispreferred, hops):\n",
    "                try:\n",
    "                    prompt = f\"Generate a search query for: {q}\\nQuery: \"\n",
    "                    \n",
    "                    logp_w = compute_logp(prompt, w.strip())\n",
    "                    logp_l = compute_logp(prompt, l.strip())\n",
    "                    \n",
    "                    # Check for invalid logprobs\n",
    "                    if torch.isnan(logp_w) or torch.isnan(logp_l) or torch.isinf(logp_w) or torch.isinf(logp_l):\n",
    "                        print(f\"Invalid logprob detected, skipping batch {batch_idx}\")\n",
    "                        batch_has_error = True\n",
    "                        break\n",
    "                    \n",
    "                    # Get weight for this hop (default to 0.5 if hop not found)\n",
    "                    weight = hop_weights.get(hop, 0.5)\n",
    "                    \n",
    "                    logp_w_list.append(logp_w)\n",
    "                    logp_l_list.append(logp_l)\n",
    "                    weights_list.append(weight)\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    if \"CUDA\" in str(e):\n",
    "                        print(f\"CUDA error in batch {batch_idx}: {e}\")\n",
    "                        print(\"Clearing CUDA cache and skipping batch...\")\n",
    "                        torch.cuda.empty_cache()\n",
    "                        batch_has_error = True\n",
    "                        break\n",
    "                    else:\n",
    "                        raise e\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error in batch {batch_idx}: {e}\")\n",
    "                    batch_has_error = True\n",
    "                    break\n",
    "\n",
    "            if batch_has_error or len(logp_w_list) == 0:\n",
    "                skipped_batches += 1\n",
    "                continue\n",
    "\n",
    "            # Stack and compute IPO loss\n",
    "            try:\n",
    "                logp_w_batch = torch.stack(logp_w_list)\n",
    "                logp_l_batch = torch.stack(logp_l_list)\n",
    "                weights_batch = torch.tensor(weights_list, device=logp_w_batch.device)\n",
    "                \n",
    "                # Compute IPO loss with hop weighting\n",
    "                loss = ipo_loss_weighted(logp_w_batch, logp_l_batch, weights_batch, tau)\n",
    "                \n",
    "                # Normalize loss by accumulation steps\n",
    "                loss = loss / accumulation_steps\n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"Skipping batch {batch_idx} due to invalid loss.\")\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "\n",
    "                # Accumulate gradients\n",
    "                loss.backward()\n",
    "                accumulated_loss += loss.item()\n",
    "                \n",
    "                # Apply gradients every accumulation_steps\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Log the accumulated loss with hop distribution\n",
    "                    avg_hop_weight = torch.mean(weights_batch).item()\n",
    "                    wandb.log({\n",
    "                        \"step_loss\": accumulated_loss, \n",
    "                        \"batch_idx\": batch_idx,\n",
    "                        \"avg_hop_weight\": avg_hop_weight,\n",
    "                        \"skipped_batches\": skipped_batches\n",
    "                    })\n",
    "                    accumulated_loss = 0.0\n",
    "\n",
    "                current_loss = loss.item() * accumulation_steps  # Scale back for display\n",
    "                total_loss += current_loss\n",
    "                avg_loss = total_loss / (batch_idx + 1 - skipped_batches) if (batch_idx + 1 - skipped_batches) > 0 else 0\n",
    "                pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"skipped\": skipped_batches})\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA\" in str(e):\n",
    "                    print(f\"CUDA error during loss computation in batch {batch_idx}: {e}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Critical error in batch {batch_idx}: {e}\")\n",
    "            skipped_batches += 1\n",
    "            continue\n",
    "        \n",
    "        global_step = epoch * len(dataloader) + batch_idx + 1\n",
    "        \n",
    "        # Save model every 500 iterations (overwrite)\n",
    "        if global_step % 500 == 0:\n",
    "            try:\n",
    "                checkpoint_path = \"/ipo_trained_model/checkpoint_500\"\n",
    "                os.makedirs(checkpoint_path, exist_ok=True)\n",
    "                model.save_pretrained(checkpoint_path)\n",
    "                tokenizer.save_pretrained(checkpoint_path)\n",
    "                print(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving checkpoint: {e}\")\n",
    "        \n",
    "        # Download model every 5000 iterations\n",
    "        if global_step % 5000 == 0:\n",
    "            try:\n",
    "                download_path = \"/ipo_trained_model/checkpoint_5000\"\n",
    "                os.makedirs(download_path, exist_ok=True)\n",
    "                model.save_pretrained(download_path)\n",
    "                tokenizer.save_pretrained(download_path)\n",
    "                \n",
    "                # Create zip file and download\n",
    "                zip_filename = f\"model_checkpoint_{global_step}.zip\"\n",
    "                with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "                    for root, dirs, files_list in os.walk(download_path):\n",
    "                        for file in files_list:\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            arcname = os.path.relpath(file_path, download_path)\n",
    "                            zipf.write(file_path, arcname)\n",
    "                \n",
    "                files.download(zip_filename)\n",
    "                print(f\"Model checkpoint at step {global_step} downloaded\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating checkpoint download: {e}\")\n",
    "\n",
    "    effective_batches = len(dataloader) - skipped_batches\n",
    "    epoch_avg_loss = total_loss / effective_batches if effective_batches > 0 else 0\n",
    "    print(f\"[Epoch {epoch + 1}] Average Loss: {epoch_avg_loss:.4f}, Skipped Batches: {skipped_batches}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1, \n",
    "        \"epoch_average_loss\": epoch_avg_loss,\n",
    "        \"epoch_skipped_batches\": skipped_batches,\n",
    "        \"epoch_effective_batches\": effective_batches\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        epoch_save_path = f\"ipo_trained_model/epoch_{epoch+1}_{model_configuration}\"\n",
    "        os.makedirs(epoch_save_path, exist_ok=True)\n",
    "        model.save_pretrained(epoch_save_path)\n",
    "        tokenizer.save_pretrained(epoch_save_path)\n",
    "        print(f\"Epoch {epoch + 1} model saved to {epoch_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving epoch model: {e}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# --- Finish the W&B run ---\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "DMxskyzWT8uv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DMxskyzWT8uv",
    "outputId": "5103b5e5-8a7d-4b14-a11f-92659e5090e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f37bf",
   "metadata": {
    "id": "7b8f37bf"
   },
   "outputs": [],
   "source": [
    "def generate_query(question, max_length=30, temperature=0.8, top_p=0.9):\n",
    "    \"\"\"Generate a search query for the given question using the trained T5 model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # T5 works better with clear instruction format\n",
    "    prompt = f\"Generate a search query for this question: {question}\"\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            early_stopping=False,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "    # For seq2seq models, decode the entire output (no need to slice)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Clean up the generated text\n",
    "    generated_text = generated_text.strip()\n",
    "\n",
    "    # Remove question marks and periods\n",
    "    generated_text = generated_text.replace('?', '').replace('.', '')\n",
    "\n",
    "\n",
    "    return generated_text if generated_text else \"search query\"\n",
    "\n",
    "# Test with example questions\n",
    "test_questions = [\n",
    "    \"Who was the first person to climb Mount Everest?\",\n",
    "    \"What is the capital of the country where the Eiffel Tower is located?\",\n",
    "    \"Which movie won the Academy Award for Best Picture in 2020?\",\n",
    "    \"What is the largest planet in our solar system?\",\n",
    "    \"Who wrote the novel '1984'?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the trained model on example questions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "\n",
    "    # Generate multiple queries with different parameters\n",
    "    configs = [\n",
    "        {\"temperature\": 0.5, \"top_p\": 0.8},\n",
    "        {\"temperature\": 0.8, \"top_p\": 0.9},\n",
    "        {\"temperature\": 1.0, \"top_p\": 0.95}\n",
    "    ]\n",
    "\n",
    "    for j, config in enumerate(configs):\n",
    "        query = generate_query(question, **config)\n",
    "        print(f\"   Query {j+1}: {query}\")\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nTesting completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
