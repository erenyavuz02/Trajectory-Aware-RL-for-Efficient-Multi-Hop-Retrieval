{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "NnVzLD_NZ1GY",
        "outputId": "852f1fa2-11c6-4d40-8399-9c4a35a6e858"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers torch==2.7 tqdm numpy pylate bitsandbytes accelerate huggingface_hub wandb torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpRuVXJLZ1GY"
      },
      "source": [
        "# Preference Dataset Creation\n",
        "\n",
        "* This notebook is used to create preference datasets for training the IPO model.\n",
        "* It processes the HotpotQA dataset and generates preference pairs based on the answers to the questions\n",
        "* The generated dataset is saved in a JSON format for further use in training the IPO model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting the Environment, Libraries, models and dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries and Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1a7042f4c2f9437e97add9ed2626b303",
            "e4c73553fe004519997da3d3b87446df",
            "8c07bef152cb4c24b6e6d1bb8e82e80d",
            "4b1d3302022b41b0b155baf00ecb7108",
            "d990cf98481046a2be32a2a9384648fc",
            "cf9e6897cfd94cb99224cc290a680bb5",
            "8a20f589a78446738d66a11607aab7ff",
            "e9b5665829674dfbb8dd5727883d36cd",
            "98fbf851a0054ebe98baa7a6fb6cf3c3",
            "8f512a45ef004fa2abf5c730e907921b",
            "de699e45a2ce4966a0be897496f48a1b",
            "a07b7b7c9579400b90e51fd0af580b4e",
            "9211cd618aa84c5491312d8449deb42b",
            "d0949a7c6ec346fcbe227de2b962babd",
            "ee1b7e00779a4a04bcbdde86ccc3a114",
            "efec34ad43f54ef88dbc0ec30b0ad305",
            "bb10c533c56d45ad9eca5182125ea3c0",
            "b71a682aa3064035ad8d4648b32205c7",
            "6623fc7b7a6049c386d3dbeb54122ce3",
            "28d01fd6c0504e678951e1fafe717be2",
            "2a691ea8105344069e25812142cf3377",
            "3f54cdf156214111b17e4f89a82f3182",
            "03d13efa5e1346a1a92f551c1a3fcb4f",
            "a4ecc3feea634295b941f0a9ddf8b850",
            "f9aa96357d6f404b851035f502501b32",
            "f3728d8187bf4552b1392d12a0c9b7a2",
            "c6691cf20c604d9ebc445dbe66c88218",
            "907b86cba7ed4e1ab0bd17f93c77ec49",
            "586b40dc17354e68b2c5818d3e365d97",
            "8e8d55db31074ad5891cf248dc85887e",
            "f29838990b60437a8ecbdfe26e659119",
            "ab34e65559fd4084b98ae5b58dacd0fa",
            "a39aa8fa55834f55853f077ff034c6c1",
            "08b09e60c7bb412090560a894b407765",
            "f575cf6fe4dd409e87ca357d4ae40ce5",
            "c491a9cba9c347909e227552268ba615",
            "d79be87d1356466b8cf4565ed2fa188a",
            "ef70a43e4b464baaaadac0d277399db6",
            "bba6c760a41b40cdaa729071c0159d3a",
            "6e3fb07d31764b79bcb418ac2108cd5e",
            "bd3aac3364f146dda750e214834c4ebf",
            "79b9774dd61c4ad6a7406e28d2adac01",
            "3ede8f1889bb45669e8a66ad6bbc10ed",
            "e9c304ca78144d90a9a8e7c26842baf4",
            "66b4a32ede0d46418a08f693f9a7563a",
            "3c99a2748c43402d9cf77b5c562b0934",
            "32e685092596477388172092dd68fa26",
            "0eb90c078e324ed69ce923c78ab35439",
            "b19baf32140e4e3bb0f45a63159d5767",
            "949e2e14daee448d85daa9730718fff0",
            "5827e473079b4b67be0a8dec6881c44d",
            "401eff87b32a469d97cf3833563fc305",
            "72c580b196184a5ca66d1294ee7847dd",
            "efce798e15fb4b5fb07563c11258a451",
            "c1917a65121f4074aaa9f22a84582cc5",
            "f0a22ca681a344469237c318d83a76d0",
            "78ef6ba34eed46aa9ec11f7a86277a56",
            "3c6cc34bf4804e95871f091af83b7c16",
            "a2d39c751a1245a4a7a6cfb2be988653",
            "ca63d52aa22b447b8bece0f2dd499aaa",
            "5a02cb0b2d204636b0a7cad156fc58fd",
            "b506cc2735ae424dac1510c159d39016",
            "9259952ed0db46829e1b30c67befbc1e",
            "aa112bd5d0c647f4a89e908581a65c57",
            "48cbd43ee92b425383d74dc5ecf96c0a",
            "94f62e48d14c47d18a553935576d5ebc",
            "7047965925204f219e2fa749e0484616",
            "1f95baa0106f480b9948a762c2e9659d",
            "50566e2c55224db7b58489d786ebef8d",
            "9866f9bb416540ce8ac36c6cf7b66f72",
            "8ec2672d11ff4c5a8f41a3894a46f8e5",
            "892159ab68e84a4784362fa9f7ab624b",
            "8f9e076ff5264214ad297f619edb71e0",
            "f6733139051646d6a59ca646b2e8940c",
            "0d14bb2f62f2451db859f667590d3e5d",
            "5154efc7bad64ff39df45be39b001b71",
            "4f4bc8966cc947a7a741b8792d94c764",
            "9875f18bf5ee428ebcfdbbe6ffe905ef",
            "0434b22c316d4662be466b80cac4befd",
            "874993c2cd3746848bd6076ffc65ab97",
            "54da579838954b68bb0d901dad589cb1",
            "059775adb3a64d7a94308cd4718c6456",
            "1f1ef532a24d4c5c8f569fba8fbb238f",
            "9d0eac4c73dd4bb89467d9201c63f250",
            "389a187071de40e9aad87ef0fa927bee",
            "839b60478634447dbdf25e11ffd565b2",
            "0e5a88aa6d2c42a88fa33659078d96a3",
            "aeee065891174e47b6d447c41db828e7",
            "42ec8ef76191453ab8fb67699ef08217",
            "d92fb480260f4ad2a3893cd657460d86",
            "0ac3d5e3718744bfa9e3f6ad248845c2",
            "c2beaa6bf71d4bedbe46f6724944a31f",
            "ff9c834e9cf14081abfde690eea3dd2e",
            "94b2f753682f41e3a9b4a226f07d8110",
            "09459e7d00df498a81b5802592ea9f2c",
            "eb27bac2f9224802ab32401b4e58cfb9",
            "6770bddbe08f4090b7b2297527f5e72e",
            "a348a68913bb4e84b1b02c683b8e7234",
            "746a2b21e31a4c61957c740111363a28",
            "f1cceacd8b3c471aa624e73a028be7a7",
            "d435bec171b440b5b15d49e690ab7621",
            "11ced061db094f9fb1e473f81ed31e1a",
            "170640e5623e4cf4be3f8264875c304c",
            "d6a3924d52e4444c9b17662693a915da",
            "b93b66add48040829af815019a7e6b08",
            "29a14414703744cdab501d6fa42f0a9c",
            "80ccd68ad0264d0db3837c1c431c9314",
            "2d04751ebec14a77ac2d3227f6a15972",
            "f81ea9f8ca874ccbb9e57dfe36ebe151",
            "e659248ed843420f8195dcb896496d53",
            "0e3467bcd05f4a2eaae5e3428124732f",
            "a9a26777ed714b159f552947dd903ce6",
            "fe00800d2b60402f9efd4ee9ea3a7e83",
            "9d087621159c46ffabbed8fe835ad0b9",
            "29ec944e6a744f5fb360e3a3f8dfc106",
            "3f920884fc6d4167aef536cdc867ba2a",
            "2ca21d71769d468bbce4c7c4c59632a9",
            "4cad8b91848447269cbf88358fcc8bc1",
            "f20abe847a4d45cb8a899a72b0be44c2",
            "23e380c43b094a2a8f9b657d51b95a3d",
            "301632f9371942afa1711dc4fa8aa43e",
            "48d4458abca346b4baa28334f10a40c9",
            "6778eac8938d47a0ae05caa1b3253f9a",
            "e7d9a25403db469f9e50f638e86ac5c1",
            "73cd1d16dbfc4ece95fbc6b773f8618f",
            "2017d4f2b43046f5aeadf2115bb7d2bc",
            "9b3542f3248546888ef06bd449db1798",
            "670ae1c1f8764edca04f3a59adfa95ec",
            "0144d93096c04d648450bbbacda08331",
            "d1fc633f0f934c9d803847bbabde6475",
            "06f3bb25f6e4401e8b2b24b86ddf002b",
            "59f73744fc8844ecb0c3959fa1b6eabe"
          ]
        },
        "id": "cU9YoaZnLDK4",
        "outputId": "f6e310a4-4c52-4e10-dde6-7add98c19a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d463d0a1434deab12df0e6b1a147c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:  34%|###4      | 105M/308M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b4e28e8a5f04a7a84f662ab8e88f678",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0129b64b05e84a66a82d7f2a0a7015cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "821eb0937b3a463f84b283e02c385a76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "138efb68d8824e38b3fff094c406488f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81a4ea6141424edd809c42d5e253125b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92e6a7d6802446aca65ed28cdb894898",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e174011c0b7441ec80e8dda51a9d9286",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "CURRENT_TIME_STAMP = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# === Query Generator (T5 Flan) ===\n",
        "model_path = \"google/flan-t5-small\"\n",
        "query_generator = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
        "query_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "query_generator.eval()\n",
        "\n",
        "# === ColBERT for retrieval scoring ===\n",
        "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\").to(device)\n",
        "colbert_model.eval()\n",
        "\n",
        "# === set few-shot examples file path ===\n",
        "# TODO: Update the path to your few-shot examples file\n",
        "\n",
        "few_shot_file = \"/content/few_shot_examples.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLuoLwW5bOUl",
        "outputId": "ef442114-4bb5-4191-fde1-6957e38ea9e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be01a4accd92420d86c42df01bcd439f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1542d6d08ffe401c9d67b1cc4c42281a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m validation samples from existing file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Load from HuggingFace and create new files\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotpot_qa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfullwiki\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m     DATASET_SPLIT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m  \u001b[38;5;66;03m# 90% for training, 10% for validation\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m5000\u001b[39m]  \u001b[38;5;66;03m# Use 5K for faster processing\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2084\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[1;32m   2085\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2086\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   2087\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m   2088\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m   2089\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2095\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[1;32m    926\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[1;32m    927\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[1;32m    930\u001b[0m )\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1649\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[1;32m   1650\u001b[0m         dl_manager,\n\u001b[1;32m   1651\u001b[0m         verification_mode,\n\u001b[1;32m   1652\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[1;32m   1653\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[1;32m   1654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[1;32m   1655\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/builder.py:979\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m    978\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 979\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/hotpot_qa/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5/hotpot_qa.py:116\u001b[0m, in \u001b[0;36mHotpotQA._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfullwiki\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    114\u001b[0m     paths[datasets\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTEST] \u001b[38;5;241m=\u001b[39m _URL_BASE \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotpot_test_fullwiki_v1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 116\u001b[0m files \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload(paths)\n\u001b[1;32m    118\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m files:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/download/download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m map_nested(\n\u001b[1;32m    160\u001b[0m         download_func,\n\u001b[1;32m    161\u001b[0m         url_or_urls,\n\u001b[1;32m    162\u001b[0m         map_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    163\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mnum_proc,\n\u001b[1;32m    164\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading data files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    165\u001b[0m         batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/py_utils.py:522\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    519\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    520\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    521\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 522\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    524\u001b[0m ]\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    526\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/py_utils.py:390\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    385\u001b[0m     batched\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    389\u001b[0m ):\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/download/download_manager.py:220\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[1;32m    207\u001b[0m         download_func,\n\u001b[1;32m    208\u001b[0m         url_or_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    222\u001b[0m     ]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/download/download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m cached_path(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[1;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/file_utils.py:206\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Download external files\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m get_from_cache(\n\u001b[1;32m    207\u001b[0m             url_or_filename,\n\u001b[1;32m    208\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    209\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mforce_download,\n\u001b[1;32m    210\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muser_agent,\n\u001b[1;32m    211\u001b[0m             use_etag\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_etag,\n\u001b[1;32m    212\u001b[0m             token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    213\u001b[0m             storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    214\u001b[0m             download_desc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mdownload_desc,\n\u001b[1;32m    215\u001b[0m             disable_tqdm\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mdisable_tqdm,\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/file_utils.py:412\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, user_agent, use_etag, token, storage_options, download_desc, disable_tqdm)\u001b[0m\n\u001b[1;32m    410\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# GET file object\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     fsspec_get(url, temp_file, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, desc\u001b[38;5;241m=\u001b[39mdownload_desc, disable_tqdm\u001b[38;5;241m=\u001b[39mdisable_tqdm)\n\u001b[1;32m    414\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    415\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/utils/file_utils.py:331\u001b[0m, in \u001b[0;36mfsspec_get\u001b[0;34m(url, temp_file, storage_options, desc, disable_tqdm)\u001b[0m\n\u001b[1;32m    318\u001b[0m fs, path \u001b[38;5;241m=\u001b[39m url_to_fs(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m    319\u001b[0m callback \u001b[38;5;241m=\u001b[39m TqdmCallback(\n\u001b[1;32m    320\u001b[0m     tqdm_kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m: desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     }\n\u001b[1;32m    330\u001b[0m )\n\u001b[0;32m--> 331\u001b[0m fs\u001b[38;5;241m.\u001b[39mget_file(path, temp_file\u001b[38;5;241m.\u001b[39mname, callback\u001b[38;5;241m=\u001b[39mcallback)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "# === Dataset ===\n",
        "# Check if datasets already exist\n",
        "\n",
        "# Generate timestamped and parameter-specific filenames\n",
        "timestamp = CURRENT_TIME_STAMP.replace(\" \", \"_\").replace(\":\", \"-\")\n",
        "dataset_size = 5000\n",
        "val_size = 1000\n",
        "\n",
        "\n",
        "# TODO: Update filenames to import from existing files \n",
        "train_filename = \"hotpot_train_5000samples_2025-06-11_18-31-48.jsonl\"\n",
        "val_filename = \"hotpot_val_1000samples_2025-06-11_18-31-48.jsonl\"\n",
        "\n",
        "# Check for existing files with similar pattern\n",
        "existing_train_files = [f for f in os.listdir('.') if f.startswith('hotpot_train_') and f.endswith('.jsonl')]\n",
        "existing_val_files = [f for f in os.listdir('.') if f.startswith('hotpot_val_') and f.endswith('.jsonl')]\n",
        "\n",
        "if existing_train_files and existing_val_files:\n",
        "    # Use the most recent existing files\n",
        "    train_filename = sorted(existing_train_files)[-1]\n",
        "    val_filename = sorted(existing_val_files)[-1]\n",
        "\n",
        "    print(f\"Using existing dataset files:\")\n",
        "    print(f\"Training: {train_filename}\")\n",
        "    print(f\"Validation: {val_filename}\")\n",
        "\n",
        "    # Load from existing files\n",
        "    def load_dataset_from_jsonl(filename):\n",
        "        data = defaultdict(list)\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                item = json.loads(line.strip())\n",
        "                for key, value in item.items():\n",
        "                    data[key].append(value)\n",
        "        return dict(data)\n",
        "\n",
        "    train_dataset = load_dataset_from_jsonl(train_filename)\n",
        "    val_dataset = load_dataset_from_jsonl(val_filename)\n",
        "\n",
        "    print(f\"Loaded {len(train_dataset['question'])} training samples from existing file\")\n",
        "    print(f\"Loaded {len(val_dataset['question'])} validation samples from existing file\")\n",
        "else:\n",
        "    # Load from HuggingFace and create new files\n",
        "    dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
        "    DATASET_SPLIT = 0.9  # 90% for training, 10% for validation\n",
        "    train_dataset = dataset['train'][:5000]  # Use 5K for faster processing\n",
        "    val_dataset = dataset['train'][5000:6000]  # Use 1K for validation\n",
        "\n",
        "    print(f\"Loaded {len(train_dataset['question'])} samples for preference dataset creation\")\n",
        "\n",
        "    # Dump dataset into JSONL files for future use\n",
        "    def dump_dataset_to_jsonl(dataset, filename):\n",
        "        with open(filename, 'w') as f:\n",
        "            for item in dataset:\n",
        "                f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    dump_dataset_to_jsonl(train_dataset, train_filename)\n",
        "    dump_dataset_to_jsonl(val_dataset, val_filename)\n",
        "\n",
        "    print(f\"Training dataset saved to: {train_filename}\")\n",
        "    print(f\"Validation dataset saved to: {val_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "- Number of hops: 2\n",
            "- Queries per hop: 5\n",
            "- Top-K retrieval: 5\n",
            "- Batch size: 32\n"
          ]
        }
      ],
      "source": [
        "# === Configuration Parameters ===\n",
        "NUM_HOPS = 2          # Number of retrieval hops\n",
        "NUM_QUERIES = 5       # Generate 5 queries per hop for ranking\n",
        "TOP_K = 5            # Top-K documents to retrieve\n",
        "BATCH_SIZE = 32      # Process samples in batches for speed\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"- Number of hops: {NUM_HOPS}\")\n",
        "print(f\"- Queries per hop: {NUM_QUERIES}\")\n",
        "print(f\"- Top-K retrieval: {TOP_K}\")\n",
        "print(f\"- Batch size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms2eAO5LjHKS"
      },
      "source": [
        "## Constructing Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculating query-document embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WczqraXnjGqv"
      },
      "outputs": [],
      "source": [
        "def compute_colbert_embeddings_batched(texts, batch_size=32):\n",
        "    \"\"\"Compute ColBERT embeddings for texts in batches\"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        encoded = colbert_tokenizer(\n",
        "            batch_texts,\n",
        "            max_length=512,\n",
        "            padding=True,  # Changed from \"max_length\" to True for efficiency\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = colbert_model(**encoded).last_hidden_state\n",
        "\n",
        "        masks = encoded[\"attention_mask\"].bool()\n",
        "        batch_embeddings = [output[i][masks[i]].cpu().numpy() for i in range(len(batch_texts))]\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIQfyvb0Z1Gc"
      },
      "source": [
        "### Scoring and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL6VDVzpZ1Gd"
      },
      "outputs": [],
      "source": [
        "def maxsim_score(query_emb, doc_emb):\n",
        "    \"\"\"Optimized MaxSim score computation\"\"\"\n",
        "    if isinstance(query_emb, np.ndarray):\n",
        "        query_tensor = torch.tensor(query_emb, dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        query_tensor = query_emb.to(device)\n",
        "\n",
        "    if isinstance(doc_emb, np.ndarray):\n",
        "        doc_tensor = torch.tensor(doc_emb, dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        doc_tensor = doc_emb.to(device)\n",
        "\n",
        "    # Use torch.mm for better performance\n",
        "    similarity_matrix = torch.mm(query_tensor, doc_tensor.T)\n",
        "    return float(similarity_matrix.max(dim=1).values.sum())\n",
        "\n",
        "def compute_ap_recall(supporting_pairs, retrieved_ids, sentence_metadata, prev_top_index=None):\n",
        "    # Check if retrieved_ids is a numpy array and if it's empty\n",
        "    if (isinstance(retrieved_ids, np.ndarray) and retrieved_ids.size == 0) or not supporting_pairs:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    retrieved_pairs = {\n",
        "        (sentence_metadata[i][\"title\"], sentence_metadata[i][\"sent_idx\"]) for i in retrieved_ids\n",
        "    }\n",
        "    \n",
        "    hits = []\n",
        "    for i in retrieved_ids:\n",
        "        # If this index was the previous top index, don't count as hit\n",
        "        if prev_top_index is not None and i == prev_top_index:\n",
        "            hits.append(0)\n",
        "        else:\n",
        "            current_pair = (sentence_metadata[i][\"title\"], sentence_metadata[i][\"sent_idx\"])\n",
        "            is_supporting = current_pair in supporting_pairs\n",
        "            hits.append(1 if is_supporting else 0)\n",
        "\n",
        "    # Calculate AP (Average Precision)\n",
        "    ap = sum(hits[i] / (i + 1) for i in range(len(hits)) if hits[i]) / max(sum(hits), 1)\n",
        "\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = sum(hits) / len(supporting_pairs) if supporting_pairs else 0\n",
        "\n",
        "    return ap, recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache document embeddings for efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_embedding_cache = {}\n",
        "\n",
        "def get_cached_embeddings(flattened_sentences):\n",
        "    \"\"\"Get embeddings with caching to avoid recomputation\"\"\"\n",
        "    # Create a hash key for the sentences\n",
        "    sentences_key = hash(tuple(flattened_sentences))\n",
        "\n",
        "    if sentences_key not in document_embedding_cache:\n",
        "        document_embedding_cache[sentences_key] = compute_colbert_embeddings_batched(flattened_sentences)\n",
        "\n",
        "    return document_embedding_cache[sentences_key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRZ2BxhbZ1Gd"
      },
      "source": [
        "### Query Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCf7v4OOZ1Ge"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generate_queries_batch(question, current_context, num_queries=5, use_fewshot=True):\n",
        "    \"\"\"Generate multiple queries in a single batch\"\"\"\n",
        "    if use_fewshot:\n",
        "        try:\n",
        "            with open(few_shot_file, 'r') as f:\n",
        "                fewshot_examples = json.load(f)\n",
        "\n",
        "            selected_examples = random.sample(fewshot_examples, min(3, len(fewshot_examples)))\n",
        "            fewshot_prompt = \"\\n\".join([f\"Question: {ex['question']}\\nQuery: {ex['query']}\" for ex in selected_examples])\n",
        "\n",
        "            if current_context:\n",
        "                prompt = f\"{fewshot_prompt}\\n\\nContext: {current_context}\\nQuestion: {question}\\nQuery:\"\n",
        "            else:\n",
        "                prompt = f\"{fewshot_prompt}\\n\\nQuestion: {question}\\nQuery:\"\n",
        "        except FileNotFoundError:\n",
        "            if current_context:\n",
        "                prompt = f\"Context: {current_context}\\n\\nGenerate a search query for: {question}\"\n",
        "            else:\n",
        "                prompt = f\"Generate a search query for: {question}\"\n",
        "    else:\n",
        "        if current_context:\n",
        "            prompt = f\"Context: {current_context}\\n\\nGenerate a search query for: {question}\"\n",
        "        else:\n",
        "            prompt = f\"Generate a search query for: {question}\"\n",
        "\n",
        "    inputs = query_tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = query_generator.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,\n",
        "            do_sample=True,\n",
        "            temperature=0.85,\n",
        "            top_p=0.9,\n",
        "            num_return_sequences=num_queries  # Generate all queries at once\n",
        "        )\n",
        "\n",
        "    queries = []\n",
        "    for output in outputs:\n",
        "        query = query_tokenizer.decode(output, skip_special_tokens=True).strip()\n",
        "        # Extract only the generated part\n",
        "        if \"Query:\" in query:\n",
        "            query = query.split(\"Query:\")[-1].strip()\n",
        "        if query and query not in queries:\n",
        "            queries.append(query)\n",
        "\n",
        "    return queries[:num_queries]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Context Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data processing functions defined\n"
          ]
        }
      ],
      "source": [
        "def prepare_sample_context(sample):\n",
        "    \"\"\"Prepare and flatten context for a single sample\"\"\"\n",
        "    context_titles = sample['context']['title']\n",
        "    context_sentences_grouped = sample['context']['sentences']\n",
        "    flattened_sentences = []\n",
        "    sentence_metadata = []\n",
        "\n",
        "    for title, sentences in zip(context_titles, context_sentences_grouped):\n",
        "        for i, sent in enumerate(sentences):\n",
        "            flattened_sentences.append(sent)\n",
        "            sentence_metadata.append({\"title\": title, \"sent_idx\": i})\n",
        "\n",
        "    return flattened_sentences, sentence_metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single Hop Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_single_hop_optimized(question, current_context, flattened_sentences,\n",
        "                                context_embeddings, supporting_pairs, sentence_metadata, current_context_index=None):\n",
        "    \"\"\"Optimized single hop processing\"\"\"\n",
        "\n",
        "    # Generate all queries at once\n",
        "    queries = generate_queries_batch(question, current_context, NUM_QUERIES, use_fewshot=True)\n",
        "\n",
        "    if not queries:\n",
        "        return None\n",
        "\n",
        "    # Batch compute query embeddings\n",
        "    query_embeddings = compute_colbert_embeddings_batched(queries, batch_size=len(queries))\n",
        "\n",
        "    # Score all queries against all documents in vectorized manner\n",
        "    scored_queries = []\n",
        "    for i, (query, query_emb) in enumerate(zip(queries, query_embeddings)):\n",
        "        # Vectorized scoring\n",
        "        scores = np.array([maxsim_score(query_emb, doc_emb) for doc_emb in context_embeddings])\n",
        "        top_indices = np.argsort(scores)[-TOP_K:][::-1]\n",
        "\n",
        "        ap, recall = compute_ap_recall(supporting_pairs, top_indices, sentence_metadata, current_context_index)\n",
        "\n",
        "        scored_queries.append({\n",
        "            \"query\": query,\n",
        "            \"ap\": ap,\n",
        "            \"recall\": recall,\n",
        "            \"top_indices\": top_indices.tolist(),\n",
        "            \"retrieved_context\": [flattened_sentences[i] for i in top_indices]\n",
        "        })\n",
        "    \n",
        "    scored_queries.sort(key=lambda x: x[\"ap\"], reverse=True)\n",
        "\n",
        "    # Create preference pairs\n",
        "    preference_pairs = []\n",
        "    for i in range(len(scored_queries)):\n",
        "        for j in range(i + 1, len(scored_queries)):\n",
        "            if scored_queries[i][\"ap\"] > scored_queries[j][\"ap\"]:\n",
        "                preference_pairs.append((i, j))\n",
        "\n",
        "    return {\n",
        "        \"queries\": [x[\"query\"] for x in scored_queries],\n",
        "        \"aps\": [x[\"ap\"] for x in scored_queries],\n",
        "        \"recalls\": [x[\"recall\"] for x in scored_queries],\n",
        "        \"preference_pairs\": preference_pairs,\n",
        "        \"best_retrieved_context\": \"\\n\".join(scored_queries[0][\"retrieved_context\"]) if scored_queries else \"\",\n",
        "        \"best_retrieved_context_idx\": scored_queries[0][\"top_indices\"][0] if scored_queries else None\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning model for query generation\n",
        "\n",
        "Run these cells to fine-tune the model for query generation. This will help in generating better queries for the preference dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "FTTkrhvu69QF",
        "outputId": "a1333bef-57e5-4874-b3c5-dbfa153d5ed7"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "def finetune_query_generator_with_fewshot():\n",
        "    \"\"\"Fine-tune T5 Flan model using few-shot examples in prompts\"\"\"\n",
        "\n",
        "    global query_generator # Moved to the beginning of the function\n",
        "\n",
        "    # Load few-shot examples\n",
        "    fewshot_file_path = '/content/fewshot_examples.json'\n",
        "    try:\n",
        "        with open(fewshot_file_path, 'r') as f:\n",
        "            fewshot_examples = json.load(f)\n",
        "        print(f\"Loaded {len(fewshot_examples)} few-shot examples for fine-tuning\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: fewshot_examples.json not found. Please create this file first.\")\n",
        "        return None\n",
        "\n",
        "    def create_fewshot_prompt(target_question, target_query, examples, num_shots=3):\n",
        "        \"\"\"Create a few-shot prompt with examples\"\"\"\n",
        "        # Randomly select few-shot examples (excluding the target)\n",
        "        available_examples = [ex for ex in examples if ex['question'] != target_question]\n",
        "        selected_examples = random.sample(available_examples, min(num_shots, len(available_examples)))\n",
        "\n",
        "        # Build the prompt\n",
        "        prompt = \"Generate search queries based on questions. Here are some examples:\\n\\n\"\n",
        "\n",
        "        # Add few-shot examples\n",
        "        for i, example in enumerate(selected_examples, 1):\n",
        "            prompt += f\"Example {i}:\\n\"\n",
        "            prompt += f\"Question: {example['question']}\\n\"\n",
        "            prompt += f\"Query: {example['query']}\\n\\n\"\n",
        "\n",
        "        # Add the target question\n",
        "        prompt += f\"Now generate a query for:\\nQuestion: {target_question}\\nQuery:\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    class QueryDataset(Dataset):\n",
        "        def __init__(self, examples, tokenizer, max_length=512):\n",
        "            self.examples = examples\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.examples)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            example = self.examples[idx]\n",
        "\n",
        "            # Create few-shot prompt\n",
        "            input_text = create_fewshot_prompt(\n",
        "                example['question'],\n",
        "                example['query'],\n",
        "                self.examples,\n",
        "                num_shots=3\n",
        "            )\n",
        "            target_text = example['query']\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                input_text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            targets = self.tokenizer(\n",
        "                target_text,\n",
        "                max_length=64,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'input_ids': inputs.input_ids.squeeze(),\n",
        "                'attention_mask': inputs.attention_mask.squeeze(),\n",
        "                'labels': targets.input_ids.squeeze()\n",
        "            }\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = QueryDataset(fewshot_examples, query_tokenizer)\n",
        "\n",
        "    # Split into train/val (80/20)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset_ft, val_dataset_ft = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=query_tokenizer,\n",
        "        model=query_generator,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./finetuned_t5_flan_{timestamp}\",\n",
        "        num_train_epochs=20,\n",
        "        per_device_train_batch_size=2,  # Reduced due to longer prompts\n",
        "        per_device_eval_batch_size=2,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"./logs_{timestamp}\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        gradient_accumulation_steps=2,  # To compensate for smaller batch size\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=query_generator,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset_ft,\n",
        "        eval_dataset=val_dataset_ft,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=query_tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"Starting fine-tuning with few-shot prompts...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model_save_path = f\"./finetuned_t5_flan_final_{timestamp}\"\n",
        "    trainer.save_model(model_save_path)\n",
        "    query_tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "    print(f\"Fine-tuned model saved to: {model_save_path}\")\n",
        "\n",
        "    # Update global model with fine-tuned version\n",
        "    query_generator = trainer.model\n",
        "\n",
        "    return model_save_path\n",
        "\n",
        "# Execute fine-tuning if few-shot examples exist\n",
        "if os.path.exists('/content/fewshot_examples.json'):\n",
        "    finetuned_model_path = finetune_query_generator_with_fewshot()\n",
        "else:\n",
        "    print(\"Few-shot examples file not found. Skipping fine-tuning.\")\n",
        "    print(\"To enable fine-tuning, create '/content/fewshot_examples.json' with format:\")\n",
        "    print('[{\"question\": \"example question\", \"query\": \"example query\"}, ...]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Query Generation Examples With different settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViQk-noOZ1Gg",
        "outputId": "c6ab9464-599b-45b9-d017-3ee471ee957c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "QUERY GENERATOR EXAMPLES\n",
            "================================================================================\n",
            "\n",
            "1. QUERIES WITHOUT CONTEXT:\n",
            "--------------------------------------------------\n",
            "\n",
            "Question 1: Which magazine was started first Arthur's Magazine or First for Women?...\n",
            "Generated Query: Arthur's Magazine\n",
            "\n",
            "Question 2: The Oberoi family is part of a hotel company that has a head office in what city?...\n",
            "Generated Query: Oberoi family head office in city\n",
            "\n",
            "Question 3: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?...\n",
            "Generated Query: Allie Goertz song written by The Simpsons character Milhouse (Madage\n",
            "\n",
            "Question 4:  What nationality was James Henry Miller's wife?...\n",
            "Generated Query: James Henry Miller's wife nationality\n",
            "\n",
            "Question 5: Cadmium Chloride is slightly soluble in this chemical, it is also called what?...\n",
            "Generated Query: Cadmium Chloride in soluble in chemistry\n",
            "\n",
            "\n",
            "2. QUERIES WITH CONTEXT and without fewshot:\n",
            "--------------------------------------------------\n",
            "\n",
            "Question 1: Which magazine was started first Arthur's Magazine or First for Women?...\n",
            "Context: Radio City is India's first private FM radio station and was started on 3 July 2001.  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).  It plays Hindi, English and regional songs.......\n",
            "Generated Query: Arthur's Magazine or First for women\n",
            "\n",
            "Question 2: The Oberoi family is part of a hotel company that has a head office in what city?...\n",
            "Context: The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta.  It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel.  It is operated by The Ritz-Carlton Hotel Company.......\n",
            "Generated Query: The Oberoi family\n",
            "\n",
            "Question 3: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?...\n",
            "Context: Lisa Marie Simpson is a fictional character in the animated television series \"The Simpsons\".  She is the middle child and most intelligent of the Simpson family.  Voiced by Yeardley Smith, Lisa first appeared on television in \"The Tracey Ullman Show\" short \"Good Night\" on April 19, 1987.......\n",
            "Generated Query: Michael Michael Bloomberg Jr. DD-TABLECONTEXT LEARNING FOR PAS\n",
            "\n",
            "Question 4:  What nationality was James Henry Miller's wife?...\n",
            "Context: Moloch: or, This Gentile World is a semi-autobiographical novel written by Henry Miller in 1927-28, initially under the guise of a novel written by his wife, June.  The book went unpublished until 1992, 65 years after it was written and 12 years after Miller\u2019s death.  It is widely considered to be of interest more as a study of Miller\u2019s artistic growth than as a worthy piece of fiction.......\n",
            "Generated Query: James Henry Miller wife June\n",
            "\n",
            "Question 5: Cadmium Chloride is slightly soluble in this chemical, it is also called what?...\n",
            "Context: Cadmium chloride is a white crystalline compound of cadmium and chlorine, with the formula CdCl.  It is a hygroscopic solid that is highly soluble in water and slightly soluble in alcohol.  Although it is considered to be ionic, it has considerable covalent character to its bonding.......\n",
            "Generated Query: Coca chloride -LRB-CD\n",
            "\n",
            "\n",
            "3. QUERIES WITH CONTEXT and with fewshot:\n",
            "--------------------------------------------------\n",
            "\n",
            "Question 1: Which magazine was started first Arthur's Magazine or First for Women?...\n",
            "Context: Radio City is India's first private FM radio station and was started on 3 July 2001.  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).  It plays Hindi, English and regional songs.......\n",
            "Generated Query: Arthur's Magazine\n",
            "\n",
            "Question 2: The Oberoi family is part of a hotel company that has a head office in what city?...\n",
            "Context: The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta.  It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel.  It is operated by The Ritz-Carlton Hotel Company.......\n",
            "Generated Query: Hotels in the city of Pune\n",
            "\n",
            "Question 3: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?...\n",
            "Context: Lisa Marie Simpson is a fictional character in the animated television series \"The Simpsons\".  She is the middle child and most intelligent of the Simpson family.  Voiced by Yeardley Smith, Lisa first appeared on television in \"The Tracey Ullman Show\" short \"Good Night\" on April 19, 1987.......\n",
            "Generated Query: Matt Groening's song Matt Groenheit 18\n",
            "\n",
            "Question 4:  What nationality was James Henry Miller's wife?...\n",
            "Context: Moloch: or, This Gentile World is a semi-autobiographical novel written by Henry Miller in 1927-28, initially under the guise of a novel written by his wife, June.  The book went unpublished until 1992, 65 years after it was written and 12 years after Miller\u2019s death.  It is widely considered to be of interest more as a study of Miller\u2019s artistic growth than as a worthy piece of fiction.......\n",
            "Generated Query: American nationality James Henry Henry Miller wife\n",
            "\n",
            "Question 5: Cadmium Chloride is slightly soluble in this chemical, it is also called what?...\n",
            "Context: Cadmium chloride is a white crystalline compound of cadmium and chlorine, with the formula CdCl.  It is a hygroscopic solid that is highly soluble in water and slightly soluble in alcohol.  Although it is considered to be ionic, it has considerable covalent character to its bonding.......\n",
            "Generated Query: Cadmium chloride\n",
            "\n",
            "\n",
            "4. QUERIES WITHout CONTEXT (FEW SHOT EXAMPLES):\n",
            "--------------------------------------------------\n",
            "\n",
            "Question 1: Which magazine was started first Arthur's Magazine or First for Women?...\n",
            "Context: Radio City is India's first private FM radio station and was started on 3 July 2001.  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).  It plays Hindi, English and regional songs.......\n",
            "Generated Query: Arthur's Magazine\n",
            "\n",
            "Question 2: The Oberoi family is part of a hotel company that has a head office in what city?...\n",
            "Context: The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta.  It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel.  It is operated by The Ritz-Carlton Hotel Company.......\n",
            "Generated Query: Zurich\n",
            "\n",
            "Question 3: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?...\n",
            "Context: Lisa Marie Simpson is a fictional character in the animated television series \"The Simpsons\".  She is the middle child and most intelligent of the Simpson family.  Voiced by Yeardley Smith, Lisa first appeared on television in \"The Tracey Ullman Show\" short \"Good Night\" on April 19, 1987.......\n",
            "Generated Query: Milhouse character \"Milhouse\" song written by allie Goertz\n",
            "\n",
            "Question 4:  What nationality was James Henry Miller's wife?...\n",
            "Context: Moloch: or, This Gentile World is a semi-autobiographical novel written by Henry Miller in 1927-28, initially under the guise of a novel written by his wife, June.  The book went unpublished until 1992, 65 years after it was written and 12 years after Miller\u2019s death.  It is widely considered to be of interest more as a study of Miller\u2019s artistic growth than as a worthy piece of fiction.......\n",
            "Generated Query: United States nationality\n",
            "\n",
            "Question 5: Cadmium Chloride is slightly soluble in this chemical, it is also called what?...\n",
            "Context: Cadmium chloride is a white crystalline compound of cadmium and chlorine, with the formula CdCl.  It is a hygroscopic solid that is highly soluble in water and slightly soluble in alcohol.  Although it is considered to be ionic, it has considerable covalent character to its bonding.......\n",
            "Generated Query: nitrogen phosphorus\n",
            "\n",
            "\n",
            "5. MULTIPLE QUERIES FOR SAME QUESTION:\n",
            "--------------------------------------------------\n",
            "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
            "Generated queries:\n",
            "  1. Arthur's magazine first date first\n",
            "  2. Arthur's' Magazine\n",
            "  3. Arthur's Magazine first magazine women\n",
            "  4. First for Women magazine\n",
            "  5. Arthur's Magazine first magazine\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate example queries to showcase the T5 Flan query generator\n",
        "print(\"=\"*80)\n",
        "print(\"QUERY GENERATOR EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get a few sample questions from the dataset\n",
        "sample_questions = train_dataset['question'][:5]\n",
        "sample_contexts = []\n",
        "\n",
        "# Prepare some sample contexts from the dataset\n",
        "for i in range(5):\n",
        "    sample = {k: train_dataset[k][i] for k in train_dataset.keys()}\n",
        "    flattened_sentences, _ = prepare_sample_context(sample)\n",
        "    # Take first few sentences as context\n",
        "    context = \" \".join(flattened_sentences[:3])\n",
        "    sample_contexts.append(context + \"...\" if len(context) > 200 else context)\n",
        "\n",
        "print(\"\\n1. QUERIES WITHOUT CONTEXT:\")\n",
        "print(\"-\" * 50)\n",
        "for i, question in enumerate(sample_questions):\n",
        "    query = generate_query(question, use_fewshot= False )\n",
        "    print(f\"\\nQuestion {i+1}: {question}...\")\n",
        "    print(f\"Generated Query: {query}\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n2. QUERIES WITH CONTEXT and without fewshot:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (question, context) in enumerate(zip(sample_questions, sample_contexts)):\n",
        "    query = generate_query(question, context, use_fewshot = False)\n",
        "    print(f\"\\nQuestion {i+1}: {question}...\")\n",
        "    print(f\"Context: {context}...\")\n",
        "    print(f\"Generated Query: {query}\")\n",
        "\n",
        "print(\"\\n\\n3. QUERIES WITH CONTEXT and with fewshot:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (question, context) in enumerate(zip(sample_questions, sample_contexts)):\n",
        "    query = generate_query(question, context, use_fewshot = True)\n",
        "    print(f\"\\nQuestion {i+1}: {question}...\")\n",
        "    print(f\"Context: {context}...\")\n",
        "    print(f\"Generated Query: {query}\")\n",
        "\n",
        "print(\"\\n\\n4. QUERIES WITHout CONTEXT (FEW SHOT EXAMPLES):\")\n",
        "print(\"-\" * 50)\n",
        "for i, (question, context) in enumerate(zip(sample_questions, sample_contexts)):\n",
        "    query = generate_query(question, context = None, use_fewshot = True)\n",
        "    print(f\"\\nQuestion {i+1}: {question}...\")\n",
        "    print(f\"Context: {context}...\")\n",
        "    print(f\"Generated Query: {query}\")\n",
        "\n",
        "print(\"\\n\\n5. MULTIPLE QUERIES FOR SAME QUESTION:\")\n",
        "print(\"-\" * 50)\n",
        "example_question = sample_questions[0]\n",
        "print(f\"Question: {example_question}\")\n",
        "print(\"Generated queries:\")\n",
        "for j in range(NUM_QUERIES):\n",
        "    query = generate_query(example_question, sample_contexts[0], use_fewshot= True)\n",
        "    print(f\"  {j+1}. {query}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJwreFPCZ1Gh"
      },
      "source": [
        "## Main Processing Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "3YlFNKftZ1Gh",
        "outputId": "adef5eee-b579-46e3-a160-76cef322319c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>hop_0_avg_ap</td><td>\u2586\u2586\u2587\u2584\u2581\u2582\u2585\u2588\u2588\u2585\u2585\u2583\u2587\u2583\u2588\u2588\u2583\u2583\u2582\u2588\u2581\u2581\u2581\u2586\u2586\u2581\u2587\u2584\u2586\u2585\u2586\u2585\u2586\u2586\u2583\u2585\u2586\u2586\u2582\u2582</td></tr><tr><td>hop_0_avg_recall</td><td>\u2588\u2588\u2584\u2583\u2581\u2582\u2585\u2583\u2582\u2585\u2587\u2585\u2583\u2585\u2585\u2582\u2582\u2582\u2585\u2585\u2581\u2581\u2588\u2585\u2585\u2581\u2585\u2585\u2587\u2583\u2585\u2587\u2583\u2583\u2583\u2583\u2588\u2586\u2585\u2583</td></tr><tr><td>hop_0_max_ap</td><td>\u2586\u2586\u2588\u2585\u2581\u2583\u2586\u2588\u2588\u2583\u2588\u2588\u2583\u2588\u2585\u2581\u2588\u2588\u2588\u2583\u2581\u2581\u2581\u2586\u2588\u2588\u2581\u2588\u2588\u2586\u2586\u2588\u2586\u2588\u2583\u2588\u2586\u2586\u2583\u2583</td></tr><tr><td>hop_0_num_preferences</td><td>\u2581\u2581\u2585\u2582\u2581\u2583\u2588\u2581\u2581\u2583\u2581\u2587\u2581\u2583\u2588\u2581\u2581\u2583\u2586\u2585\u2584\u2581\u2581\u2581\u2581\u2581\u2583\u2586\u2582\u2581\u2586\u2585\u2585\u2588\u2581\u2583\u2582\u2583\u2583\u2582</td></tr><tr><td>hop_0_num_queries</td><td>\u2586\u2583\u2586\u2581\u2586\u2583\u2588\u2588\u2581\u2586\u2586\u2588\u2581\u2583\u2588\u2588\u2586\u2586\u2588\u2586\u2586\u2588\u2583\u2581\u2583\u2586\u2581\u2583\u2586\u2586\u2583\u2583\u2586\u2586\u2588\u2583\u2583\u2581\u2583\u2583</td></tr><tr><td>hop_1_avg_ap</td><td>\u2587\u2588\u2586\u2583\u2581\u2586\u2583\u2584\u2581\u2581\u2584\u2582\u2585\u2585\u2583\u2585\u2588\u2584\u2582\u2581\u2581\u2581\u2581\u2585\u2586\u2581\u2585\u2582\u2584\u2585\u2585\u2586\u2587\u2586\u2585\u2581\u2587\u2585\u2583\u2582</td></tr><tr><td>hop_1_avg_recall</td><td>\u2586\u2585\u2585\u2586\u2581\u2581\u2585\u2585\u2584\u2581\u2588\u2585\u2584\u2583\u2585\u2582\u2586\u2585\u2585\u2582\u2585\u2581\u2581\u2581\u2581\u2585\u2585\u2584\u2585\u2583\u2586\u2588\u2585\u2583\u2583\u2581\u2586\u2588\u2585\u2583</td></tr><tr><td>hop_1_max_ap</td><td>\u2588\u2588\u2588\u2583\u2581\u2581\u2588\u2583\u2585\u2581\u2585\u2586\u2583\u2588\u2588\u2583\u2586\u2588\u2586\u2586\u2584\u2581\u2581\u2581\u2588\u2588\u2581\u2588\u2585\u2586\u2588\u2586\u2588\u2588\u2588\u2581\u2588\u2585\u2585\u2583</td></tr><tr><td>hop_1_num_preferences</td><td>\u2583\u2581\u2584\u2582\u2581\u2581\u2583\u2581\u2582\u2581\u2581\u2582\u2588\u2582\u2583\u2583\u2581\u2582\u2586\u2581\u2581\u2581\u2581\u2581\u2586\u2583\u2581\u2584\u2585\u2583\u2581\u2583\u2582\u2585\u2588\u2581\u2582\u2581\u2583\u2582</td></tr><tr><td>hop_1_num_queries</td><td>\u2585\u2581\u2586\u2585\u2585\u2585\u2581\u2583\u2581\u2581\u2583\u2588\u2588\u2583\u2585\u2585\u2581\u2583\u2588\u2581\u2585\u2583\u2581\u2586\u2581\u2581\u2586\u2586\u2585\u2581\u2585\u2585\u2583\u2586\u2588\u2581\u2583\u2583\u2586\u2583</td></tr><tr><td>train/epoch</td><td>\u2581</td></tr><tr><td>train/global_step</td><td>\u2581</td></tr><tr><td>train/grad_norm</td><td>\u2581</td></tr><tr><td>train/learning_rate</td><td>\u2581</td></tr><tr><td>train/loss</td><td>\u2581</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>hop_0_avg_ap</td><td>0.125</td></tr><tr><td>hop_0_avg_recall</td><td>0.25</td></tr><tr><td>hop_0_max_ap</td><td>0.25</td></tr><tr><td>hop_0_num_preferences</td><td>1</td></tr><tr><td>hop_0_num_queries</td><td>2</td></tr><tr><td>hop_1_avg_ap</td><td>0.125</td></tr><tr><td>hop_1_avg_recall</td><td>0.25</td></tr><tr><td>hop_1_max_ap</td><td>0.25</td></tr><tr><td>hop_1_num_preferences</td><td>1</td></tr><tr><td>hop_1_num_queries</td><td>2</td></tr><tr><td>train/epoch</td><td>0.5</td></tr><tr><td>train/global_step</td><td>10</td></tr><tr><td>train/grad_norm</td><td>1.42635</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1068</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">preference_creation_2025-07-21_10-08-17</strong> at: <a href='https://wandb.ai/ohitit/t5-flan-preference-dataset/runs/q2pbmbkd' target=\"_blank\">https://wandb.ai/ohitit/t5-flan-preference-dataset/runs/q2pbmbkd</a><br> View project at: <a href='https://wandb.ai/ohitit/t5-flan-preference-dataset' target=\"_blank\">https://wandb.ai/ohitit/t5-flan-preference-dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250721_124004-q2pbmbkd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250721_124322-n0y43vif</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ohitit/t5-flan-preference-dataset/runs/n0y43vif' target=\"_blank\">preference_creation_2025-07-21_10-08-17</a></strong> to <a href='https://wandb.ai/ohitit/t5-flan-preference-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ohitit/t5-flan-preference-dataset' target=\"_blank\">https://wandb.ai/ohitit/t5-flan-preference-dataset</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ohitit/t5-flan-preference-dataset/runs/n0y43vif' target=\"_blank\">https://wandb.ai/ohitit/t5-flan-preference-dataset/runs/n0y43vif</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting preference dataset creation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 157/157 [51:54<00:00, 19.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing completed!\n",
            "- Total processed: 5000\n",
            "- Total skipped: 0\n",
            "- Final dataset size: 5000 questions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>completion_rate</td><td>\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581</td></tr><tr><td>final_dataset_size</td><td>\u2581</td></tr><tr><td>final_total_processed</td><td>\u2581</td></tr><tr><td>final_total_skipped</td><td>\u2581</td></tr><tr><td>hop_0_avg_ap</td><td>\u2584\u2585\u2582\u2585\u2587\u2585\u2583\u2582\u2587\u2583\u2582\u2582\u2585\u2586\u2586\u2585\u2585\u2585\u2588\u2581\u2587\u2585\u2585\u2582\u2583\u2584\u2583\u2583\u2582\u2585\u2582\u2583\u2582\u2588\u2585\u2587\u2587\u2587\u2585\u2584</td></tr><tr><td>hop_0_avg_recall</td><td>\u2586\u2585\u2585\u2586\u2586\u2585\u2583\u2585\u2583\u2584\u2586\u2581\u2582\u2583\u2585\u2583\u2586\u2584\u2586\u2586\u2587\u2586\u2583\u2582\u2586\u2588\u2584\u2585\u2584\u2583\u2583\u2586\u2584\u2584\u2585\u2583\u2583\u2582\u2585\u2583</td></tr><tr><td>hop_0_max_ap</td><td>\u2583\u2581\u2581\u2588\u2585\u2588\u2588\u2581\u2588\u2584\u2588\u2588\u2585\u2588\u2585\u2588\u2585\u2588\u2588\u2588\u2581\u2586\u2588\u2588\u2588\u2588\u2585\u2588\u2585\u2588\u2585\u2581\u2588\u2586\u2585\u2585\u2588\u2588\u2588\u2588</td></tr><tr><td>hop_0_num_preferences</td><td>\u2586\u2587\u2584\u2587\u2587\u2587\u2584\u2586\u2587\u2584\u2587\u2587\u2587\u2586\u2586\u2586\u2588\u2588\u2587\u2587\u2586\u2587\u2584\u2587\u2583\u2581\u2581\u2581\u2587\u2585\u2587\u2585\u2581\u2587\u2587\u2585\u2587\u2581\u2584\u2583</td></tr><tr><td>hop_0_num_queries</td><td>\u2588\u2588\u2588\u2588\u2588\u2588\u2586\u2588\u2588\u2588\u2586\u2583\u2586\u2586\u2588\u2583\u2588\u2588\u2588\u2588\u2583\u2588\u2583\u2588\u2588\u2588\u2581\u2588\u2588\u2588\u2588\u2586\u2588\u2588\u2588\u2583\u2588\u2586\u2588\u2588</td></tr><tr><td>hop_1_avg_ap</td><td>\u2582\u2587\u2587\u2586\u2587\u2586\u2583\u2586\u2585\u2584\u2585\u2582\u2586\u2581\u2584\u2581\u2584\u2582\u2586\u2582\u2587\u2581\u2586\u2586\u2585\u2588\u2586\u2584\u2587\u2586\u2581\u2582\u2588\u2587\u2584\u2586\u2588\u2584\u2582\u2582</td></tr><tr><td>hop_1_avg_recall</td><td>\u2585\u2581\u2583\u2584\u2587\u2588\u2582\u2585\u2581\u2583\u2585\u2588\u2585\u2585\u2582\u2583\u2585\u2583\u2585\u2587\u2587\u2588\u2585\u2587\u2581\u2588\u2582\u2585\u2585\u2583\u2583\u2584\u2584\u2586\u2582\u2587\u2584\u2582\u2585\u2583</td></tr><tr><td>hop_1_max_ap</td><td>\u2588\u2583\u2588\u2588\u2588\u2586\u2583\u2582\u2588\u2588\u2586\u2588\u2588\u2583\u2588\u2588\u2588\u2586\u2588\u2586\u2588\u2588\u2588\u2588\u2588\u2586\u2588\u2585\u2588\u2581\u2583\u2588\u2588\u2588\u2588\u2588\u2588\u2582\u2588\u2588</td></tr><tr><td>hop_1_num_preferences</td><td>\u2582\u2585\u2584\u2587\u2588\u2581\u2583\u2582\u2586\u2585\u2581\u2581\u2583\u2583\u2581\u2585\u2588\u2583\u2586\u2586\u2584\u2585\u2583\u2586\u2581\u2587\u2584\u2586\u2588\u2583\u2587\u2585\u2581\u2581\u2581\u2585\u2586\u2585\u2582\u2581</td></tr><tr><td>hop_1_num_queries</td><td>\u2588\u2588\u2588\u2586\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2586\u2588\u2588\u2586\u2586\u2586\u2586\u2588\u2588\u2588\u2586\u2585\u2586\u2585\u2583\u2585\u2588\u2581\u2583\u2588\u2585\u2588\u2585\u2588\u2583\u2588\u2588\u2586</td></tr><tr><td>processing_success_rate</td><td>\u2581</td></tr><tr><td>total_processed</td><td>\u2581\u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2583\u2583\u2583\u2584\u2584\u2584\u2584\u2584\u2585\u2585\u2585\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2588</td></tr><tr><td>total_skipped</td><td>\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>completion_rate</td><td>1</td></tr><tr><td>final_dataset_size</td><td>5000</td></tr><tr><td>final_total_processed</td><td>5000</td></tr><tr><td>final_total_skipped</td><td>0</td></tr><tr><td>hop_0_avg_ap</td><td>0.66667</td></tr><tr><td>hop_0_avg_recall</td><td>0.7</td></tr><tr><td>hop_0_max_ap</td><td>1</td></tr><tr><td>hop_0_num_preferences</td><td>8</td></tr><tr><td>hop_0_num_queries</td><td>5</td></tr><tr><td>hop_1_avg_ap</td><td>0.75</td></tr><tr><td>hop_1_avg_recall</td><td>0.375</td></tr><tr><td>hop_1_max_ap</td><td>1</td></tr><tr><td>hop_1_num_preferences</td><td>3</td></tr><tr><td>hop_1_num_queries</td><td>4</td></tr><tr><td>processing_success_rate</td><td>1</td></tr><tr><td>total_processed</td><td>5000</td></tr><tr><td>total_skipped</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">preference_creation_2025-07-21_10-08-17</strong> at: <a href='https://wandb.ai/ohitit/t5-flan-preference-dataset/runs/n0y43vif' target=\"_blank\">https://wandb.ai/ohitit/t5-flan-preference-dataset/runs/n0y43vif</a><br> View project at: <a href='https://wandb.ai/ohitit/t5-flan-preference-dataset' target=\"_blank\">https://wandb.ai/ohitit/t5-flan-preference-dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250721_124322-n0y43vif/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"t5-flan-preference-dataset\",\n",
        "    name=f\"preference_creation_{timestamp}\",\n",
        "    config={\n",
        "        \"model_path\": model_path,\n",
        "        \"dataset_size\": dataset_size,\n",
        "        \"num_hops\": NUM_HOPS,\n",
        "        \"num_queries\": NUM_QUERIES,\n",
        "        \"top_k\": TOP_K,\n",
        "        \"batch_size\": BATCH_SIZE\n",
        "    }\n",
        ")\n",
        "\n",
        "preference_dataset = {}\n",
        "total_processed = 0\n",
        "total_skipped = 0\n",
        "\n",
        "print(\"Starting preference dataset creation...\")\n",
        "\n",
        "for batch_start in tqdm(range(0, len(train_dataset['question']), BATCH_SIZE), desc=\"Processing batches\"):\n",
        "    batch_end = min(batch_start + BATCH_SIZE, len(train_dataset['question']))\n",
        "\n",
        "    for idx in range(batch_start, batch_end):\n",
        "        sample = {k: train_dataset[k][idx] for k in train_dataset.keys()}\n",
        "        question = sample['question']\n",
        "        supporting_facts = sample['supporting_facts']\n",
        "\n",
        "        # Skip if no supporting facts\n",
        "        if not supporting_facts['title']:\n",
        "            total_skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Prepare context\n",
        "        flattened_sentences, sentence_metadata = prepare_sample_context(sample)\n",
        "        context_embeddings = get_cached_embeddings(flattened_sentences)\n",
        "        supporting_pairs = set(zip(supporting_facts['title'], supporting_facts['sent_id']))\n",
        "\n",
        "        # Initialize dataset entry\n",
        "        preference_dataset[question] = {\n",
        "            \"question\": question,\n",
        "            \"hops\": {}\n",
        "        }\n",
        "\n",
        "        current_context = \"\"\n",
        "        current_context_index = None\n",
        "\n",
        "        # Process each hop\n",
        "        for hop in range(NUM_HOPS):\n",
        "\n",
        "            hop_data = process_single_hop_optimized(\n",
        "                question, current_context, flattened_sentences,\n",
        "                context_embeddings, supporting_pairs, sentence_metadata, current_context_index\n",
        "            )\n",
        "\n",
        "            if hop_data:\n",
        "                preference_dataset[question][\"hops\"][f\"hop_{hop}\"] = hop_data\n",
        "                # Update context with best retrieval for next hop\n",
        "                if hop_data[\"queries\"]:\n",
        "                    current_context = hop_data.get(\"best_retrieved_context\", \"\")\n",
        "                    current_context_index = hop_data.get(\"best_retrieved_context_idx\", None)\n",
        "\n",
        "                # Log hop metrics to wandb\n",
        "                wandb.log({\n",
        "                    f\"hop_{hop}_avg_ap\": np.mean(hop_data[\"aps\"]) if hop_data[\"aps\"] else 0,\n",
        "                    f\"hop_{hop}_max_ap\": max(hop_data[\"aps\"]) if hop_data[\"aps\"] else 0,\n",
        "                    f\"hop_{hop}_avg_recall\": np.mean(hop_data[\"recalls\"]) if hop_data[\"recalls\"] else 0,\n",
        "                    f\"hop_{hop}_num_queries\": len(hop_data[\"queries\"]),\n",
        "                    f\"hop_{hop}_num_preferences\": len(hop_data[\"preference_pairs\"])\n",
        "                })\n",
        "\n",
        "        total_processed += 1\n",
        "\n",
        "        # Log progress every 100 samples\n",
        "        if total_processed % 100 == 0:\n",
        "            wandb.log({\n",
        "                \"total_processed\": total_processed,\n",
        "                \"total_skipped\": total_skipped,\n",
        "                \"completion_rate\": total_processed / (total_processed + total_skipped)\n",
        "            })\n",
        "\n",
        "# Log final metrics\n",
        "wandb.log({\n",
        "    \"final_total_processed\": total_processed,\n",
        "    \"final_total_skipped\": total_skipped,\n",
        "    \"final_dataset_size\": len(preference_dataset),\n",
        "    \"processing_success_rate\": total_processed / (total_processed + total_skipped) if (total_processed + total_skipped) > 0 else 0\n",
        "})\n",
        "\n",
        "print(f\"Processing completed!\")\n",
        "print(f\"- Total processed: {total_processed}\")\n",
        "print(f\"- Total skipped: {total_skipped}\")\n",
        "print(f\"- Final dataset size: {len(preference_dataset)} questions\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y6T4sfJZ1Gh"
      },
      "source": [
        "## Saving the Prefence Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPR98DhzZ1Gh",
        "outputId": "a0a607c1-e035-4d90-9c0e-de2ab563770e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preference dataset saved to preference_dataset_t5_flan_2025-07-21_10-08-17.json\n",
            "\n",
            "Dataset Statistics:\n",
            "- Total questions: 5000\n",
            "- Total hops: 10000\n",
            "- Total preference pairs: 53717\n",
            "- Average preference pairs per hop: 5.37\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "timestamp = CURRENT_TIME_STAMP.replace(\" \", \"_\").replace(\":\", \"-\")\n",
        "\n",
        "# TODO : Change the output file name by including the current generation settings\n",
        "output_file = f\"preference_dataset_t5_flan_{timestamp}.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(preference_dataset, f, indent=2)\n",
        "\n",
        "print(f\"Preference dataset saved to {output_file}\")\n",
        "\n",
        "# Display statistics\n",
        "total_preference_pairs = 0\n",
        "total_hops = 0\n",
        "\n",
        "for question, data in preference_dataset.items():\n",
        "    for hop_key, hop_data in data[\"hops\"].items():\n",
        "        total_hops += 1\n",
        "        total_preference_pairs += len(hop_data[\"preference_pairs\"])\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"- Total questions: {len(preference_dataset)}\")\n",
        "print(f\"- Total hops: {total_hops}\")\n",
        "print(f\"- Total preference pairs: {total_preference_pairs}\")\n",
        "print(f\"- Average preference pairs per hop: {total_preference_pairs/total_hops:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Format the preference dataset for training usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VxO0_OSLJB0",
        "outputId": "177231b5-f8e9-4822-ea4d-6ae3b4889e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data formatting function defined\n"
          ]
        }
      ],
      "source": [
        "def format_preference_data_for_training(preference_dataset_path):\n",
        "    \"\"\"Convert preference dataset to training format\"\"\"\n",
        "    with open(preference_dataset_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for question, entry in data.items():\n",
        "        for hop_key, hop_data in entry[\"hops\"].items():\n",
        "            queries = hop_data[\"queries\"]\n",
        "            aps = hop_data[\"aps\"]\n",
        "            preference_pairs = hop_data[\"preference_pairs\"]\n",
        "\n",
        "            for preferred_idx, dispreferred_idx in preference_pairs:\n",
        "                training_data.append({\n",
        "                    \"question\": question,\n",
        "                    \"preferred\": queries[preferred_idx],\n",
        "                    \"dispreferred\": queries[dispreferred_idx],\n",
        "                    \"preferred_ap\": aps[preferred_idx],\n",
        "                    \"dispreferred_ap\": aps[dispreferred_idx],\n",
        "                    \"hop\": hop_key\n",
        "                })\n",
        "\n",
        "    return training_data\n",
        "\n",
        "print(\"Training data formatting function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0GxVmimZ1Gj",
        "outputId": "89d4b7b7-a984-4442-a38d-46c19796adbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 53717 preference pairs for training\n",
            "Training data saved to preference_training_data_formatted_2025-07-21_10-08-17.json\n"
          ]
        }
      ],
      "source": [
        "# Format the preference dataset for training\n",
        "training_data = format_preference_data_for_training(output_file)\n",
        "\n",
        "print(f\"Created {len(training_data)} preference pairs for training\")\n",
        "\n",
        "# Save training data\n",
        "training_filename = f\"preference_training_data_formatted_{timestamp}.json\"\n",
        "with open(training_filename, \"w\") as f:\n",
        "    json.dump(training_data, f, indent=2)\n",
        "\n",
        "print(f\"Training data saved to {training_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LoHdtboZ1Gj"
      },
      "source": [
        "## Display Sample Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYQ8kl9hZ1Gj",
        "outputId": "5775511f-c197-4b93-a089-0f9ef825aab1"
      },
      "outputs": [],
      "source": [
        "# Display sample preference pairs\n",
        "if training_data:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAMPLE PREFERENCE PAIRS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, sample in enumerate(training_data[:5]):  # Show first 3 samples\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"Question: {sample['question'][:100]}...\")\n",
        "        print(f\"Hop: {sample['hop']}\")\n",
        "        print(f\"Preferred Query (AP={sample['preferred_ap']:.3f}): {sample['preferred']}\")\n",
        "        print(f\"Dispreferred Query (AP={sample['dispreferred_ap']:.3f}): {sample['dispreferred']}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\nTotal training samples: {len(training_data)}\")\n",
        "else:\n",
        "    print(\"No training data generated.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}