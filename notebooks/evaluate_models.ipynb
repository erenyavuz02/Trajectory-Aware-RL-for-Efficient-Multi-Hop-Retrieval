{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330514c1",
   "metadata": {},
   "source": [
    "# Query Generation for HotpotQA with ColBERT\n",
    "\n",
    "This notebook demonstrates a query generation system for the HotpotQA dataset using ColBERT embeddings. The workflow includes:\n",
    "\n",
    "- Loading the HotpotQA fullwiki dataset\n",
    "- Setting up ColBERT model for text embedding and retrieval\n",
    "- Implementing few-shot prompting for query generation\n",
    "- Evaluating query quality for multi-hop question answering\n",
    "\n",
    "The system generates search queries that can effectively retrieve relevant passages for complex multi-hop questions in the HotpotQA benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7065f3d",
   "metadata": {},
   "source": [
    "#### Unzip the pretrained Query Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: extract this zip file : \"/content/epoch_1_model.zip\" to the a folder\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Define the path to the zip file and the destination folder\n",
    "zip_path = \"/content/epoch_1_model.zip\"\n",
    "extract_path = \"/content/extracted_model\"\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "import os\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Extracted {zip_path} to {extract_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673897a",
   "metadata": {},
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5bd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x140849f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Importing the necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Setting the seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9edff",
   "metadata": {},
   "source": [
    "#### Initialize ColBERT Model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Iniitializing the retrieval model\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\").to(device)\n",
    "colbert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3917369",
   "metadata": {},
   "source": [
    "#### Clone the github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c462f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloning the GitHub repository\n",
    "import getpass\n",
    "\n",
    "# Login to GitHub\n",
    "# You will be prompted to enter your GitHub username and password.\n",
    "username = input(\"Enter your GitHub username: \")\n",
    "password = getpass.getpass(\"Enter your GitHub password: \")\n",
    "\n",
    "!git clone https://{username}:{password}@github.com/erenyavuz02/Trajectory-Aware-RL-for-Efficient-Multi-Hop-Retrieval.git\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/Trajectory-Aware-RL-for-Efficient-Multi-Hop-Retrieval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd175796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset files exist, otherwise load from HuggingFace\n",
    "def load_dataset_from_jsonl(filename):\n",
    "    data = defaultdict(list)\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            for key, value in item.items():\n",
    "                data[key].append(value)\n",
    "    return dict(data)\n",
    "\n",
    "# Check if pre-saved dataset files exist\n",
    "train_file = \"train_dataset.jsonl\"\n",
    "eval_file = \"eval_dataset.jsonl\"\n",
    "\n",
    "if os.path.exists(train_file) and os.path.exists(eval_file):\n",
    "    print(\"Loading datasets from existing files...\")\n",
    "    train_dataset = load_dataset_from_jsonl(train_file)\n",
    "    eval_dataset = load_dataset_from_jsonl(eval_file)\n",
    "    print(f\"Loaded {len(train_dataset['question'])} training samples\")\n",
    "    print(f\"Loaded {len(eval_dataset['question'])} evaluation samples\")\n",
    "else:\n",
    "    print(\"Loading dataset from HuggingFace...\")\n",
    "    dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
    "    DATASET_SPLIT = 0.9  # 90% for training, 10% for validation\n",
    "    train_dataset = dataset['train'][:5000]  # Use 5K for faster processing\n",
    "    eval_dataset = dataset['train'][5000:6000]  # Use 1K for validation\n",
    "    print(f\"Loaded {len(train_dataset)} training samples\")\n",
    "    print(f\"Loaded {len(eval_dataset)} evaluation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa9fe5",
   "metadata": {},
   "source": [
    "#### Load few-shot prompts\n",
    "\n",
    "Print some examples of few-shot prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47fc24d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which Nirvana album featured The Vaselines, Dave Grohl, and Chad Channing?\n",
      "Query: Nirvana album featuring The Vaselines Dave Grohl Chad Channing\n",
      "----------------------------------------\n",
      "Question: Who contributed to more Disney films, Claire Keane or her father Glen Keane?\n",
      "Query: Claire Keane vs Glen Keane Disney films\n",
      "----------------------------------------\n",
      "Question: What do the films \"Giuliani Time\" and \"Life After People\" have in common?\n",
      "Query: Giuliani Time and Life After People commonalities\n",
      "----------------------------------------\n",
      "Question:  \"I Saw Her Again\" was co-written by what Canadian singer born in 1940?\n",
      "Query: \"I Saw Her Again\" co-written by Canadian singer born 1940\n",
      "----------------------------------------\n",
      "Question: What former Detroit Pistons player hosted a talkshow on MTV in 1996?\n",
      "Query: Detroit Pistons player hosted MTV talk show 1996\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get the fewshot examples from the json file\n",
    "with open(\"fewshot_examples.json\", \"r\") as f:\n",
    "    FEWSHOT_EXAMPLES = json.load(f)\n",
    "\n",
    "# print some examples\n",
    "for example in FEWSHOT_EXAMPLES[:5]:  # Print the first 5 examples\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37671fcd",
   "metadata": {},
   "source": [
    "#### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd6e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Few-shot examples for generating search queries\n",
    "def build_fewshot_prompt(question, context=\"\", add_fewshot=False):\n",
    "    \n",
    "    task_str = f\"Generate a search query for the following question:\\n{question}\"\n",
    "    \n",
    "    context_str = f\"Context:\\n{context}\\n\\n\" if context else \"\"\n",
    "    \n",
    "    if add_fewshot:\n",
    "        num_fewshots = random.randint(1, 3)\n",
    "        fewshots = random.sample(FEWSHOT_EXAMPLES, num_fewshots)\n",
    "\n",
    "        fewshot_str = \"Examples:\\n\"\n",
    "        for ex in fewshots:\n",
    "            fewshot_str += f\"Question:{ex['question']}\\nQuery:{ex['query']}\\n\\n\"\n",
    "        return f\"{fewshot_str}{context_str}{task_str}\"\n",
    "    else:\n",
    "        return f\"{context_str}{task_str}\"\n",
    "    \n",
    "# === Embedding utility ===\n",
    "def compute_colbert_embeddings(texts):\n",
    "    encoded = colbert_tokenizer(\n",
    "        texts,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = colbert_model(**encoded).last_hidden_state\n",
    "    masks = encoded[\"attention_mask\"].bool()\n",
    "    return [output[i][masks[i]].cpu().numpy() for i in range(len(texts))]\n",
    "\n",
    "# === Scoring utility ===\n",
    "def maxsim_score(query_emb, doc_embs):\n",
    "    return float((torch.matmul(query_emb, doc_embs.T)).max(dim=1).values.sum())\n",
    "\n",
    "def compute_ap_recall_precision(supporting_pairs, retrieved_ids, sentence_metadata):\n",
    "    if not retrieved_ids or not supporting_pairs:\n",
    "        return 0.0, 0.0, 0.0\n",
    "        \n",
    "    retrieved_pairs = {\n",
    "        (sentence_metadata[i][\"title\"], sentence_metadata[i][\"sent_idx\"]) for i in retrieved_ids\n",
    "    }\n",
    "    hits = [1 if (sentence_metadata[i][\"title\"], sentence_metadata[i][\"sent_idx\"]) in supporting_pairs else 0 for i in retrieved_ids]\n",
    "    \n",
    "    # Calculate AP (Average Precision)\n",
    "    ap = sum(hits[i] / (i + 1) for i in range(len(hits)) if hits[i]) / max(sum(hits), 1)\n",
    "    \n",
    "    # Calculate regular precision\n",
    "    precision = sum(hits) / len(retrieved_ids) if retrieved_ids else 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = sum(hits) / len(supporting_pairs) if supporting_pairs else 0\n",
    "    \n",
    "    return ap, precision, recall\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return (2 * precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ea14e",
   "metadata": {},
   "source": [
    "# Evaluater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hotpotqa(\n",
    "    eval_dataset,\n",
    "    query_generator,\n",
    "    query_tokenizer,\n",
    "    num_hops=2,\n",
    "    top_k_retrieval=5,\n",
    "    max_new_tokens=20  # Allow more tokens for potentially longer queries\n",
    "):\n",
    "    print(f\"Starting evaluation with {len(eval_dataset)} samples...\")  # Print evaluation start message\n",
    "\n",
    "    # ---------- PREPARING ----------\n",
    "    metrics_per_hop = [{  # Initialize per-hop metrics storage\n",
    "        \"total_ap\": 0.0,\n",
    "        \"total_precision\": 0.0,\n",
    "        \"total_recall\": 0.0,\n",
    "        \"num_samples\": 0\n",
    "    } for _ in range(num_hops)]\n",
    "\n",
    "    all_results = []  # To store detailed results for inspection\n",
    "    pbar = tqdm(range(len(eval_dataset)), desc=\"Evaluating\")  # Initialize progress bar\n",
    "    \n",
    "    for idx in pbar:  # Iterate through each sample in the dataset\n",
    "        sample = eval_dataset[idx]  # Get current sample\n",
    "        question = sample['question']  # Extract question text\n",
    "        supporting_facts = sample['supporting_facts']  # Extract ground truth supporting facts\n",
    "        \n",
    "        context_titles = sample['context']['title']  # Extract context titles\n",
    "        context_sentences_grouped = sample['context']['sentences']  # Extract grouped context sentences\n",
    "        flattened_sentences = []  # Initialize list for all sentences\n",
    "        sentence_metadata = []  # Initialize metadata for each sentence\n",
    "        for title, sentences in zip(context_titles, context_sentences_grouped):  # Iterate through title-sentence pairs\n",
    "            for i, sent in enumerate(sentences):  # Iterate through sentences in each group\n",
    "                flattened_sentences.append(sent)  # Add sentence to flat list\n",
    "                sentence_metadata.append({\"title\": title, \"sent_idx\": i})  # Add metadata\n",
    "\n",
    "        context_embeddings = compute_colbert_embeddings(flattened_sentences)  # Compute embeddings for all context sentences\n",
    "\n",
    "        vector_store_embeddings_for_scoring = [torch.tensor(emb, dtype=torch.float32).to(device) for emb in context_embeddings]  # Convert embeddings to tensors\n",
    "\n",
    "        current_context = \"\"  # Initialize context for first hop (empty)\n",
    "        ground_truth_supporting_pairs = set(zip(supporting_facts['title'], supporting_facts['sent_id']))  # Create set of ground truth pairs\n",
    "\n",
    "        question_results = {  # Initialize results storage for current question\n",
    "            \"question\": question,\n",
    "            \"ground_truth_supporting_pairs\": list(ground_truth_supporting_pairs),\n",
    "            \"hops\": []\n",
    "        }\n",
    "\n",
    "        if not ground_truth_supporting_pairs:  # Skip questions with no supporting facts\n",
    "            continue\n",
    "\n",
    "        for hop in range(num_hops):  # Iterate through each hop\n",
    "\n",
    "            # ---------- QUERY GENERATION ----------\n",
    "            prompt = build_fewshot_prompt(question, context=current_context)  # Build few-shot prompt\n",
    "\n",
    "            inputs = query_tokenizer(  # Tokenize the prompt\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,  # Apply padding if batching\n",
    "                truncation=True\n",
    "            ).to(query_generator.device)\n",
    "\n",
    "            outputs = query_generator.generate(  # Generate query using T5 model\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Deterministic generation\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=query_tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=False\n",
    "            )\n",
    "\n",
    "            generated_sequence = outputs.sequences[0]  # Extract generated sequence\n",
    "            generated_query = query_tokenizer.decode(generated_sequence, skip_special_tokens=True).strip()  # Decode to text\n",
    "\n",
    "            if not generated_query:  # Check if query is empty\n",
    "                print(f\"Warning: Empty query generated for question: '{question}' hop: {hop}.\")\n",
    "                continue  # Skip if empty query\n",
    "\n",
    "            # ---------- RETRIEVAL ----------\n",
    "            query_emb_list = compute_colbert_embeddings([generated_query])  # Compute query embeddings\n",
    "            if not query_emb_list:  # Check if embedding generation failed\n",
    "                print(f\"Warning: No embedding generated for query: '{generated_query}' for question: '{question}' hop: {hop}.\")\n",
    "                continue  # Skip if embedding fails\n",
    "\n",
    "            query_emb_np = query_emb_list[0]  # Get query embedding as numpy array\n",
    "            query_emb = torch.tensor(query_emb_np, dtype=torch.float32).to(device)  # Convert to tensor\n",
    "\n",
    "            scores = []  # Initialize scores list\n",
    "            for doc_emb in vector_store_embeddings_for_scoring:  # Iterate through document embeddings\n",
    "                scores.append(maxsim_score(query_emb, doc_emb))  # Compute similarity score\n",
    "\n",
    "            if not scores:  # Check if no scores computed\n",
    "                continue\n",
    "\n",
    "            top_indices = np.argsort(scores)[-top_k_retrieval:][::-1]  # Get top-k retrieved document indices\n",
    "\n",
    "            # ---------- PRECISION CALCULATION ----------\n",
    "            ap, precision, recall = compute_ap_recall_precision(  # Calculate evaluation metrics\n",
    "                ground_truth_supporting_pairs, \n",
    "                top_indices, \n",
    "                sentence_metadata\n",
    "            )\n",
    "\n",
    "            f1 = calculate_f1(precision, recall)  # Calculate F1 score\n",
    "\n",
    "            metrics_per_hop[hop][\"total_ap\"] += ap  # Accumulate AP score\n",
    "            metrics_per_hop[hop][\"total_precision\"] += precision  # Accumulate precision\n",
    "            metrics_per_hop[hop][\"total_recall\"] += recall  # Accumulate recall\n",
    "            metrics_per_hop[hop][\"num_samples\"] += 1  # Increment sample count\n",
    "\n",
    "            retrieved_context = [flattened_sentences[i] for i in top_indices]  # Get retrieved sentences\n",
    "\n",
    "            question_results[\"hops\"].append({  # Store hop results\n",
    "                \"hop\": hop,\n",
    "                \"generated_query\": generated_query,\n",
    "                \"raw_generated_query\": generated_query,\n",
    "                \"ap\": ap,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"top_k_retrieved_docs\": retrieved_context,\n",
    "                \"top_k_retrieved_ids\": top_indices.tolist()\n",
    "            })\n",
    "\n",
    "            current_context = \"\\n\".join(retrieved_context)  # Update context for next hop\n",
    "\n",
    "        all_results.append(question_results)  # Store results for current question\n",
    "        \n",
    "        if idx > 0:  # Update progress bar with current metrics (avoid division by zero)\n",
    "            total_samples = sum(hop[\"num_samples\"] for hop in metrics_per_hop)  # Calculate total samples\n",
    "            if total_samples > 0:  # Check if samples exist\n",
    "                current_ap = sum(hop[\"total_ap\"] for hop in metrics_per_hop) / total_samples  # Calculate current AP\n",
    "                current_precision = sum(hop[\"total_precision\"] for hop in metrics_per_hop) / total_samples  # Calculate current precision\n",
    "                current_recall = sum(hop[\"total_recall\"] for hop in metrics_per_hop) / total_samples  # Calculate current recall\n",
    "                current_f1 = calculate_f1(current_precision, current_recall)  # Calculate current F1\n",
    "                \n",
    "                pbar.set_postfix({  # Update progress bar display\n",
    "                    'AP': f'{current_ap:.3f}',\n",
    "                    'P': f'{current_precision:.3f}',\n",
    "                    'R': f'{current_recall:.3f}',\n",
    "                    'F1': f'{current_f1:.3f}'\n",
    "                })\n",
    "\n",
    "    # ---------- PREPARING FINAL RESULTS ----------\n",
    "    print(\"\\n=== Per-Hop Evaluation Summary ===\")  # Print summary header\n",
    "    hop_summaries = []  # Initialize hop summaries list\n",
    "    for hop in range(num_hops):  # Iterate through each hop\n",
    "        num_samples = metrics_per_hop[hop][\"num_samples\"]  # Get sample count for hop\n",
    "        if num_samples > 0:  # Check if hop has samples\n",
    "            avg_ap = metrics_per_hop[hop][\"total_ap\"] / num_samples  # Calculate average AP\n",
    "            avg_precision = metrics_per_hop[hop][\"total_precision\"] / num_samples  # Calculate average precision\n",
    "            avg_recall = metrics_per_hop[hop][\"total_recall\"] / num_samples  # Calculate average recall\n",
    "            avg_f1 = calculate_f1(avg_precision, avg_recall)  # Calculate average F1\n",
    "            \n",
    "            print(f\"\\nHop {hop + 1} Metrics:\")  # Print hop header\n",
    "            print(f\"Number of Samples: {num_samples}\")  # Print sample count\n",
    "            print(f\"Average AP: {avg_ap:.4f}\")  # Print average AP\n",
    "            print(f\"Average Precision: {avg_precision:.4f}\")  # Print average precision\n",
    "            print(f\"Average Recall: {avg_recall:.4f}\")  # Print average recall\n",
    "            print(f\"Average F1: {avg_f1:.4f}\")  # Print average F1\n",
    "            \n",
    "            hop_summaries.append({  # Store hop summary\n",
    "                \"hop\": hop + 1,\n",
    "                \"num_samples\": num_samples,\n",
    "                \"average_ap\": avg_ap,\n",
    "                \"average_precision\": avg_precision,\n",
    "                \"average_recall\": avg_recall,\n",
    "                \"average_f1\": avg_f1\n",
    "            })\n",
    "\n",
    "    total_samples = sum(hop[\"num_samples\"] for hop in metrics_per_hop)  # Calculate total samples across all hops\n",
    "    overall_ap = sum(hop[\"total_ap\"] for hop in metrics_per_hop) / total_samples if total_samples > 0 else 0.0  # Calculate overall AP\n",
    "    overall_precision = sum(hop[\"total_precision\"] for hop in metrics_per_hop) / total_samples if total_samples > 0 else 0.0  # Calculate overall precision\n",
    "    overall_recall = sum(hop[\"total_recall\"] for hop in metrics_per_hop) / total_samples if total_samples > 0 else 0.0  # Calculate overall recall\n",
    "    overall_f1 = calculate_f1(overall_precision, overall_recall)  # Calculate overall F1\n",
    "\n",
    "    print(\"\\n=== Overall Metrics (Averaged Across Hops) ===\")  # Print overall summary header\n",
    "    print(f\"Total Samples Evaluated: {total_samples}\")  # Print total sample count\n",
    "    print(f\"Overall AP: {overall_ap:.4f}\")  # Print overall AP\n",
    "    print(f\"Overall Precision: {overall_precision:.4f}\")  # Print overall precision\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")  # Print overall recall\n",
    "    print(f\"Overall F1: {overall_f1:.4f}\")  # Print overall F1\n",
    "\n",
    "    return {  # Return evaluation results\n",
    "        \"overall_metrics\": {\n",
    "            \"average_ap\": overall_ap,\n",
    "            \"average_precision\": overall_precision,\n",
    "            \"average_recall\": overall_recall,\n",
    "            \"average_f1\": overall_f1,\n",
    "            \"total_samples\": total_samples\n",
    "        },\n",
    "        \"per_hop_metrics\": hop_summaries,\n",
    "        \"detailed_results\": all_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f853cd",
   "metadata": {},
   "source": [
    "# Load the query generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b864d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Model to evaluate\n",
    "model_path= \"google/flan-t5-small\"\n",
    "model_to_eval = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "model_to_eval_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91428fa4",
   "metadata": {},
   "source": [
    "# Evaluate the base model on the HotpotQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "evaluation_metrics = evaluate_hotpotqa(\n",
    "    eval_dataset=eval_dataset,\n",
    "    query_generator=model_to_eval,\n",
    "    query_tokenizer=model_to_eval_tokenizer,\n",
    "    num_hops=2,           # Keep consistent with your training/preference dataset generation\n",
    "    top_k_retrieval=5,    # Keep consistent with your preference dataset generation\n",
    "    max_new_tokens=20     # Adjust as needed for query length\n",
    ")\n",
    "\n",
    "# --- Save Results (Optional) ---\n",
    "output_filename = \"hotpotqa_evaluation_results.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(evaluation_metrics, f, indent=4)\n",
    "print(f\"\\nDetailed evaluation results saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702e913",
   "metadata": {},
   "source": [
    "# Load the trained query generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained query generation model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Replace with the path to your trained model\n",
    "trained_model_path = \"path/to/your/trained/model\"  # Update this path\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "trained_query_generator = AutoModelForSeq2SeqLM.from_pretrained(trained_model_path).to(device)\n",
    "trained_query_tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n",
    "\n",
    "# Set to evaluation mode\n",
    "trained_query_generator.eval()\n",
    "\n",
    "print(f\"Loaded trained model from: {trained_model_path}\")\n",
    "print(f\"Model device: {trained_query_generator.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243aa3f",
   "metadata": {},
   "source": [
    "# Evalute the trained model on the HotpotQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run Evaluation ---\n",
    "evaluation_metrics_trained = evaluate_hotpotqa(\n",
    "    eval_dataset=eval_dataset,\n",
    "    query_generator=trained_query_generator,\n",
    "    query_tokenizer=trained_query_tokenizer,\n",
    "    num_hops=2,           # Keep consistent with your training/preference dataset generation\n",
    "    top_k_retrieval=5,    # Keep consistent with your preference dataset generation\n",
    "    max_new_tokens=20     # Adjust as needed for query length\n",
    ")\n",
    "\n",
    "# --- Save Results (Optional) ---\n",
    "output_filename = \"hotpotqa_evaluation_results_trained_model.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(evaluation_metrics_trained, f, indent=4)\n",
    "print(f\"\\nDetailed evaluation results saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare queries generated by both models\n",
    "print(\"=== Query Comparison: Base Model vs Trained Model ===\\n\")\n",
    "\n",
    "# Get some example questions from the evaluation dataset\n",
    "sample_questions = [\n",
    "    eval_dataset[0]['question'],\n",
    "    eval_dataset[1]['question'],\n",
    "    eval_dataset[2]['question'],\n",
    "    eval_dataset[3]['question'],\n",
    "    eval_dataset[4]['question']\n",
    "]\n",
    "\n",
    "for i, question in enumerate(sample_questions):\n",
    "    print(f\"Question {i+1}: {question}\\n\")\n",
    "    \n",
    "    # Generate query with base model\n",
    "    prompt = build_fewshot_prompt(question, add_fewshot=True)\n",
    "    \n",
    "    # Base model query generation\n",
    "    inputs = model_to_eval_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    base_outputs = model_to_eval.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=model_to_eval_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    base_query = model_to_eval_tokenizer.decode(base_outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Trained model query generation\n",
    "    trained_inputs = trained_query_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    trained_outputs = trained_query_generator.generate(\n",
    "        input_ids=trained_inputs[\"input_ids\"],\n",
    "        attention_mask=trained_inputs[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=trained_query_tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    trained_query = trained_query_tokenizer.decode(trained_outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(f\"Base Model Query: {base_query}\")\n",
    "    print(f\"Trained Model Query: {trained_query}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
